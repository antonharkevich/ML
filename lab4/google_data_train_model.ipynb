{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44658173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle5 as pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "#necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3111d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datasets and labels\n",
    "all_data = pickle.load(open('D:/magistratura/magistratura/MO/lab4/SVHN_multi_crop_normalized_32.pickle', 'rb'))\n",
    "train_data = all_data['train_dataset']\n",
    "test_data = all_data['test_dataset']\n",
    "valid_data = all_data['valid_dataset']\n",
    "\n",
    "train_labels = all_data['train_labels']\n",
    "test_labels = all_data['test_labels']\n",
    "valid_labels = all_data['valid_labels']\n",
    "\n",
    "del all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de4e70ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete invalid data\n",
    "train_labels = np.delete(train_labels, 19623, axis=0)\n",
    "train_data = np.delete(train_data, 19623, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d009b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot representaton for train labels\n",
    "dataSize = train_labels.shape[0]\n",
    "onehotLabels1 = np.zeros((dataSize, 7))\n",
    "onehotLabels2 = np.zeros((dataSize, 10))\n",
    "onehotLabels3 = np.zeros((dataSize, 10))\n",
    "onehotLabels4 = np.zeros((dataSize, 10))\n",
    "onehotLabels5 = np.zeros((dataSize, 10))\n",
    "onehotLabels6 = np.zeros((dataSize, 10))\n",
    "\n",
    "for i in range(dataSize):\n",
    "    labels = train_labels[i]\n",
    "    num_of_digits = labels[0]\n",
    "    onehotLabels1[i, num_of_digits] = 1\n",
    "    counter = 0\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels2[i, labels[1] % 10] = 1\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels3[i, labels[2] % 10] = 1\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels4[i, labels[3] % 10] = 1\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels5[i, labels[4] % 10] = 1\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels6[i, labels[5] % 10] = 1\n",
    "        \n",
    "trainOneHotLabels = [onehotLabels1, onehotLabels2, onehotLabels3, onehotLabels4, onehotLabels5, onehotLabels6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca9d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot representaton for test labels\n",
    "dataSize = test_labels.shape[0]\n",
    "onehotLabels1 = np.zeros((dataSize, 7))\n",
    "onehotLabels2 = np.zeros((dataSize, 10))\n",
    "onehotLabels3 = np.zeros((dataSize, 10))\n",
    "onehotLabels4 = np.zeros((dataSize, 10))\n",
    "onehotLabels5 = np.zeros((dataSize, 10))\n",
    "onehotLabels6 = np.zeros((dataSize, 10))\n",
    "\n",
    "for i in range(dataSize):\n",
    "    labels = test_labels[i]\n",
    "    num_of_digits = labels[0]\n",
    "    onehotLabels1[i, num_of_digits] = 1\n",
    "    counter = 0\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels2[i, labels[1] % 10] = 1\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels3[i, labels[2] % 10] = 1\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels4[i, labels[3] % 10] = 1\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels5[i, labels[4] % 10] = 1\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels6[i, labels[5] % 10] = 1\n",
    "        \n",
    "testOneHotLabels = [onehotLabels1, onehotLabels2, onehotLabels3, onehotLabels4, onehotLabels5, onehotLabels6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13c4db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot representaton for validation labels\n",
    "dataSize = valid_labels.shape[0]\n",
    "onehotLabels1 = np.zeros((dataSize, 7))\n",
    "onehotLabels2 = np.zeros((dataSize, 10))\n",
    "onehotLabels3 = np.zeros((dataSize, 10))\n",
    "onehotLabels4 = np.zeros((dataSize, 10))\n",
    "onehotLabels5 = np.zeros((dataSize, 10))\n",
    "onehotLabels6 = np.zeros((dataSize, 10))\n",
    "\n",
    "for i in range(dataSize):\n",
    "    labels = valid_labels[i]\n",
    "    num_of_digits = labels[0]\n",
    "    onehotLabels1[i, num_of_digits] = 1\n",
    "    counter = 0\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels2[i, labels[1] % 10] = 1\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels3[i, labels[2] % 10] = 1\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels4[i, labels[3] % 10] = 1\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels5[i, labels[4] % 10] = 1\n",
    "    if counter < num_of_digits:\n",
    "        onehotLabels6[i, labels[5] % 10] = 1\n",
    "\n",
    "validOneHotLabels = [onehotLabels1, onehotLabels2, onehotLabels3, onehotLabels4, onehotLabels5, onehotLabels6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd79466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for grayscale images\n",
    "num_channels = 1  \n",
    "\n",
    "image_size = 32\n",
    "pixel_depth = 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4af6d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape from (183400, 32, 32) to (183400, 32, 32, 1)\n",
    "train_data = train_data.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "test_data = test_data.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "valid_data = valid_data.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffebe47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is : (183400, 32, 32, 1)\n",
      "test_data shape is : (63067, 32, 32, 1)\n",
      "valid_data shape is : (2353, 32, 32, 1)\n",
      "train_labels shape is : (183400, 6)\n",
      "test_labels shape is : (63067, 6)\n",
      "valid_labels shape is : (2353, 6)\n"
     ]
    }
   ],
   "source": [
    "print('train_data shape is : %s' % (train_data.shape,))\n",
    "print('test_data shape is : %s' % (test_data.shape,))\n",
    "print('valid_data shape is : %s' % (valid_data.shape,))\n",
    "\n",
    "print('train_labels shape is : %s' % (train_labels.shape,))\n",
    "print('test_labels shape is : %s' % (test_labels.shape,))\n",
    "print('valid_labels shape is : %s' % (valid_labels.shape,))\n",
    "\n",
    "test_size = test_data.shape[0]\n",
    "validation_size = valid_data.shape[0]\n",
    "train_size = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc04f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of digits + classifier for every one of 5 digits\n",
    "num_classifiers = 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dcf9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels length of the first classifier which is the number of digits\n",
    "num_digits_labels = 7\n",
    "\n",
    "# labels length for the digit classifier\n",
    "digits_labels = 10      \n",
    "\n",
    "# number of training images in a single iteration\n",
    "batch_size = 64         \n",
    "\n",
    "# used to calculate test predictions over many iterations to avoid memory issues\n",
    "test_batch_size = 457   \n",
    "\n",
    "# convolution filter size\n",
    "patch_size = 5          \n",
    "\n",
    "# number of filters in first convolution layer\n",
    "depth1 = 16             \n",
    "\n",
    "# number of filters in second convolution layer\n",
    "depth2 = 32             \n",
    "\n",
    "# number of filters in third convolution layer\n",
    "depth3 = 64             \n",
    "\n",
    "# the size of the unrolled vector after convolution\n",
    "num_hidden1 = 1024      \n",
    "\n",
    "# the size of the hidden neurons in fully connected layer\n",
    "num_hidden2 = 512       \n",
    "\n",
    "# the size of the hidden neurons in fully connected layer\n",
    "num_hidden3 = 256       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d59bc305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph of model\n",
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06782795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\userr\\anaconda3\\envs\\py365\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-19-b1de83c8ad6e>:200: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "\n",
    "    #labels for every classifier\n",
    "    tf_train_labels_c1 = tf.placeholder(tf.float32, shape=(batch_size, num_digits_labels))\n",
    "    tf_train_labels_c2 = tf.placeholder(tf.float32, shape=(batch_size, digits_labels))\n",
    "    tf_train_labels_c3 = tf.placeholder(tf.float32, shape=(batch_size, digits_labels))\n",
    "    tf_train_labels_c4 = tf.placeholder(tf.float32, shape=(batch_size, digits_labels))\n",
    "    tf_train_labels_c5 = tf.placeholder(tf.float32, shape=(batch_size, digits_labels))\n",
    "    tf_train_labels_c6 = tf.placeholder(tf.float32, shape=(batch_size, digits_labels))\n",
    "\n",
    "    tf_train_labels = [tf_train_labels_c1,\n",
    "                       tf_train_labels_c2,\n",
    "                       tf_train_labels_c3,\n",
    "                       tf_train_labels_c4,\n",
    "                       tf_train_labels_c5,\n",
    "                       tf_train_labels_c6]\n",
    "    #test data\n",
    "    tf_test_dataset = tf.placeholder(tf.float32, shape=(test_batch_size, image_size, image_size, num_channels))\n",
    "    \n",
    "    #validation data\n",
    "    tf_validation_dataset = tf.constant(valid_data)\n",
    "\n",
    "    #to take one image and classify it (used in android)\n",
    "    tf_one_input = tf.placeholder(tf.float32, shape=(1, image_size, image_size, num_channels),name='one_input_placeholder')\n",
    "    \n",
    "    # variables\n",
    "    \n",
    "    #                                                       5*5*1*16 \n",
    "    conv1_weights = tf.get_variable('conv1_weights', shape=[patch_size, patch_size, num_channels, depth1],\n",
    "                               initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    conv1_biases = tf.Variable(tf.constant(1.0, shape=[depth1]))\n",
    "    \n",
    "     #                                                      5*5*16*32 \n",
    "    conv2_weights = tf.get_variable('conv2_weights', shape=[patch_size, patch_size, depth1, depth2],\n",
    "                               initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    conv2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "    \n",
    "    #                                                      5*5*32*64\n",
    "    conv3_weights = tf.get_variable('conv3_weights', shape=[patch_size, patch_size, depth2, depth3],\n",
    "                               initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    conv3_biases = tf.Variable(tf.constant(1.0, shape=[depth3]))\n",
    "\n",
    "    # number of digits classifier\n",
    "    \n",
    "    #                                                                 1024*512\n",
    "    hidden1_weights_c1 = tf.get_variable('hidden1_weights_c1', shape=[num_hidden1, num_hidden2],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden1_biases_c1 = tf.Variable(tf.constant(1.0, shape=[num_hidden2]))\n",
    "    \n",
    "    #                                                                 512*256\n",
    "    hidden2_weights_c1 = tf.get_variable('hidden2_weights_c1', shape=[num_hidden2, num_hidden3],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden2_biases_c1 = tf.Variable(tf.constant(1.0, shape=[num_hidden3]))\n",
    "    \n",
    "    #                                                                 256*7\n",
    "    hidden3_weights_c1 = tf.get_variable('hidden3_weights_c1', shape=[num_hidden3, num_digits_labels],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden3_biases_c1 = tf.Variable(tf.constant(1.0, shape=[num_digits_labels]))\n",
    "\n",
    "    # first number classifier\n",
    "    \n",
    "    #                                                                 1024*512\n",
    "    hidden1_weights_c2 = tf.get_variable('hidden1_weights_c2', shape=[num_hidden1, num_hidden2],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden1_biases_c2 = tf.Variable(tf.constant(1.0, shape=[num_hidden2]))\n",
    "\n",
    "    #                                                                 512*256\n",
    "    hidden2_weights_c2 = tf.get_variable('hidden2_weights_c2', shape=[num_hidden2, num_hidden3],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden2_biases_c2 = tf.Variable(tf.constant(1.0, shape=[num_hidden3]))\n",
    "\n",
    "    #                                                                 256*10\n",
    "    hidden3_weights_c2 = tf.get_variable('hidden3_weights_c2', shape=[num_hidden3, digits_labels],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden3_biases_c2 = tf.Variable(tf.constant(1.0, shape=[digits_labels]))\n",
    "\n",
    "    # second number classifier\n",
    "    #                                                                 1024*512\n",
    "    hidden1_weights_c3 = tf.get_variable('hidden1_weights_c3', shape=[num_hidden1, num_hidden2],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden1_biases_c3 = tf.Variable(tf.constant(1.0, shape=[num_hidden2]))\n",
    "\n",
    "    #                                                                 512*256\n",
    "    hidden2_weights_c3 = tf.get_variable('hidden2_weights_c3', shape=[num_hidden2, num_hidden3],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden2_biases_c3 = tf.Variable(tf.constant(1.0, shape=[num_hidden3]))\n",
    "\n",
    "    #                                                                 256*10\n",
    "    hidden3_weights_c3 = tf.get_variable('hidden3_weights_c3', shape=[num_hidden3, digits_labels],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden3_biases_c3 = tf.Variable(tf.constant(1.0, shape=[digits_labels]))\n",
    "\n",
    "    # third number classifier\n",
    "    #                                                                 1024*512\n",
    "    hidden1_weights_c4 = tf.get_variable('hidden1_weights_c4', shape=[num_hidden1, num_hidden2],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden1_biases_c4 = tf.Variable(tf.constant(1.0, shape=[num_hidden2]))\n",
    "\n",
    "    #                                                                 512*256\n",
    "    hidden2_weights_c4 = tf.get_variable('hidden2_weights_c4', shape=[num_hidden2, num_hidden3],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden2_biases_c4 = tf.Variable(tf.constant(1.0, shape=[num_hidden3]))\n",
    "\n",
    "    #                                                                 256*10\n",
    "    hidden3_weights_c4 = tf.get_variable('hidden3_weights_c4', shape=[num_hidden3, digits_labels],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden3_biases_c4 = tf.Variable(tf.constant(1.0, shape=[digits_labels]))\n",
    "\n",
    "    # fourth number classifier\n",
    "    #                                                                 1024*512\n",
    "    hidden1_weights_c5 = tf.get_variable('hidden1_weights_c5', shape=[num_hidden1, num_hidden2],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden1_biases_c5 = tf.Variable(tf.constant(1.0, shape=[num_hidden2]))\n",
    "\n",
    "    #                                                                 512*256\n",
    "    hidden2_weights_c5 = tf.get_variable('hidden2_weights_c5', shape=[num_hidden2, num_hidden3],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden2_biases_c5 = tf.Variable(tf.constant(1.0, shape=[num_hidden3]))\n",
    "\n",
    "    #                                                                 256*10\n",
    "    hidden3_weights_c5 = tf.get_variable('hidden3_weights_c5', shape=[num_hidden3, digits_labels],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden3_biases_c5 = tf.Variable(tf.constant(1.0, shape=[digits_labels]))\n",
    "\n",
    "    # fifth number classifier\n",
    "    #                                                                 1024*512\n",
    "    hidden1_weights_c6 = tf.get_variable('hidden1_weights_c6', shape=[num_hidden1, num_hidden2],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden1_biases_c6 = tf.Variable(tf.constant(1.0, shape=[num_hidden2]))\n",
    "\n",
    "    #                                                                 512*256\n",
    "    hidden2_weights_c6 = tf.get_variable('hidden2_weights_c6', shape=[num_hidden2, num_hidden3],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden2_biases_c6 = tf.Variable(tf.constant(1.0, shape=[num_hidden3]))\n",
    "\n",
    "    #                                                                 256*10\n",
    "    hidden3_weights_c6 = tf.get_variable('hidden3_weights_c6', shape=[num_hidden3, digits_labels],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden3_biases_c6 = tf.Variable(tf.constant(1.0, shape=[digits_labels]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # first conv bloc\n",
    "    conv = tf.nn.conv2d(tf_train_dataset, conv1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.max_pool(value=conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv = tf.nn.local_response_normalization(conv)\n",
    "    hidden = tf.nn.relu(conv + conv1_biases)\n",
    "    \n",
    "    \n",
    "    # second conv block\n",
    "    conv = tf.nn.conv2d(hidden, conv2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.max_pool(value=conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv = tf.nn.local_response_normalization(conv)\n",
    "    hidden = tf.nn.relu(conv + conv2_biases)\n",
    "    \n",
    "    \n",
    "\n",
    "    # third conv block\n",
    "    conv = tf.nn.conv2d(hidden, conv3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.max_pool(value=conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv = tf.nn.local_response_normalization(conv)\n",
    "    hidden = tf.nn.relu(conv + conv3_biases)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #flatten\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "    # first classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c1) + hidden1_biases_c1)\n",
    "    hidden = tf.nn.dropout(hidden, 0.7)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c1) + hidden2_biases_c1)\n",
    "    hidden = tf.nn.dropout(hidden, 0.7)\n",
    "    logits1 = tf.matmul(hidden, hidden3_weights_c1) + hidden3_biases_c1\n",
    "\n",
    "    # second classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c2) + hidden1_biases_c2)\n",
    "    hidden = tf.nn.dropout(hidden, 0.7)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c2) + hidden2_biases_c2)\n",
    "    hidden = tf.nn.dropout(hidden, 0.7)\n",
    "    logits2 = tf.matmul(hidden, hidden3_weights_c2) + hidden3_biases_c2\n",
    "\n",
    "\n",
    "    # third classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c3) + hidden1_biases_c3)\n",
    "    hidden = tf.nn.dropout(hidden, 0.7)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c3) + hidden2_biases_c3)\n",
    "    hidden = tf.nn.dropout(hidden, 0.7)\n",
    "    logits3 = tf.matmul(hidden, hidden3_weights_c3) + hidden3_biases_c3\n",
    "\n",
    "    # fourth classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c4) + hidden1_biases_c4)\n",
    "    hidden = tf.nn.dropout(hidden, 0.7)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c4) + hidden2_biases_c4)\n",
    "    hidden = tf.nn.dropout(hidden, 0.7)\n",
    "    logits4 = tf.matmul(hidden, hidden3_weights_c4) + hidden3_biases_c4\n",
    "    \n",
    "\n",
    "    # fifth classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c5) + hidden1_biases_c5)\n",
    "    hidden = tf.nn.dropout(hidden, 0.7)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c5) + hidden2_biases_c5)\n",
    "    hidden = tf.nn.dropout(hidden, 0.7)\n",
    "    logits5 = tf.matmul(hidden, hidden3_weights_c5) + hidden3_biases_c5\n",
    "\n",
    "    # sixth classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c6) + hidden1_biases_c6)\n",
    "    hidden = tf.nn.dropout(hidden, 0.7)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c6) + hidden2_biases_c6)\n",
    "    hidden = tf.nn.dropout(hidden, 0.7)\n",
    "    logits6 = tf.matmul(hidden, hidden3_weights_c6) + hidden3_biases_c6\n",
    "    \n",
    "    logits = [logits1, logits2, logits3, logits4, logits5, logits6]\n",
    "    \n",
    "    \n",
    "    #sum loss of all classifiers\n",
    "    loss = 0.0\n",
    "    for i in range(num_classifiers):\n",
    "        loss = loss + tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels[i], logits=logits[i]))  \n",
    "    \n",
    "    #run step\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    #learning rate with decay\n",
    "    learning_rate = tf.train.exponential_decay(0.001, global_step, 20000, 0.90, staircase=True)  \n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    #backprop\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), 1.0) \n",
    "    optimize = optimizer.apply_gradients(zip(grads, tvars),global_step=global_step)\n",
    "    \n",
    "    \n",
    "    # predictions for training data\n",
    "    prediction_c1 = tf.nn.softmax(logits[0])\n",
    "    prediction_c2 = tf.nn.softmax(logits[1])\n",
    "    prediction_c3 = tf.nn.softmax(logits[2])\n",
    "    prediction_c4 = tf.nn.softmax(logits[3])\n",
    "    prediction_c5 = tf.nn.softmax(logits[4])\n",
    "    prediction_c6 = tf.nn.softmax(logits[5])\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_prediction_c1 = prediction_c1\n",
    "    train_prediction_c2 = prediction_c2\n",
    "    train_prediction_c3 = prediction_c3\n",
    "    train_prediction_c4 = prediction_c4\n",
    "    train_prediction_c5 = prediction_c5\n",
    "    train_prediction_c6 = prediction_c6\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # first conv bloc\n",
    "    conv = tf.nn.conv2d(tf_validation_dataset, conv1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.max_pool(value=conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv = tf.nn.local_response_normalization(conv)\n",
    "    hidden = tf.nn.relu(conv + conv1_biases)\n",
    "    \n",
    "    \n",
    "    # second conv block\n",
    "    conv = tf.nn.conv2d(hidden, conv2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.max_pool(value=conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv = tf.nn.local_response_normalization(conv)\n",
    "    hidden = tf.nn.relu(conv + conv2_biases)\n",
    "    \n",
    "    \n",
    "\n",
    "    # third conv block\n",
    "    conv = tf.nn.conv2d(hidden, conv3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.max_pool(value=conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv = tf.nn.local_response_normalization(conv)\n",
    "    hidden = tf.nn.relu(conv + conv3_biases)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #flatten\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "    # first classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c1) + hidden1_biases_c1)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c1) + hidden2_biases_c1)\n",
    "    logits1 = tf.matmul(hidden, hidden3_weights_c1) + hidden3_biases_c1\n",
    "\n",
    "    # second classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c2) + hidden1_biases_c2)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c2) + hidden2_biases_c2)\n",
    "    logits2 = tf.matmul(hidden, hidden3_weights_c2) + hidden3_biases_c2\n",
    "\n",
    "\n",
    "    # third classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c3) + hidden1_biases_c3)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c3) + hidden2_biases_c3)\n",
    "    logits3 = tf.matmul(hidden, hidden3_weights_c3) + hidden3_biases_c3\n",
    "\n",
    "    # fourth classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c4) + hidden1_biases_c4)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c4) + hidden2_biases_c4)\n",
    "    logits4 = tf.matmul(hidden, hidden3_weights_c4) + hidden3_biases_c4\n",
    "    \n",
    "\n",
    "    # fifth classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c5) + hidden1_biases_c5)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c5) + hidden2_biases_c5)\n",
    "    logits5 = tf.matmul(hidden, hidden3_weights_c5) + hidden3_biases_c5\n",
    "\n",
    "    # sixth classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c6) + hidden1_biases_c6)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c6) + hidden2_biases_c6)\n",
    "    logits6 = tf.matmul(hidden, hidden3_weights_c6) + hidden3_biases_c6\n",
    "    \n",
    "    logits = [logits1, logits2, logits3, logits4, logits5, logits6]\n",
    "    \n",
    "    \n",
    "    # predictions for validation data\n",
    "    prediction_c1 = tf.nn.softmax(logits[0])\n",
    "    prediction_c2 = tf.nn.softmax(logits[1])\n",
    "    prediction_c3 = tf.nn.softmax(logits[2])\n",
    "    prediction_c4 = tf.nn.softmax(logits[3])\n",
    "    prediction_c5 = tf.nn.softmax(logits[4])\n",
    "    prediction_c6 = tf.nn.softmax(logits[5])\n",
    "    \n",
    "    valid_prediction_c1 = prediction_c1\n",
    "    valid_prediction_c2 = prediction_c2\n",
    "    valid_prediction_c3 = prediction_c3\n",
    "    valid_prediction_c4 = prediction_c4\n",
    "    valid_prediction_c5 = prediction_c5\n",
    "    valid_prediction_c6 = prediction_c6\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # first conv bloc\n",
    "    conv = tf.nn.conv2d(tf_test_dataset, conv1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.max_pool(value=conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv = tf.nn.local_response_normalization(conv)\n",
    "    hidden = tf.nn.relu(conv + conv1_biases)\n",
    "    \n",
    "    \n",
    "    # second conv block\n",
    "    conv = tf.nn.conv2d(hidden, conv2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.max_pool(value=conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv = tf.nn.local_response_normalization(conv)\n",
    "    hidden = tf.nn.relu(conv + conv2_biases)\n",
    "    \n",
    "    \n",
    "\n",
    "    # third conv block\n",
    "    conv = tf.nn.conv2d(hidden, conv3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.max_pool(value=conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv = tf.nn.local_response_normalization(conv)\n",
    "    hidden = tf.nn.relu(conv + conv3_biases)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #flatten\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "    # first classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c1) + hidden1_biases_c1)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c1) + hidden2_biases_c1)\n",
    "    logits1 = tf.matmul(hidden, hidden3_weights_c1) + hidden3_biases_c1\n",
    "\n",
    "    # second classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c2) + hidden1_biases_c2)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c2) + hidden2_biases_c2)\n",
    "    logits2 = tf.matmul(hidden, hidden3_weights_c2) + hidden3_biases_c2\n",
    "\n",
    "\n",
    "    # third classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c3) + hidden1_biases_c3)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c3) + hidden2_biases_c3)\n",
    "    logits3 = tf.matmul(hidden, hidden3_weights_c3) + hidden3_biases_c3\n",
    "\n",
    "    # fourth classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c4) + hidden1_biases_c4)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c4) + hidden2_biases_c4)\n",
    "    logits4 = tf.matmul(hidden, hidden3_weights_c4) + hidden3_biases_c4\n",
    "    \n",
    "\n",
    "    # fifth classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c5) + hidden1_biases_c5)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c5) + hidden2_biases_c5)\n",
    "    logits5 = tf.matmul(hidden, hidden3_weights_c5) + hidden3_biases_c5\n",
    "\n",
    "    # sixth classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c6) + hidden1_biases_c6)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c6) + hidden2_biases_c6)\n",
    "    logits6 = tf.matmul(hidden, hidden3_weights_c6) + hidden3_biases_c6\n",
    "    \n",
    "    logits = [logits1, logits2, logits3, logits4, logits5, logits6]\n",
    "    \n",
    "    \n",
    "    # predictions for test data\n",
    "    prediction_c1 = tf.nn.softmax(logits[0])\n",
    "    prediction_c2 = tf.nn.softmax(logits[1])\n",
    "    prediction_c3 = tf.nn.softmax(logits[2])\n",
    "    prediction_c4 = tf.nn.softmax(logits[3])\n",
    "    prediction_c5 = tf.nn.softmax(logits[4])\n",
    "    prediction_c6 = tf.nn.softmax(logits[5])\n",
    "    \n",
    "    test_prediction_c1 = prediction_c1\n",
    "    test_prediction_c2 = prediction_c2\n",
    "    test_prediction_c3 = prediction_c3\n",
    "    test_prediction_c4 = prediction_c4\n",
    "    test_prediction_c5 = prediction_c5\n",
    "    test_prediction_c6 = prediction_c6\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # first conv bloc\n",
    "    conv = tf.nn.conv2d(tf_one_input, conv1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.max_pool(value=conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv = tf.nn.local_response_normalization(conv)\n",
    "    hidden = tf.nn.relu(conv + conv1_biases)\n",
    "    \n",
    "    \n",
    "    # second conv block\n",
    "    conv = tf.nn.conv2d(hidden, conv2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.max_pool(value=conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv = tf.nn.local_response_normalization(conv)\n",
    "    hidden = tf.nn.relu(conv + conv2_biases)\n",
    "    \n",
    "    \n",
    "\n",
    "    # third conv block\n",
    "    conv = tf.nn.conv2d(hidden, conv3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.max_pool(value=conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    conv = tf.nn.local_response_normalization(conv)\n",
    "    hidden = tf.nn.relu(conv + conv3_biases)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #flatten\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "    # first classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c1) + hidden1_biases_c1)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c1) + hidden2_biases_c1)\n",
    "    logits1 = tf.matmul(hidden, hidden3_weights_c1) + hidden3_biases_c1\n",
    "\n",
    "    # second classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c2) + hidden1_biases_c2)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c2) + hidden2_biases_c2)\n",
    "    logits2 = tf.matmul(hidden, hidden3_weights_c2) + hidden3_biases_c2\n",
    "\n",
    "\n",
    "    # third classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c3) + hidden1_biases_c3)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c3) + hidden2_biases_c3)\n",
    "    logits3 = tf.matmul(hidden, hidden3_weights_c3) + hidden3_biases_c3\n",
    "\n",
    "    # fourth classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c4) + hidden1_biases_c4)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c4) + hidden2_biases_c4)\n",
    "    logits4 = tf.matmul(hidden, hidden3_weights_c4) + hidden3_biases_c4\n",
    "    \n",
    "\n",
    "    # fifth classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c5) + hidden1_biases_c5)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c5) + hidden2_biases_c5)\n",
    "    logits5 = tf.matmul(hidden, hidden3_weights_c5) + hidden3_biases_c5\n",
    "\n",
    "    # sixth classifier\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, hidden1_weights_c6) + hidden1_biases_c6)\n",
    "    hidden = tf.nn.relu(tf.matmul(hidden, hidden2_weights_c6) + hidden2_biases_c6)\n",
    "    logits6 = tf.matmul(hidden, hidden3_weights_c6) + hidden3_biases_c6\n",
    "    \n",
    "    logits = [logits1, logits2, logits3, logits4, logits5, logits6]\n",
    "    \n",
    "    \n",
    "    # predictions for android input data.\n",
    "    prediction_c1 = tf.nn.softmax(logits[0])\n",
    "    prediction_c2 = tf.nn.softmax(logits[1])\n",
    "    prediction_c3 = tf.nn.softmax(logits[2])\n",
    "    prediction_c4 = tf.nn.softmax(logits[3])\n",
    "    prediction_c5 = tf.nn.softmax(logits[4])\n",
    "    prediction_c6 = tf.nn.softmax(logits[5])\n",
    "    \n",
    "    one_prediction_c1 = prediction_c1\n",
    "    one_prediction_c2 = prediction_c2\n",
    "    one_prediction_c3 = prediction_c3\n",
    "    one_prediction_c4 = prediction_c4\n",
    "    one_prediction_c5 = prediction_c5\n",
    "    one_prediction_c6 = prediction_c6\n",
    "    \n",
    "    one_prediction_c1, one_prediction_c2, one_prediction_c3, one_prediction_c4, one_prediction_c5, one_prediction_c6 = tf.identity(one_prediction_c1, name=\"one_prediction_c1\"),tf.identity(one_prediction_c2, name=\"one_prediction_c2\"),tf.identity(one_prediction_c3, name=\"one_prediction_c3\"),tf.identity(one_prediction_c4, name=\"one_prediction_c4\"),tf.identity(one_prediction_c5, name=\"one_prediction_c5\"),tf.identity(one_prediction_c6, name=\"one_prediction_c6\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "595f6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 200001   #number of training iterations\n",
    "\n",
    "\n",
    "#used for drawing error and accuracy over time\n",
    "training_loss = []\n",
    "training_loss_epoch = []\n",
    "\n",
    "train_accuracy = []\n",
    "train_accuracy_epoch = []\n",
    "\n",
    "valid_accuracy = []\n",
    "valid_accuracy_epoch = []\n",
    "\n",
    "test_prediction = []\n",
    "\n",
    "test_accuracy=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be736950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Learning rate at step 0: 0.00100000004750\n",
      "Minibatch loss at step 0: 36.216850\n",
      "Minibatch accuracy: 0.0%\n",
      "validation accuracy: 0.3%\n",
      "Learning rate at step 50: 0.00100000004750\n",
      "Minibatch loss at step 50: 7.781147\n",
      "Minibatch accuracy: 3.1%\n",
      "Learning rate at step 100: 0.00100000004750\n",
      "Minibatch loss at step 100: 6.938569\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 150: 0.00100000004750\n",
      "Minibatch loss at step 150: 6.835230\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 200: 0.00100000004750\n",
      "Minibatch loss at step 200: 6.701900\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 250: 0.00100000004750\n",
      "Minibatch loss at step 250: 6.886686\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 300: 0.00100000004750\n",
      "Minibatch loss at step 300: 7.007493\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 350: 0.00100000004750\n",
      "Minibatch loss at step 350: 6.719604\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 400: 0.00100000004750\n",
      "Minibatch loss at step 400: 6.741855\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 450: 0.00100000004750\n",
      "Minibatch loss at step 450: 6.753977\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 500: 0.00100000004750\n",
      "Minibatch loss at step 500: 7.531482\n",
      "Minibatch accuracy: 1.6%\n",
      "validation accuracy: 0.7%\n",
      "Learning rate at step 550: 0.00100000004750\n",
      "Minibatch loss at step 550: 8.095036\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 600: 0.00100000004750\n",
      "Minibatch loss at step 600: 7.448484\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 650: 0.00100000004750\n",
      "Minibatch loss at step 650: 7.311187\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 700: 0.00100000004750\n",
      "Minibatch loss at step 700: 7.541084\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 750: 0.00100000004750\n",
      "Minibatch loss at step 750: 7.361405\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 800: 0.00100000004750\n",
      "Minibatch loss at step 800: 7.877928\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 850: 0.00100000004750\n",
      "Minibatch loss at step 850: 7.227190\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 900: 0.00100000004750\n",
      "Minibatch loss at step 900: 7.455152\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 950: 0.00100000004750\n",
      "Minibatch loss at step 950: 7.551252\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 1000: 0.00100000004750\n",
      "Minibatch loss at step 1000: 7.145812\n",
      "Minibatch accuracy: 0.0%\n",
      "validation accuracy: 0.2%\n",
      "Learning rate at step 1050: 0.00100000004750\n",
      "Minibatch loss at step 1050: 7.555555\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 1100: 0.00100000004750\n",
      "Minibatch loss at step 1100: 7.582777\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 1150: 0.00100000004750\n",
      "Minibatch loss at step 1150: 7.268147\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 1200: 0.00100000004750\n",
      "Minibatch loss at step 1200: 7.013699\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 1250: 0.00100000004750\n",
      "Minibatch loss at step 1250: 7.463154\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 1300: 0.00100000004750\n",
      "Minibatch loss at step 1300: 7.545500\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 1350: 0.00100000004750\n",
      "Minibatch loss at step 1350: 7.494251\n",
      "Minibatch accuracy: 3.1%\n",
      "Learning rate at step 1400: 0.00100000004750\n",
      "Minibatch loss at step 1400: 7.578658\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 1450: 0.00100000004750\n",
      "Minibatch loss at step 1450: 7.909373\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 1500: 0.00100000004750\n",
      "Minibatch loss at step 1500: 8.019542\n",
      "Minibatch accuracy: 0.0%\n",
      "validation accuracy: 0.2%\n",
      "Learning rate at step 1550: 0.00100000004750\n",
      "Minibatch loss at step 1550: 8.077724\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 1600: 0.00100000004750\n",
      "Minibatch loss at step 1600: 7.541603\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 1650: 0.00100000004750\n",
      "Minibatch loss at step 1650: 7.342575\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 1700: 0.00100000004750\n",
      "Minibatch loss at step 1700: 7.243714\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 1750: 0.00100000004750\n",
      "Minibatch loss at step 1750: 7.227460\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 1800: 0.00100000004750\n",
      "Minibatch loss at step 1800: 7.148674\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 1850: 0.00100000004750\n",
      "Minibatch loss at step 1850: 7.589664\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 1900: 0.00100000004750\n",
      "Minibatch loss at step 1900: 7.454765\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 1950: 0.00100000004750\n",
      "Minibatch loss at step 1950: 7.533567\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2000: 0.00100000004750\n",
      "Minibatch loss at step 2000: 8.070415\n",
      "Minibatch accuracy: 0.0%\n",
      "validation accuracy: 0.2%\n",
      "Learning rate at step 2050: 0.00100000004750\n",
      "Minibatch loss at step 2050: 7.418466\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2100: 0.00100000004750\n",
      "Minibatch loss at step 2100: 8.375278\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2150: 0.00100000004750\n",
      "Minibatch loss at step 2150: 8.105464\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2200: 0.00100000004750\n",
      "Minibatch loss at step 2200: 7.828604\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2250: 0.00100000004750\n",
      "Minibatch loss at step 2250: 7.126392\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 2300: 0.00100000004750\n",
      "Minibatch loss at step 2300: 7.574236\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2350: 0.00100000004750\n",
      "Minibatch loss at step 2350: 7.717192\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2400: 0.00100000004750\n",
      "Minibatch loss at step 2400: 7.427845\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2450: 0.00100000004750\n",
      "Minibatch loss at step 2450: 7.464102\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2500: 0.00100000004750\n",
      "Minibatch loss at step 2500: 7.447253\n",
      "Minibatch accuracy: 1.6%\n",
      "validation accuracy: 0.2%\n",
      "Learning rate at step 2550: 0.00100000004750\n",
      "Minibatch loss at step 2550: 7.918368\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2600: 0.00100000004750\n",
      "Minibatch loss at step 2600: 7.245176\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2650: 0.00100000004750\n",
      "Minibatch loss at step 2650: 7.504815\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2700: 0.00100000004750\n",
      "Minibatch loss at step 2700: 7.704461\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2750: 0.00100000004750\n",
      "Minibatch loss at step 2750: 7.149597\n",
      "Minibatch accuracy: 3.1%\n",
      "Learning rate at step 2800: 0.00100000004750\n",
      "Minibatch loss at step 2800: 7.723411\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 2850: 0.00100000004750\n",
      "Minibatch loss at step 2850: 7.290448\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2900: 0.00100000004750\n",
      "Minibatch loss at step 2900: 6.607186\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 2950: 0.00100000004750\n",
      "Minibatch loss at step 2950: 6.767744\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 3000: 0.00100000004750\n",
      "Minibatch loss at step 3000: 6.055117\n",
      "Minibatch accuracy: 3.1%\n",
      "validation accuracy: 0.7%\n",
      "Learning rate at step 3050: 0.00100000004750\n",
      "Minibatch loss at step 3050: 6.708215\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 3100: 0.00100000004750\n",
      "Minibatch loss at step 3100: 6.449884\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 3150: 0.00100000004750\n",
      "Minibatch loss at step 3150: 6.509107\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 3200: 0.00100000004750\n",
      "Minibatch loss at step 3200: 7.362109\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 3250: 0.00100000004750\n",
      "Minibatch loss at step 3250: 7.579317\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 3300: 0.00100000004750\n",
      "Minibatch loss at step 3300: 7.002831\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 3350: 0.00100000004750\n",
      "Minibatch loss at step 3350: 6.582608\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 3400: 0.00100000004750\n",
      "Minibatch loss at step 3400: 7.972583\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 3450: 0.00100000004750\n",
      "Minibatch loss at step 3450: 8.142790\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 3500: 0.00100000004750\n",
      "Minibatch loss at step 3500: 7.194058\n",
      "Minibatch accuracy: 1.6%\n",
      "validation accuracy: 0.2%\n",
      "Learning rate at step 3550: 0.00100000004750\n",
      "Minibatch loss at step 3550: 7.686936\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 3600: 0.00100000004750\n",
      "Minibatch loss at step 3600: 7.477327\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 3650: 0.00100000004750\n",
      "Minibatch loss at step 3650: 7.455839\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 3700: 0.00100000004750\n",
      "Minibatch loss at step 3700: 7.416001\n",
      "Minibatch accuracy: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 3750: 0.00100000004750\n",
      "Minibatch loss at step 3750: 7.150419\n",
      "Minibatch accuracy: 3.1%\n",
      "Learning rate at step 3800: 0.00100000004750\n",
      "Minibatch loss at step 3800: 7.647744\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 3850: 0.00100000004750\n",
      "Minibatch loss at step 3850: 7.674353\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 3900: 0.00100000004750\n",
      "Minibatch loss at step 3900: 7.259807\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 3950: 0.00100000004750\n",
      "Minibatch loss at step 3950: 7.579596\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 4000: 0.00100000004750\n",
      "Minibatch loss at step 4000: 7.612290\n",
      "Minibatch accuracy: 0.0%\n",
      "validation accuracy: 0.2%\n",
      "Learning rate at step 4050: 0.00100000004750\n",
      "Minibatch loss at step 4050: 7.101030\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 4100: 0.00100000004750\n",
      "Minibatch loss at step 4100: 7.455647\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4150: 0.00100000004750\n",
      "Minibatch loss at step 4150: 7.505779\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4200: 0.00100000004750\n",
      "Minibatch loss at step 4200: 7.312801\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4250: 0.00100000004750\n",
      "Minibatch loss at step 4250: 7.535125\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4300: 0.00100000004750\n",
      "Minibatch loss at step 4300: 7.496657\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4350: 0.00100000004750\n",
      "Minibatch loss at step 4350: 7.352656\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4400: 0.00100000004750\n",
      "Minibatch loss at step 4400: 7.780299\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4450: 0.00100000004750\n",
      "Minibatch loss at step 4450: 7.207176\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 4500: 0.00100000004750\n",
      "Minibatch loss at step 4500: 7.255729\n",
      "Minibatch accuracy: 1.6%\n",
      "validation accuracy: 0.2%\n",
      "Learning rate at step 4550: 0.00100000004750\n",
      "Minibatch loss at step 4550: 7.943919\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4600: 0.00100000004750\n",
      "Minibatch loss at step 4600: 6.993293\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4650: 0.00100000004750\n",
      "Minibatch loss at step 4650: 7.476856\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4700: 0.00100000004750\n",
      "Minibatch loss at step 4700: 7.663456\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4750: 0.00100000004750\n",
      "Minibatch loss at step 4750: 7.458758\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4800: 0.00100000004750\n",
      "Minibatch loss at step 4800: 7.953982\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4850: 0.00100000004750\n",
      "Minibatch loss at step 4850: 7.402178\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 4900: 0.00100000004750\n",
      "Minibatch loss at step 4900: 7.628478\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 4950: 0.00100000004750\n",
      "Minibatch loss at step 4950: 8.236028\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5000: 0.00100000004750\n",
      "Minibatch loss at step 5000: 7.391261\n",
      "Minibatch accuracy: 1.6%\n",
      "validation accuracy: 0.2%\n",
      "Learning rate at step 5050: 0.00100000004750\n",
      "Minibatch loss at step 5050: 7.455268\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5100: 0.00100000004750\n",
      "Minibatch loss at step 5100: 7.323550\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5150: 0.00100000004750\n",
      "Minibatch loss at step 5150: 7.599608\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 5200: 0.00100000004750\n",
      "Minibatch loss at step 5200: 7.482507\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5250: 0.00100000004750\n",
      "Minibatch loss at step 5250: 7.861586\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5300: 0.00100000004750\n",
      "Minibatch loss at step 5300: 7.995896\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5350: 0.00100000004750\n",
      "Minibatch loss at step 5350: 8.008857\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5400: 0.00100000004750\n",
      "Minibatch loss at step 5400: 7.860494\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5450: 0.00100000004750\n",
      "Minibatch loss at step 5450: 7.297719\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5500: 0.00100000004750\n",
      "Minibatch loss at step 5500: 7.433367\n",
      "Minibatch accuracy: 0.0%\n",
      "validation accuracy: 0.2%\n",
      "Learning rate at step 5550: 0.00100000004750\n",
      "Minibatch loss at step 5550: 7.412410\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5600: 0.00100000004750\n",
      "Minibatch loss at step 5600: 7.631143\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5650: 0.00100000004750\n",
      "Minibatch loss at step 5650: 7.440367\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5700: 0.00100000004750\n",
      "Minibatch loss at step 5700: 7.398085\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5750: 0.00100000004750\n",
      "Minibatch loss at step 5750: 7.176542\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5800: 0.00100000004750\n",
      "Minibatch loss at step 5800: 6.679637\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5850: 0.00100000004750\n",
      "Minibatch loss at step 5850: 7.169907\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5900: 0.00100000004750\n",
      "Minibatch loss at step 5900: 6.934218\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 5950: 0.00100000004750\n",
      "Minibatch loss at step 5950: 6.560337\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 6000: 0.00100000004750\n",
      "Minibatch loss at step 6000: 7.142219\n",
      "Minibatch accuracy: 0.0%\n",
      "validation accuracy: 0.7%\n",
      "Learning rate at step 6050: 0.00100000004750\n",
      "Minibatch loss at step 6050: 6.864515\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 6100: 0.00100000004750\n",
      "Minibatch loss at step 6100: 6.729362\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 6150: 0.00100000004750\n",
      "Minibatch loss at step 6150: 7.095071\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 6200: 0.00100000004750\n",
      "Minibatch loss at step 6200: 6.938949\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 6250: 0.00100000004750\n",
      "Minibatch loss at step 6250: 6.600569\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 6300: 0.00100000004750\n",
      "Minibatch loss at step 6300: 7.359821\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 6350: 0.00100000004750\n",
      "Minibatch loss at step 6350: 7.717059\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 6400: 0.00100000004750\n",
      "Minibatch loss at step 6400: 7.367388\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 6450: 0.00100000004750\n",
      "Minibatch loss at step 6450: 7.538544\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 6500: 0.00100000004750\n",
      "Minibatch loss at step 6500: 7.387331\n",
      "Minibatch accuracy: 1.6%\n",
      "validation accuracy: 0.2%\n",
      "Learning rate at step 6550: 0.00100000004750\n",
      "Minibatch loss at step 6550: 7.706588\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 6600: 0.00100000004750\n",
      "Minibatch loss at step 6600: 7.964350\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 6650: 0.00100000004750\n",
      "Minibatch loss at step 6650: 7.271069\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 6700: 0.00100000004750\n",
      "Minibatch loss at step 6700: 6.968635\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 6750: 0.00100000004750\n",
      "Minibatch loss at step 6750: 7.173305\n",
      "Minibatch accuracy: 3.1%\n",
      "Learning rate at step 6800: 0.00100000004750\n",
      "Minibatch loss at step 6800: 7.084823\n",
      "Minibatch accuracy: 3.1%\n",
      "Learning rate at step 6850: 0.00100000004750\n",
      "Minibatch loss at step 6850: 6.554858\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 6900: 0.00100000004750\n",
      "Minibatch loss at step 6900: 6.366522\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 6950: 0.00100000004750\n",
      "Minibatch loss at step 6950: 6.728616\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 7000: 0.00100000004750\n",
      "Minibatch loss at step 7000: 6.519006\n",
      "Minibatch accuracy: 1.6%\n",
      "validation accuracy: 0.9%\n",
      "Learning rate at step 7050: 0.00100000004750\n",
      "Minibatch loss at step 7050: 6.274615\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 7100: 0.00100000004750\n",
      "Minibatch loss at step 7100: 6.438623\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 7150: 0.00100000004750\n",
      "Minibatch loss at step 7150: 6.050450\n",
      "Minibatch accuracy: 3.1%\n",
      "Learning rate at step 7200: 0.00100000004750\n",
      "Minibatch loss at step 7200: 6.261944\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 7250: 0.00100000004750\n",
      "Minibatch loss at step 7250: 6.192762\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 7300: 0.00100000004750\n",
      "Minibatch loss at step 7300: 7.120791\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 7350: 0.00100000004750\n",
      "Minibatch loss at step 7350: 5.989563\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 7400: 0.00100000004750\n",
      "Minibatch loss at step 7400: 6.046701\n",
      "Minibatch accuracy: 3.1%\n",
      "Learning rate at step 7450: 0.00100000004750\n",
      "Minibatch loss at step 7450: 6.552396\n",
      "Minibatch accuracy: 1.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 7500: 0.00100000004750\n",
      "Minibatch loss at step 7500: 5.916779\n",
      "Minibatch accuracy: 1.6%\n",
      "validation accuracy: 2.2%\n",
      "Learning rate at step 7550: 0.00100000004750\n",
      "Minibatch loss at step 7550: 6.108604\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 7600: 0.00100000004750\n",
      "Minibatch loss at step 7600: 6.163637\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 7650: 0.00100000004750\n",
      "Minibatch loss at step 7650: 5.726625\n",
      "Minibatch accuracy: 0.0%\n",
      "Learning rate at step 7700: 0.00100000004750\n",
      "Minibatch loss at step 7700: 5.389808\n",
      "Minibatch accuracy: 4.7%\n",
      "Learning rate at step 7750: 0.00100000004750\n",
      "Minibatch loss at step 7750: 5.527652\n",
      "Minibatch accuracy: 1.6%\n",
      "Learning rate at step 7800: 0.00100000004750\n",
      "Minibatch loss at step 7800: 5.413193\n",
      "Minibatch accuracy: 4.7%\n",
      "Learning rate at step 7850: 0.00100000004750\n",
      "Minibatch loss at step 7850: 5.624057\n",
      "Minibatch accuracy: 3.1%\n",
      "Learning rate at step 7900: 0.00100000004750\n",
      "Minibatch loss at step 7900: 5.474684\n",
      "Minibatch accuracy: 4.7%\n",
      "Learning rate at step 7950: 0.00100000004750\n",
      "Minibatch loss at step 7950: 5.182618\n",
      "Minibatch accuracy: 3.1%\n",
      "Learning rate at step 8000: 0.00100000004750\n",
      "Minibatch loss at step 8000: 5.464136\n",
      "Minibatch accuracy: 6.2%\n",
      "validation accuracy: 5.8%\n",
      "Learning rate at step 8050: 0.00100000004750\n",
      "Minibatch loss at step 8050: 4.765899\n",
      "Minibatch accuracy: 7.8%\n",
      "Learning rate at step 8100: 0.00100000004750\n",
      "Minibatch loss at step 8100: 5.024425\n",
      "Minibatch accuracy: 6.2%\n",
      "Learning rate at step 8150: 0.00100000004750\n",
      "Minibatch loss at step 8150: 4.854181\n",
      "Minibatch accuracy: 9.4%\n",
      "Learning rate at step 8200: 0.00100000004750\n",
      "Minibatch loss at step 8200: 4.853559\n",
      "Minibatch accuracy: 14.1%\n",
      "Learning rate at step 8250: 0.00100000004750\n",
      "Minibatch loss at step 8250: 4.808170\n",
      "Minibatch accuracy: 10.9%\n",
      "Learning rate at step 8300: 0.00100000004750\n",
      "Minibatch loss at step 8300: 4.469078\n",
      "Minibatch accuracy: 7.8%\n",
      "Learning rate at step 8350: 0.00100000004750\n",
      "Minibatch loss at step 8350: 4.350535\n",
      "Minibatch accuracy: 6.2%\n",
      "Learning rate at step 8400: 0.00100000004750\n",
      "Minibatch loss at step 8400: 4.376566\n",
      "Minibatch accuracy: 15.6%\n",
      "Learning rate at step 8450: 0.00100000004750\n",
      "Minibatch loss at step 8450: 4.698943\n",
      "Minibatch accuracy: 17.2%\n",
      "Learning rate at step 8500: 0.00100000004750\n",
      "Minibatch loss at step 8500: 3.391615\n",
      "Minibatch accuracy: 18.8%\n",
      "validation accuracy: 21.5%\n",
      "Learning rate at step 8550: 0.00100000004750\n",
      "Minibatch loss at step 8550: 2.859895\n",
      "Minibatch accuracy: 26.6%\n",
      "Learning rate at step 8600: 0.00100000004750\n",
      "Minibatch loss at step 8600: 4.543870\n",
      "Minibatch accuracy: 17.2%\n",
      "Learning rate at step 8650: 0.00100000004750\n",
      "Minibatch loss at step 8650: 3.641072\n",
      "Minibatch accuracy: 25.0%\n",
      "Learning rate at step 8700: 0.00100000004750\n",
      "Minibatch loss at step 8700: 3.959756\n",
      "Minibatch accuracy: 28.1%\n",
      "Learning rate at step 8750: 0.00100000004750\n",
      "Minibatch loss at step 8750: 3.545743\n",
      "Minibatch accuracy: 25.0%\n",
      "Learning rate at step 8800: 0.00100000004750\n",
      "Minibatch loss at step 8800: 3.669740\n",
      "Minibatch accuracy: 29.7%\n",
      "Learning rate at step 8850: 0.00100000004750\n",
      "Minibatch loss at step 8850: 3.607535\n",
      "Minibatch accuracy: 32.8%\n",
      "Learning rate at step 8900: 0.00100000004750\n",
      "Minibatch loss at step 8900: 3.307106\n",
      "Minibatch accuracy: 26.6%\n",
      "Learning rate at step 8950: 0.00100000004750\n",
      "Minibatch loss at step 8950: 3.557832\n",
      "Minibatch accuracy: 25.0%\n",
      "Learning rate at step 9000: 0.00100000004750\n",
      "Minibatch loss at step 9000: 3.118791\n",
      "Minibatch accuracy: 32.8%\n",
      "validation accuracy: 35.4%\n",
      "Learning rate at step 9050: 0.00100000004750\n",
      "Minibatch loss at step 9050: 2.512614\n",
      "Minibatch accuracy: 43.8%\n",
      "Learning rate at step 9100: 0.00100000004750\n",
      "Minibatch loss at step 9100: 2.858603\n",
      "Minibatch accuracy: 42.2%\n",
      "Learning rate at step 9150: 0.00100000004750\n",
      "Minibatch loss at step 9150: 2.692759\n",
      "Minibatch accuracy: 40.6%\n",
      "Learning rate at step 9200: 0.00100000004750\n",
      "Minibatch loss at step 9200: 3.656664\n",
      "Minibatch accuracy: 23.4%\n",
      "Learning rate at step 9250: 0.00100000004750\n",
      "Minibatch loss at step 9250: 3.200989\n",
      "Minibatch accuracy: 37.5%\n",
      "Learning rate at step 9300: 0.00100000004750\n",
      "Minibatch loss at step 9300: 2.749976\n",
      "Minibatch accuracy: 32.8%\n",
      "Learning rate at step 9350: 0.00100000004750\n",
      "Minibatch loss at step 9350: 2.542254\n",
      "Minibatch accuracy: 31.2%\n",
      "Learning rate at step 9400: 0.00100000004750\n",
      "Minibatch loss at step 9400: 3.113272\n",
      "Minibatch accuracy: 40.6%\n",
      "Learning rate at step 9450: 0.00100000004750\n",
      "Minibatch loss at step 9450: 2.949583\n",
      "Minibatch accuracy: 37.5%\n",
      "Learning rate at step 9500: 0.00100000004750\n",
      "Minibatch loss at step 9500: 2.772806\n",
      "Minibatch accuracy: 42.2%\n",
      "validation accuracy: 48.1%\n",
      "Learning rate at step 9550: 0.00100000004750\n",
      "Minibatch loss at step 9550: 3.133546\n",
      "Minibatch accuracy: 40.6%\n",
      "Learning rate at step 9600: 0.00100000004750\n",
      "Minibatch loss at step 9600: 2.474599\n",
      "Minibatch accuracy: 50.0%\n",
      "Learning rate at step 9650: 0.00100000004750\n",
      "Minibatch loss at step 9650: 3.052945\n",
      "Minibatch accuracy: 45.3%\n",
      "Learning rate at step 9700: 0.00100000004750\n",
      "Minibatch loss at step 9700: 2.650045\n",
      "Minibatch accuracy: 43.8%\n",
      "Learning rate at step 9750: 0.00100000004750\n",
      "Minibatch loss at step 9750: 2.965380\n",
      "Minibatch accuracy: 42.2%\n",
      "Learning rate at step 9800: 0.00100000004750\n",
      "Minibatch loss at step 9800: 2.813782\n",
      "Minibatch accuracy: 42.2%\n",
      "Learning rate at step 9850: 0.00100000004750\n",
      "Minibatch loss at step 9850: 2.733464\n",
      "Minibatch accuracy: 35.9%\n",
      "Learning rate at step 9900: 0.00100000004750\n",
      "Minibatch loss at step 9900: 1.963795\n",
      "Minibatch accuracy: 59.4%\n",
      "Learning rate at step 9950: 0.00100000004750\n",
      "Minibatch loss at step 9950: 2.201751\n",
      "Minibatch accuracy: 57.8%\n",
      "Learning rate at step 10000: 0.00100000004750\n",
      "Minibatch loss at step 10000: 1.956937\n",
      "Minibatch accuracy: 56.2%\n",
      "validation accuracy: 55.5%\n",
      "Learning rate at step 10050: 0.00100000004750\n",
      "Minibatch loss at step 10050: 2.143306\n",
      "Minibatch accuracy: 62.5%\n",
      "Learning rate at step 10100: 0.00100000004750\n",
      "Minibatch loss at step 10100: 2.052671\n",
      "Minibatch accuracy: 51.6%\n",
      "Learning rate at step 10150: 0.00100000004750\n",
      "Minibatch loss at step 10150: 2.788358\n",
      "Minibatch accuracy: 56.2%\n",
      "Learning rate at step 10200: 0.00100000004750\n",
      "Minibatch loss at step 10200: 2.152346\n",
      "Minibatch accuracy: 50.0%\n",
      "Learning rate at step 10250: 0.00100000004750\n",
      "Minibatch loss at step 10250: 2.293136\n",
      "Minibatch accuracy: 57.8%\n",
      "Learning rate at step 10300: 0.00100000004750\n",
      "Minibatch loss at step 10300: 2.160112\n",
      "Minibatch accuracy: 53.1%\n",
      "Learning rate at step 10350: 0.00100000004750\n",
      "Minibatch loss at step 10350: 2.346028\n",
      "Minibatch accuracy: 60.9%\n",
      "Learning rate at step 10400: 0.00100000004750\n",
      "Minibatch loss at step 10400: 2.038223\n",
      "Minibatch accuracy: 59.4%\n",
      "Learning rate at step 10450: 0.00100000004750\n",
      "Minibatch loss at step 10450: 1.879055\n",
      "Minibatch accuracy: 62.5%\n",
      "Learning rate at step 10500: 0.00100000004750\n",
      "Minibatch loss at step 10500: 2.051414\n",
      "Minibatch accuracy: 59.4%\n",
      "validation accuracy: 61.4%\n",
      "Learning rate at step 10550: 0.00100000004750\n",
      "Minibatch loss at step 10550: 1.535043\n",
      "Minibatch accuracy: 71.9%\n",
      "Learning rate at step 10600: 0.00100000004750\n",
      "Minibatch loss at step 10600: 1.910033\n",
      "Minibatch accuracy: 59.4%\n",
      "Learning rate at step 10650: 0.00100000004750\n",
      "Minibatch loss at step 10650: 2.094035\n",
      "Minibatch accuracy: 50.0%\n",
      "Learning rate at step 10700: 0.00100000004750\n",
      "Minibatch loss at step 10700: 1.628182\n",
      "Minibatch accuracy: 64.1%\n",
      "Learning rate at step 10750: 0.00100000004750\n",
      "Minibatch loss at step 10750: 1.925912\n",
      "Minibatch accuracy: 54.7%\n",
      "Learning rate at step 10800: 0.00100000004750\n",
      "Minibatch loss at step 10800: 2.534965\n",
      "Minibatch accuracy: 64.1%\n",
      "Learning rate at step 10850: 0.00100000004750\n",
      "Minibatch loss at step 10850: 2.177879\n",
      "Minibatch accuracy: 50.0%\n",
      "Learning rate at step 10900: 0.00100000004750\n",
      "Minibatch loss at step 10900: 1.966567\n",
      "Minibatch accuracy: 62.5%\n",
      "Learning rate at step 10950: 0.00100000004750\n",
      "Minibatch loss at step 10950: 1.904103\n",
      "Minibatch accuracy: 64.1%\n",
      "Learning rate at step 11000: 0.00100000004750\n",
      "Minibatch loss at step 11000: 1.480462\n",
      "Minibatch accuracy: 71.9%\n",
      "validation accuracy: 66.7%\n",
      "Learning rate at step 11050: 0.00100000004750\n",
      "Minibatch loss at step 11050: 1.579767\n",
      "Minibatch accuracy: 64.1%\n",
      "Learning rate at step 11100: 0.00100000004750\n",
      "Minibatch loss at step 11100: 2.531434\n",
      "Minibatch accuracy: 50.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 11150: 0.00100000004750\n",
      "Minibatch loss at step 11150: 2.705787\n",
      "Minibatch accuracy: 50.0%\n",
      "Learning rate at step 11200: 0.00100000004750\n",
      "Minibatch loss at step 11200: 1.941764\n",
      "Minibatch accuracy: 57.8%\n",
      "Learning rate at step 11250: 0.00100000004750\n",
      "Minibatch loss at step 11250: 1.468920\n",
      "Minibatch accuracy: 62.5%\n",
      "Learning rate at step 11300: 0.00100000004750\n",
      "Minibatch loss at step 11300: 2.329492\n",
      "Minibatch accuracy: 53.1%\n",
      "Learning rate at step 11350: 0.00100000004750\n",
      "Minibatch loss at step 11350: 2.063773\n",
      "Minibatch accuracy: 60.9%\n",
      "Learning rate at step 11400: 0.00100000004750\n",
      "Minibatch loss at step 11400: 1.219041\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 11450: 0.00100000004750\n",
      "Minibatch loss at step 11450: 2.403554\n",
      "Minibatch accuracy: 51.6%\n",
      "Learning rate at step 11500: 0.00100000004750\n",
      "Minibatch loss at step 11500: 1.777387\n",
      "Minibatch accuracy: 59.4%\n",
      "validation accuracy: 68.0%\n",
      "Learning rate at step 11550: 0.00100000004750\n",
      "Minibatch loss at step 11550: 2.054284\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 11600: 0.00100000004750\n",
      "Minibatch loss at step 11600: 1.770737\n",
      "Minibatch accuracy: 64.1%\n",
      "Learning rate at step 11650: 0.00100000004750\n",
      "Minibatch loss at step 11650: 2.024303\n",
      "Minibatch accuracy: 64.1%\n",
      "Learning rate at step 11700: 0.00100000004750\n",
      "Minibatch loss at step 11700: 1.619410\n",
      "Minibatch accuracy: 67.2%\n",
      "Learning rate at step 11750: 0.00100000004750\n",
      "Minibatch loss at step 11750: 1.992876\n",
      "Minibatch accuracy: 62.5%\n",
      "Learning rate at step 11800: 0.00100000004750\n",
      "Minibatch loss at step 11800: 1.366128\n",
      "Minibatch accuracy: 62.5%\n",
      "Learning rate at step 11850: 0.00100000004750\n",
      "Minibatch loss at step 11850: 1.401479\n",
      "Minibatch accuracy: 65.6%\n",
      "Learning rate at step 11900: 0.00100000004750\n",
      "Minibatch loss at step 11900: 2.120367\n",
      "Minibatch accuracy: 53.1%\n",
      "Learning rate at step 11950: 0.00100000004750\n",
      "Minibatch loss at step 11950: 1.651759\n",
      "Minibatch accuracy: 62.5%\n",
      "Learning rate at step 12000: 0.00100000004750\n",
      "Minibatch loss at step 12000: 1.175395\n",
      "Minibatch accuracy: 68.8%\n",
      "validation accuracy: 70.3%\n",
      "Learning rate at step 12050: 0.00100000004750\n",
      "Minibatch loss at step 12050: 1.126619\n",
      "Minibatch accuracy: 70.3%\n",
      "Learning rate at step 12100: 0.00100000004750\n",
      "Minibatch loss at step 12100: 1.153402\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 12150: 0.00100000004750\n",
      "Minibatch loss at step 12150: 1.346059\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 12200: 0.00100000004750\n",
      "Minibatch loss at step 12200: 1.477300\n",
      "Minibatch accuracy: 54.7%\n",
      "Learning rate at step 12250: 0.00100000004750\n",
      "Minibatch loss at step 12250: 1.726420\n",
      "Minibatch accuracy: 70.3%\n",
      "Learning rate at step 12300: 0.00100000004750\n",
      "Minibatch loss at step 12300: 1.414392\n",
      "Minibatch accuracy: 64.1%\n",
      "Learning rate at step 12350: 0.00100000004750\n",
      "Minibatch loss at step 12350: 2.189791\n",
      "Minibatch accuracy: 67.2%\n",
      "Learning rate at step 12400: 0.00100000004750\n",
      "Minibatch loss at step 12400: 1.620130\n",
      "Minibatch accuracy: 65.6%\n",
      "Learning rate at step 12450: 0.00100000004750\n",
      "Minibatch loss at step 12450: 1.696119\n",
      "Minibatch accuracy: 65.6%\n",
      "Learning rate at step 12500: 0.00100000004750\n",
      "Minibatch loss at step 12500: 1.278389\n",
      "Minibatch accuracy: 62.5%\n",
      "validation accuracy: 71.4%\n",
      "Learning rate at step 12550: 0.00100000004750\n",
      "Minibatch loss at step 12550: 1.447317\n",
      "Minibatch accuracy: 67.2%\n",
      "Learning rate at step 12600: 0.00100000004750\n",
      "Minibatch loss at step 12600: 1.704443\n",
      "Minibatch accuracy: 71.9%\n",
      "Learning rate at step 12650: 0.00100000004750\n",
      "Minibatch loss at step 12650: 1.322613\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 12700: 0.00100000004750\n",
      "Minibatch loss at step 12700: 1.746425\n",
      "Minibatch accuracy: 65.6%\n",
      "Learning rate at step 12750: 0.00100000004750\n",
      "Minibatch loss at step 12750: 1.254380\n",
      "Minibatch accuracy: 67.2%\n",
      "Learning rate at step 12800: 0.00100000004750\n",
      "Minibatch loss at step 12800: 1.357005\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 12850: 0.00100000004750\n",
      "Minibatch loss at step 12850: 1.188857\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 12900: 0.00100000004750\n",
      "Minibatch loss at step 12900: 1.434156\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 12950: 0.00100000004750\n",
      "Minibatch loss at step 12950: 1.419262\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 13000: 0.00100000004750\n",
      "Minibatch loss at step 13000: 1.383322\n",
      "Minibatch accuracy: 71.9%\n",
      "validation accuracy: 74.2%\n",
      "Learning rate at step 13050: 0.00100000004750\n",
      "Minibatch loss at step 13050: 1.473408\n",
      "Minibatch accuracy: 59.4%\n",
      "Learning rate at step 13100: 0.00100000004750\n",
      "Minibatch loss at step 13100: 1.484910\n",
      "Minibatch accuracy: 70.3%\n",
      "Learning rate at step 13150: 0.00100000004750\n",
      "Minibatch loss at step 13150: 1.039950\n",
      "Minibatch accuracy: 71.9%\n",
      "Learning rate at step 13200: 0.00100000004750\n",
      "Minibatch loss at step 13200: 1.224603\n",
      "Minibatch accuracy: 64.1%\n",
      "Learning rate at step 13250: 0.00100000004750\n",
      "Minibatch loss at step 13250: 1.080423\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 13300: 0.00100000004750\n",
      "Minibatch loss at step 13300: 1.140824\n",
      "Minibatch accuracy: 67.2%\n",
      "Learning rate at step 13350: 0.00100000004750\n",
      "Minibatch loss at step 13350: 1.130597\n",
      "Minibatch accuracy: 67.2%\n",
      "Learning rate at step 13400: 0.00100000004750\n",
      "Minibatch loss at step 13400: 1.470437\n",
      "Minibatch accuracy: 70.3%\n",
      "Learning rate at step 13450: 0.00100000004750\n",
      "Minibatch loss at step 13450: 1.426957\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 13500: 0.00100000004750\n",
      "Minibatch loss at step 13500: 1.169665\n",
      "Minibatch accuracy: 65.6%\n",
      "validation accuracy: 75.6%\n",
      "Learning rate at step 13550: 0.00100000004750\n",
      "Minibatch loss at step 13550: 1.456513\n",
      "Minibatch accuracy: 65.6%\n",
      "Learning rate at step 13600: 0.00100000004750\n",
      "Minibatch loss at step 13600: 1.348591\n",
      "Minibatch accuracy: 67.2%\n",
      "Learning rate at step 13650: 0.00100000004750\n",
      "Minibatch loss at step 13650: 1.195610\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 13700: 0.00100000004750\n",
      "Minibatch loss at step 13700: 0.766481\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 13750: 0.00100000004750\n",
      "Minibatch loss at step 13750: 1.240124\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 13800: 0.00100000004750\n",
      "Minibatch loss at step 13800: 1.022909\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 13850: 0.00100000004750\n",
      "Minibatch loss at step 13850: 0.985188\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 13900: 0.00100000004750\n",
      "Minibatch loss at step 13900: 1.108556\n",
      "Minibatch accuracy: 70.3%\n",
      "Learning rate at step 13950: 0.00100000004750\n",
      "Minibatch loss at step 13950: 0.884193\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 14000: 0.00100000004750\n",
      "Minibatch loss at step 14000: 1.001788\n",
      "Minibatch accuracy: 71.9%\n",
      "validation accuracy: 76.4%\n",
      "Learning rate at step 14050: 0.00100000004750\n",
      "Minibatch loss at step 14050: 1.052730\n",
      "Minibatch accuracy: 70.3%\n",
      "Learning rate at step 14100: 0.00100000004750\n",
      "Minibatch loss at step 14100: 1.010058\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 14150: 0.00100000004750\n",
      "Minibatch loss at step 14150: 1.418875\n",
      "Minibatch accuracy: 65.6%\n",
      "Learning rate at step 14200: 0.00100000004750\n",
      "Minibatch loss at step 14200: 1.350072\n",
      "Minibatch accuracy: 64.1%\n",
      "Learning rate at step 14250: 0.00100000004750\n",
      "Minibatch loss at step 14250: 1.326834\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 14300: 0.00100000004750\n",
      "Minibatch loss at step 14300: 1.821473\n",
      "Minibatch accuracy: 64.1%\n",
      "Learning rate at step 14350: 0.00100000004750\n",
      "Minibatch loss at step 14350: 1.497742\n",
      "Minibatch accuracy: 67.2%\n",
      "Learning rate at step 14400: 0.00100000004750\n",
      "Minibatch loss at step 14400: 1.613734\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 14450: 0.00100000004750\n",
      "Minibatch loss at step 14450: 1.340789\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 14500: 0.00100000004750\n",
      "Minibatch loss at step 14500: 1.820073\n",
      "Minibatch accuracy: 54.7%\n",
      "validation accuracy: 76.7%\n",
      "Learning rate at step 14550: 0.00100000004750\n",
      "Minibatch loss at step 14550: 1.502457\n",
      "Minibatch accuracy: 65.6%\n",
      "Learning rate at step 14600: 0.00100000004750\n",
      "Minibatch loss at step 14600: 1.571606\n",
      "Minibatch accuracy: 60.9%\n",
      "Learning rate at step 14650: 0.00100000004750\n",
      "Minibatch loss at step 14650: 1.600893\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 14700: 0.00100000004750\n",
      "Minibatch loss at step 14700: 1.624677\n",
      "Minibatch accuracy: 71.9%\n",
      "Learning rate at step 14750: 0.00100000004750\n",
      "Minibatch loss at step 14750: 0.965305\n",
      "Minibatch accuracy: 75.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 14800: 0.00100000004750\n",
      "Minibatch loss at step 14800: 1.541250\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 14850: 0.00100000004750\n",
      "Minibatch loss at step 14850: 1.163884\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 14900: 0.00100000004750\n",
      "Minibatch loss at step 14900: 1.909810\n",
      "Minibatch accuracy: 60.9%\n",
      "Learning rate at step 14950: 0.00100000004750\n",
      "Minibatch loss at step 14950: 1.310689\n",
      "Minibatch accuracy: 71.9%\n",
      "Learning rate at step 15000: 0.00100000004750\n",
      "Minibatch loss at step 15000: 0.715025\n",
      "Minibatch accuracy: 82.8%\n",
      "validation accuracy: 76.2%\n",
      "Learning rate at step 15050: 0.00100000004750\n",
      "Minibatch loss at step 15050: 1.092415\n",
      "Minibatch accuracy: 67.2%\n",
      "Learning rate at step 15100: 0.00100000004750\n",
      "Minibatch loss at step 15100: 0.940393\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 15150: 0.00100000004750\n",
      "Minibatch loss at step 15150: 1.135357\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 15200: 0.00100000004750\n",
      "Minibatch loss at step 15200: 1.070532\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 15250: 0.00100000004750\n",
      "Minibatch loss at step 15250: 0.841531\n",
      "Minibatch accuracy: 71.9%\n",
      "Learning rate at step 15300: 0.00100000004750\n",
      "Minibatch loss at step 15300: 0.845140\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 15350: 0.00100000004750\n",
      "Minibatch loss at step 15350: 1.289116\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 15400: 0.00100000004750\n",
      "Minibatch loss at step 15400: 1.017581\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 15450: 0.00100000004750\n",
      "Minibatch loss at step 15450: 1.313577\n",
      "Minibatch accuracy: 70.3%\n",
      "Learning rate at step 15500: 0.00100000004750\n",
      "Minibatch loss at step 15500: 1.163494\n",
      "Minibatch accuracy: 71.9%\n",
      "validation accuracy: 78.5%\n",
      "Learning rate at step 15550: 0.00100000004750\n",
      "Minibatch loss at step 15550: 1.294069\n",
      "Minibatch accuracy: 65.6%\n",
      "Learning rate at step 15600: 0.00100000004750\n",
      "Minibatch loss at step 15600: 1.786726\n",
      "Minibatch accuracy: 67.2%\n",
      "Learning rate at step 15650: 0.00100000004750\n",
      "Minibatch loss at step 15650: 1.082935\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 15700: 0.00100000004750\n",
      "Minibatch loss at step 15700: 0.875323\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 15750: 0.00100000004750\n",
      "Minibatch loss at step 15750: 1.198418\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 15800: 0.00100000004750\n",
      "Minibatch loss at step 15800: 1.218941\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 15850: 0.00100000004750\n",
      "Minibatch loss at step 15850: 1.140492\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 15900: 0.00100000004750\n",
      "Minibatch loss at step 15900: 0.953650\n",
      "Minibatch accuracy: 71.9%\n",
      "Learning rate at step 15950: 0.00100000004750\n",
      "Minibatch loss at step 15950: 1.001844\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 16000: 0.00100000004750\n",
      "Minibatch loss at step 16000: 0.999387\n",
      "Minibatch accuracy: 76.6%\n",
      "validation accuracy: 78.9%\n",
      "Learning rate at step 16050: 0.00100000004750\n",
      "Minibatch loss at step 16050: 1.565584\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 16100: 0.00100000004750\n",
      "Minibatch loss at step 16100: 0.951068\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 16150: 0.00100000004750\n",
      "Minibatch loss at step 16150: 1.321285\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 16200: 0.00100000004750\n",
      "Minibatch loss at step 16200: 1.473098\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 16250: 0.00100000004750\n",
      "Minibatch loss at step 16250: 0.800248\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 16300: 0.00100000004750\n",
      "Minibatch loss at step 16300: 1.440584\n",
      "Minibatch accuracy: 70.3%\n",
      "Learning rate at step 16350: 0.00100000004750\n",
      "Minibatch loss at step 16350: 1.067946\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 16400: 0.00100000004750\n",
      "Minibatch loss at step 16400: 1.209284\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 16450: 0.00100000004750\n",
      "Minibatch loss at step 16450: 0.824049\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 16500: 0.00100000004750\n",
      "Minibatch loss at step 16500: 0.697392\n",
      "Minibatch accuracy: 82.8%\n",
      "validation accuracy: 79.2%\n",
      "Learning rate at step 16550: 0.00100000004750\n",
      "Minibatch loss at step 16550: 0.628936\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 16600: 0.00100000004750\n",
      "Minibatch loss at step 16600: 0.910165\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 16650: 0.00100000004750\n",
      "Minibatch loss at step 16650: 1.268798\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 16700: 0.00100000004750\n",
      "Minibatch loss at step 16700: 1.153299\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 16750: 0.00100000004750\n",
      "Minibatch loss at step 16750: 0.928523\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 16800: 0.00100000004750\n",
      "Minibatch loss at step 16800: 0.805100\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 16850: 0.00100000004750\n",
      "Minibatch loss at step 16850: 1.202043\n",
      "Minibatch accuracy: 71.9%\n",
      "Learning rate at step 16900: 0.00100000004750\n",
      "Minibatch loss at step 16900: 1.198959\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 16950: 0.00100000004750\n",
      "Minibatch loss at step 16950: 1.301929\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 17000: 0.00100000004750\n",
      "Minibatch loss at step 17000: 0.810420\n",
      "Minibatch accuracy: 82.8%\n",
      "validation accuracy: 79.6%\n",
      "Learning rate at step 17050: 0.00100000004750\n",
      "Minibatch loss at step 17050: 0.657638\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 17100: 0.00100000004750\n",
      "Minibatch loss at step 17100: 1.329050\n",
      "Minibatch accuracy: 67.2%\n",
      "Learning rate at step 17150: 0.00100000004750\n",
      "Minibatch loss at step 17150: 0.905670\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 17200: 0.00100000004750\n",
      "Minibatch loss at step 17200: 0.843491\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 17250: 0.00100000004750\n",
      "Minibatch loss at step 17250: 1.788102\n",
      "Minibatch accuracy: 70.3%\n",
      "Learning rate at step 17300: 0.00100000004750\n",
      "Minibatch loss at step 17300: 1.187685\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 17350: 0.00100000004750\n",
      "Minibatch loss at step 17350: 1.622172\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 17400: 0.00100000004750\n",
      "Minibatch loss at step 17400: 1.527129\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 17450: 0.00100000004750\n",
      "Minibatch loss at step 17450: 1.310310\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 17500: 0.00100000004750\n",
      "Minibatch loss at step 17500: 0.938488\n",
      "Minibatch accuracy: 78.1%\n",
      "validation accuracy: 80.4%\n",
      "Learning rate at step 17550: 0.00100000004750\n",
      "Minibatch loss at step 17550: 2.186584\n",
      "Minibatch accuracy: 70.3%\n",
      "Learning rate at step 17600: 0.00100000004750\n",
      "Minibatch loss at step 17600: 1.274696\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 17650: 0.00100000004750\n",
      "Minibatch loss at step 17650: 1.079664\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 17700: 0.00100000004750\n",
      "Minibatch loss at step 17700: 1.077560\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 17750: 0.00100000004750\n",
      "Minibatch loss at step 17750: 1.019403\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 17800: 0.00100000004750\n",
      "Minibatch loss at step 17800: 0.787941\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 17850: 0.00100000004750\n",
      "Minibatch loss at step 17850: 0.915575\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 17900: 0.00100000004750\n",
      "Minibatch loss at step 17900: 1.267427\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 17950: 0.00100000004750\n",
      "Minibatch loss at step 17950: 1.080140\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 18000: 0.00100000004750\n",
      "Minibatch loss at step 18000: 1.303458\n",
      "Minibatch accuracy: 70.3%\n",
      "validation accuracy: 81.2%\n",
      "Learning rate at step 18050: 0.00100000004750\n",
      "Minibatch loss at step 18050: 0.550654\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 18100: 0.00100000004750\n",
      "Minibatch loss at step 18100: 1.317330\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 18150: 0.00100000004750\n",
      "Minibatch loss at step 18150: 0.650483\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 18200: 0.00100000004750\n",
      "Minibatch loss at step 18200: 0.932982\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 18250: 0.00100000004750\n",
      "Minibatch loss at step 18250: 1.325590\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 18300: 0.00100000004750\n",
      "Minibatch loss at step 18300: 1.165822\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 18350: 0.00100000004750\n",
      "Minibatch loss at step 18350: 0.878072\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 18400: 0.00100000004750\n",
      "Minibatch loss at step 18400: 1.046185\n",
      "Minibatch accuracy: 81.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 18450: 0.00100000004750\n",
      "Minibatch loss at step 18450: 0.809168\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 18500: 0.00100000004750\n",
      "Minibatch loss at step 18500: 0.521139\n",
      "Minibatch accuracy: 84.4%\n",
      "validation accuracy: 82.2%\n",
      "Learning rate at step 18550: 0.00100000004750\n",
      "Minibatch loss at step 18550: 0.692658\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 18600: 0.00100000004750\n",
      "Minibatch loss at step 18600: 1.054101\n",
      "Minibatch accuracy: 70.3%\n",
      "Learning rate at step 18650: 0.00100000004750\n",
      "Minibatch loss at step 18650: 0.348534\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 18700: 0.00100000004750\n",
      "Minibatch loss at step 18700: 0.785688\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 18750: 0.00100000004750\n",
      "Minibatch loss at step 18750: 0.657283\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 18800: 0.00100000004750\n",
      "Minibatch loss at step 18800: 1.118273\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 18850: 0.00100000004750\n",
      "Minibatch loss at step 18850: 0.938361\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 18900: 0.00100000004750\n",
      "Minibatch loss at step 18900: 0.511724\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 18950: 0.00100000004750\n",
      "Minibatch loss at step 18950: 1.397323\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 19000: 0.00100000004750\n",
      "Minibatch loss at step 19000: 1.051010\n",
      "Minibatch accuracy: 78.1%\n",
      "validation accuracy: 82.6%\n",
      "Learning rate at step 19050: 0.00100000004750\n",
      "Minibatch loss at step 19050: 0.666198\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 19100: 0.00100000004750\n",
      "Minibatch loss at step 19100: 1.017795\n",
      "Minibatch accuracy: 70.3%\n",
      "Learning rate at step 19150: 0.00100000004750\n",
      "Minibatch loss at step 19150: 0.565433\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 19200: 0.00100000004750\n",
      "Minibatch loss at step 19200: 0.640469\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 19250: 0.00100000004750\n",
      "Minibatch loss at step 19250: 0.590369\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 19300: 0.00100000004750\n",
      "Minibatch loss at step 19300: 1.366860\n",
      "Minibatch accuracy: 67.2%\n",
      "Learning rate at step 19350: 0.00100000004750\n",
      "Minibatch loss at step 19350: 0.711606\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 19400: 0.00100000004750\n",
      "Minibatch loss at step 19400: 0.653211\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 19450: 0.00100000004750\n",
      "Minibatch loss at step 19450: 0.933018\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 19500: 0.00100000004750\n",
      "Minibatch loss at step 19500: 1.322315\n",
      "Minibatch accuracy: 78.1%\n",
      "validation accuracy: 82.2%\n",
      "Learning rate at step 19550: 0.00100000004750\n",
      "Minibatch loss at step 19550: 0.871332\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 19600: 0.00100000004750\n",
      "Minibatch loss at step 19600: 1.250781\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 19650: 0.00100000004750\n",
      "Minibatch loss at step 19650: 1.118815\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 19700: 0.00100000004750\n",
      "Minibatch loss at step 19700: 0.933160\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 19750: 0.00100000004750\n",
      "Minibatch loss at step 19750: 1.054794\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 19800: 0.00100000004750\n",
      "Minibatch loss at step 19800: 0.980871\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 19850: 0.00100000004750\n",
      "Minibatch loss at step 19850: 0.626808\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 19900: 0.00100000004750\n",
      "Minibatch loss at step 19900: 1.021531\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 19950: 0.00100000004750\n",
      "Minibatch loss at step 19950: 0.438170\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 20000: 0.00090000004275\n",
      "Minibatch loss at step 20000: 1.366886\n",
      "Minibatch accuracy: 78.1%\n",
      "validation accuracy: 83.1%\n",
      "Learning rate at step 20050: 0.00090000004275\n",
      "Minibatch loss at step 20050: 0.305178\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 20100: 0.00090000004275\n",
      "Minibatch loss at step 20100: 1.126592\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 20150: 0.00090000004275\n",
      "Minibatch loss at step 20150: 1.077992\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 20200: 0.00090000004275\n",
      "Minibatch loss at step 20200: 1.211174\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 20250: 0.00090000004275\n",
      "Minibatch loss at step 20250: 1.101501\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 20300: 0.00090000004275\n",
      "Minibatch loss at step 20300: 0.971497\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 20350: 0.00090000004275\n",
      "Minibatch loss at step 20350: 0.847223\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 20400: 0.00090000004275\n",
      "Minibatch loss at step 20400: 0.809444\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 20450: 0.00090000004275\n",
      "Minibatch loss at step 20450: 1.437233\n",
      "Minibatch accuracy: 70.3%\n",
      "Learning rate at step 20500: 0.00090000004275\n",
      "Minibatch loss at step 20500: 0.883787\n",
      "Minibatch accuracy: 73.4%\n",
      "validation accuracy: 81.7%\n",
      "Learning rate at step 20550: 0.00090000004275\n",
      "Minibatch loss at step 20550: 1.279894\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 20600: 0.00090000004275\n",
      "Minibatch loss at step 20600: 0.400519\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 20650: 0.00090000004275\n",
      "Minibatch loss at step 20650: 0.798668\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 20700: 0.00090000004275\n",
      "Minibatch loss at step 20700: 0.444204\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 20750: 0.00090000004275\n",
      "Minibatch loss at step 20750: 0.875454\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 20800: 0.00090000004275\n",
      "Minibatch loss at step 20800: 0.952989\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 20850: 0.00090000004275\n",
      "Minibatch loss at step 20850: 0.890000\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 20900: 0.00090000004275\n",
      "Minibatch loss at step 20900: 0.625209\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 20950: 0.00090000004275\n",
      "Minibatch loss at step 20950: 0.972082\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 21000: 0.00090000004275\n",
      "Minibatch loss at step 21000: 0.840836\n",
      "Minibatch accuracy: 79.7%\n",
      "validation accuracy: 83.5%\n",
      "Learning rate at step 21050: 0.00090000004275\n",
      "Minibatch loss at step 21050: 0.470098\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 21100: 0.00090000004275\n",
      "Minibatch loss at step 21100: 0.569888\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 21150: 0.00090000004275\n",
      "Minibatch loss at step 21150: 0.753295\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 21200: 0.00090000004275\n",
      "Minibatch loss at step 21200: 0.898809\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 21250: 0.00090000004275\n",
      "Minibatch loss at step 21250: 0.745443\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 21300: 0.00090000004275\n",
      "Minibatch loss at step 21300: 0.844264\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 21350: 0.00090000004275\n",
      "Minibatch loss at step 21350: 1.009017\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 21400: 0.00090000004275\n",
      "Minibatch loss at step 21400: 0.659797\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 21450: 0.00090000004275\n",
      "Minibatch loss at step 21450: 1.491191\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 21500: 0.00090000004275\n",
      "Minibatch loss at step 21500: 0.530208\n",
      "Minibatch accuracy: 85.9%\n",
      "validation accuracy: 84.2%\n",
      "Learning rate at step 21550: 0.00090000004275\n",
      "Minibatch loss at step 21550: 0.644508\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 21600: 0.00090000004275\n",
      "Minibatch loss at step 21600: 1.709975\n",
      "Minibatch accuracy: 65.6%\n",
      "Learning rate at step 21650: 0.00090000004275\n",
      "Minibatch loss at step 21650: 0.639074\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 21700: 0.00090000004275\n",
      "Minibatch loss at step 21700: 0.470389\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 21750: 0.00090000004275\n",
      "Minibatch loss at step 21750: 0.575139\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 21800: 0.00090000004275\n",
      "Minibatch loss at step 21800: 1.240214\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 21850: 0.00090000004275\n",
      "Minibatch loss at step 21850: 0.626499\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 21900: 0.00090000004275\n",
      "Minibatch loss at step 21900: 0.612332\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 21950: 0.00090000004275\n",
      "Minibatch loss at step 21950: 0.758105\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 22000: 0.00090000004275\n",
      "Minibatch loss at step 22000: 0.645887\n",
      "Minibatch accuracy: 82.8%\n",
      "validation accuracy: 84.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 22050: 0.00090000004275\n",
      "Minibatch loss at step 22050: 1.617312\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 22100: 0.00090000004275\n",
      "Minibatch loss at step 22100: 0.452940\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 22150: 0.00090000004275\n",
      "Minibatch loss at step 22150: 0.384307\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 22200: 0.00090000004275\n",
      "Minibatch loss at step 22200: 0.601068\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 22250: 0.00090000004275\n",
      "Minibatch loss at step 22250: 0.597205\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 22300: 0.00090000004275\n",
      "Minibatch loss at step 22300: 0.633422\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 22350: 0.00090000004275\n",
      "Minibatch loss at step 22350: 0.565217\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 22400: 0.00090000004275\n",
      "Minibatch loss at step 22400: 0.492881\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 22450: 0.00090000004275\n",
      "Minibatch loss at step 22450: 0.432630\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 22500: 0.00090000004275\n",
      "Minibatch loss at step 22500: 0.801369\n",
      "Minibatch accuracy: 82.8%\n",
      "validation accuracy: 84.9%\n",
      "Learning rate at step 22550: 0.00090000004275\n",
      "Minibatch loss at step 22550: 0.451921\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 22600: 0.00090000004275\n",
      "Minibatch loss at step 22600: 0.809500\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 22650: 0.00090000004275\n",
      "Minibatch loss at step 22650: 1.032144\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 22700: 0.00090000004275\n",
      "Minibatch loss at step 22700: 0.517067\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 22750: 0.00090000004275\n",
      "Minibatch loss at step 22750: 0.705283\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 22800: 0.00090000004275\n",
      "Minibatch loss at step 22800: 0.845809\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 22850: 0.00090000004275\n",
      "Minibatch loss at step 22850: 1.075364\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 22900: 0.00090000004275\n",
      "Minibatch loss at step 22900: 0.621610\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 22950: 0.00090000004275\n",
      "Minibatch loss at step 22950: 0.861229\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 23000: 0.00090000004275\n",
      "Minibatch loss at step 23000: 0.994759\n",
      "Minibatch accuracy: 81.2%\n",
      "validation accuracy: 84.9%\n",
      "Learning rate at step 23050: 0.00090000004275\n",
      "Minibatch loss at step 23050: 0.661025\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 23100: 0.00090000004275\n",
      "Minibatch loss at step 23100: 1.002957\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 23150: 0.00090000004275\n",
      "Minibatch loss at step 23150: 0.814793\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 23200: 0.00090000004275\n",
      "Minibatch loss at step 23200: 1.246406\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 23250: 0.00090000004275\n",
      "Minibatch loss at step 23250: 1.124021\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 23300: 0.00090000004275\n",
      "Minibatch loss at step 23300: 0.814385\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 23350: 0.00090000004275\n",
      "Minibatch loss at step 23350: 1.153647\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 23400: 0.00090000004275\n",
      "Minibatch loss at step 23400: 0.935050\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 23450: 0.00090000004275\n",
      "Minibatch loss at step 23450: 0.744853\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 23500: 0.00090000004275\n",
      "Minibatch loss at step 23500: 0.678306\n",
      "Minibatch accuracy: 79.7%\n",
      "validation accuracy: 84.6%\n",
      "Learning rate at step 23550: 0.00090000004275\n",
      "Minibatch loss at step 23550: 0.698613\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 23600: 0.00090000004275\n",
      "Minibatch loss at step 23600: 0.584103\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 23650: 0.00090000004275\n",
      "Minibatch loss at step 23650: 0.626337\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 23700: 0.00090000004275\n",
      "Minibatch loss at step 23700: 0.932500\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 23750: 0.00090000004275\n",
      "Minibatch loss at step 23750: 0.319570\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 23800: 0.00090000004275\n",
      "Minibatch loss at step 23800: 0.220067\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 23850: 0.00090000004275\n",
      "Minibatch loss at step 23850: 0.673694\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 23900: 0.00090000004275\n",
      "Minibatch loss at step 23900: 0.559103\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 23950: 0.00090000004275\n",
      "Minibatch loss at step 23950: 0.607863\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 24000: 0.00090000004275\n",
      "Minibatch loss at step 24000: 0.420742\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 85.3%\n",
      "Learning rate at step 24050: 0.00090000004275\n",
      "Minibatch loss at step 24050: 0.808317\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 24100: 0.00090000004275\n",
      "Minibatch loss at step 24100: 0.481403\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 24150: 0.00090000004275\n",
      "Minibatch loss at step 24150: 0.509461\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 24200: 0.00090000004275\n",
      "Minibatch loss at step 24200: 0.322117\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 24250: 0.00090000004275\n",
      "Minibatch loss at step 24250: 0.668962\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 24300: 0.00090000004275\n",
      "Minibatch loss at step 24300: 0.559375\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 24350: 0.00090000004275\n",
      "Minibatch loss at step 24350: 0.602681\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 24400: 0.00090000004275\n",
      "Minibatch loss at step 24400: 1.133731\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 24450: 0.00090000004275\n",
      "Minibatch loss at step 24450: 0.878744\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 24500: 0.00090000004275\n",
      "Minibatch loss at step 24500: 0.331066\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 85.6%\n",
      "Learning rate at step 24550: 0.00090000004275\n",
      "Minibatch loss at step 24550: 0.640691\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 24600: 0.00090000004275\n",
      "Minibatch loss at step 24600: 0.419336\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 24650: 0.00090000004275\n",
      "Minibatch loss at step 24650: 0.295843\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 24700: 0.00090000004275\n",
      "Minibatch loss at step 24700: 0.319561\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 24750: 0.00090000004275\n",
      "Minibatch loss at step 24750: 0.308387\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 24800: 0.00090000004275\n",
      "Minibatch loss at step 24800: 0.545129\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 24850: 0.00090000004275\n",
      "Minibatch loss at step 24850: 0.481625\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 24900: 0.00090000004275\n",
      "Minibatch loss at step 24900: 0.800534\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 24950: 0.00090000004275\n",
      "Minibatch loss at step 24950: 0.529949\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 25000: 0.00090000004275\n",
      "Minibatch loss at step 25000: 0.708215\n",
      "Minibatch accuracy: 82.8%\n",
      "validation accuracy: 85.3%\n",
      "Learning rate at step 25050: 0.00090000004275\n",
      "Minibatch loss at step 25050: 0.467812\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 25100: 0.00090000004275\n",
      "Minibatch loss at step 25100: 0.759328\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 25150: 0.00090000004275\n",
      "Minibatch loss at step 25150: 0.497554\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 25200: 0.00090000004275\n",
      "Minibatch loss at step 25200: 0.350146\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 25250: 0.00090000004275\n",
      "Minibatch loss at step 25250: 0.469809\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 25300: 0.00090000004275\n",
      "Minibatch loss at step 25300: 0.553536\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 25350: 0.00090000004275\n",
      "Minibatch loss at step 25350: 0.499505\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 25400: 0.00090000004275\n",
      "Minibatch loss at step 25400: 0.500072\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 25450: 0.00090000004275\n",
      "Minibatch loss at step 25450: 0.378997\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 25500: 0.00090000004275\n",
      "Minibatch loss at step 25500: 0.267741\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 85.2%\n",
      "Learning rate at step 25550: 0.00090000004275\n",
      "Minibatch loss at step 25550: 0.463201\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 25600: 0.00090000004275\n",
      "Minibatch loss at step 25600: 0.778348\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 25650: 0.00090000004275\n",
      "Minibatch loss at step 25650: 0.746021\n",
      "Minibatch accuracy: 82.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 25700: 0.00090000004275\n",
      "Minibatch loss at step 25700: 0.359737\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 25750: 0.00090000004275\n",
      "Minibatch loss at step 25750: 0.846277\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 25800: 0.00090000004275\n",
      "Minibatch loss at step 25800: 0.808570\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 25850: 0.00090000004275\n",
      "Minibatch loss at step 25850: 1.171904\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 25900: 0.00090000004275\n",
      "Minibatch loss at step 25900: 1.071466\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 25950: 0.00090000004275\n",
      "Minibatch loss at step 25950: 1.041431\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 26000: 0.00090000004275\n",
      "Minibatch loss at step 26000: 0.714814\n",
      "Minibatch accuracy: 82.8%\n",
      "validation accuracy: 84.9%\n",
      "Learning rate at step 26050: 0.00090000004275\n",
      "Minibatch loss at step 26050: 1.327975\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 26100: 0.00090000004275\n",
      "Minibatch loss at step 26100: 0.985991\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 26150: 0.00090000004275\n",
      "Minibatch loss at step 26150: 0.793320\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 26200: 0.00090000004275\n",
      "Minibatch loss at step 26200: 1.353447\n",
      "Minibatch accuracy: 71.9%\n",
      "Learning rate at step 26250: 0.00090000004275\n",
      "Minibatch loss at step 26250: 0.860580\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 26300: 0.00090000004275\n",
      "Minibatch loss at step 26300: 0.721506\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 26350: 0.00090000004275\n",
      "Minibatch loss at step 26350: 0.713607\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 26400: 0.00090000004275\n",
      "Minibatch loss at step 26400: 0.324411\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 26450: 0.00090000004275\n",
      "Minibatch loss at step 26450: 0.599356\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 26500: 0.00090000004275\n",
      "Minibatch loss at step 26500: 1.123888\n",
      "Minibatch accuracy: 82.8%\n",
      "validation accuracy: 85.8%\n",
      "Learning rate at step 26550: 0.00090000004275\n",
      "Minibatch loss at step 26550: 0.417878\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 26600: 0.00090000004275\n",
      "Minibatch loss at step 26600: 1.059651\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 26650: 0.00090000004275\n",
      "Minibatch loss at step 26650: 0.316680\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 26700: 0.00090000004275\n",
      "Minibatch loss at step 26700: 0.738859\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 26750: 0.00090000004275\n",
      "Minibatch loss at step 26750: 0.456644\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 26800: 0.00090000004275\n",
      "Minibatch loss at step 26800: 0.231630\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 26850: 0.00090000004275\n",
      "Minibatch loss at step 26850: 0.771086\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 26900: 0.00090000004275\n",
      "Minibatch loss at step 26900: 0.575782\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 26950: 0.00090000004275\n",
      "Minibatch loss at step 26950: 0.760723\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 27000: 0.00090000004275\n",
      "Minibatch loss at step 27000: 0.769690\n",
      "Minibatch accuracy: 84.4%\n",
      "validation accuracy: 86.3%\n",
      "Learning rate at step 27050: 0.00090000004275\n",
      "Minibatch loss at step 27050: 1.140802\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 27100: 0.00090000004275\n",
      "Minibatch loss at step 27100: 0.475216\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 27150: 0.00090000004275\n",
      "Minibatch loss at step 27150: 0.627507\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 27200: 0.00090000004275\n",
      "Minibatch loss at step 27200: 0.706063\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 27250: 0.00090000004275\n",
      "Minibatch loss at step 27250: 0.386470\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 27300: 0.00090000004275\n",
      "Minibatch loss at step 27300: 0.495555\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 27350: 0.00090000004275\n",
      "Minibatch loss at step 27350: 0.680662\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 27400: 0.00090000004275\n",
      "Minibatch loss at step 27400: 0.339233\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 27450: 0.00090000004275\n",
      "Minibatch loss at step 27450: 0.249847\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 27500: 0.00090000004275\n",
      "Minibatch loss at step 27500: 0.494484\n",
      "Minibatch accuracy: 87.5%\n",
      "validation accuracy: 87.1%\n",
      "Learning rate at step 27550: 0.00090000004275\n",
      "Minibatch loss at step 27550: 0.449802\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 27600: 0.00090000004275\n",
      "Minibatch loss at step 27600: 0.766673\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 27650: 0.00090000004275\n",
      "Minibatch loss at step 27650: 0.361145\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 27700: 0.00090000004275\n",
      "Minibatch loss at step 27700: 0.363996\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 27750: 0.00090000004275\n",
      "Minibatch loss at step 27750: 0.506663\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 27800: 0.00090000004275\n",
      "Minibatch loss at step 27800: 0.543430\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 27850: 0.00090000004275\n",
      "Minibatch loss at step 27850: 0.377845\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 27900: 0.00090000004275\n",
      "Minibatch loss at step 27900: 1.242013\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 27950: 0.00090000004275\n",
      "Minibatch loss at step 27950: 0.570406\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 28000: 0.00090000004275\n",
      "Minibatch loss at step 28000: 0.462055\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 87.0%\n",
      "Learning rate at step 28050: 0.00090000004275\n",
      "Minibatch loss at step 28050: 0.953592\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 28100: 0.00090000004275\n",
      "Minibatch loss at step 28100: 0.572450\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 28150: 0.00090000004275\n",
      "Minibatch loss at step 28150: 0.299601\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 28200: 0.00090000004275\n",
      "Minibatch loss at step 28200: 1.187603\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 28250: 0.00090000004275\n",
      "Minibatch loss at step 28250: 0.593819\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 28300: 0.00090000004275\n",
      "Minibatch loss at step 28300: 0.704035\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 28350: 0.00090000004275\n",
      "Minibatch loss at step 28350: 0.722129\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 28400: 0.00090000004275\n",
      "Minibatch loss at step 28400: 0.748351\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 28450: 0.00090000004275\n",
      "Minibatch loss at step 28450: 0.393670\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 28500: 0.00090000004275\n",
      "Minibatch loss at step 28500: 0.671235\n",
      "Minibatch accuracy: 84.4%\n",
      "validation accuracy: 86.5%\n",
      "Learning rate at step 28550: 0.00090000004275\n",
      "Minibatch loss at step 28550: 0.727506\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 28600: 0.00090000004275\n",
      "Minibatch loss at step 28600: 0.600883\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 28650: 0.00090000004275\n",
      "Minibatch loss at step 28650: 1.674320\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 28700: 0.00090000004275\n",
      "Minibatch loss at step 28700: 0.612877\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 28750: 0.00090000004275\n",
      "Minibatch loss at step 28750: 0.783043\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 28800: 0.00090000004275\n",
      "Minibatch loss at step 28800: 0.856665\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 28850: 0.00090000004275\n",
      "Minibatch loss at step 28850: 1.038737\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 28900: 0.00090000004275\n",
      "Minibatch loss at step 28900: 1.139165\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 28950: 0.00090000004275\n",
      "Minibatch loss at step 28950: 0.987779\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 29000: 0.00090000004275\n",
      "Minibatch loss at step 29000: 1.519071\n",
      "Minibatch accuracy: 73.4%\n",
      "validation accuracy: 86.4%\n",
      "Learning rate at step 29050: 0.00090000004275\n",
      "Minibatch loss at step 29050: 1.069412\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 29100: 0.00090000004275\n",
      "Minibatch loss at step 29100: 0.456114\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 29150: 0.00090000004275\n",
      "Minibatch loss at step 29150: 0.857745\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 29200: 0.00090000004275\n",
      "Minibatch loss at step 29200: 0.965561\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 29250: 0.00090000004275\n",
      "Minibatch loss at step 29250: 0.461174\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 29300: 0.00090000004275\n",
      "Minibatch loss at step 29300: 0.802742\n",
      "Minibatch accuracy: 81.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 29350: 0.00090000004275\n",
      "Minibatch loss at step 29350: 0.502986\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 29400: 0.00090000004275\n",
      "Minibatch loss at step 29400: 0.722604\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 29450: 0.00090000004275\n",
      "Minibatch loss at step 29450: 0.441272\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 29500: 0.00090000004275\n",
      "Minibatch loss at step 29500: 0.509532\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 87.1%\n",
      "Learning rate at step 29550: 0.00090000004275\n",
      "Minibatch loss at step 29550: 0.503283\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 29600: 0.00090000004275\n",
      "Minibatch loss at step 29600: 0.526098\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 29650: 0.00090000004275\n",
      "Minibatch loss at step 29650: 0.386698\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 29700: 0.00090000004275\n",
      "Minibatch loss at step 29700: 0.505426\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 29750: 0.00090000004275\n",
      "Minibatch loss at step 29750: 0.645744\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 29800: 0.00090000004275\n",
      "Minibatch loss at step 29800: 0.715171\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 29850: 0.00090000004275\n",
      "Minibatch loss at step 29850: 0.372721\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 29900: 0.00090000004275\n",
      "Minibatch loss at step 29900: 0.502890\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 29950: 0.00090000004275\n",
      "Minibatch loss at step 29950: 0.641154\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 30000: 0.00090000004275\n",
      "Minibatch loss at step 30000: 0.349414\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 86.6%\n",
      "Learning rate at step 30050: 0.00090000004275\n",
      "Minibatch loss at step 30050: 0.887478\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 30100: 0.00090000004275\n",
      "Minibatch loss at step 30100: 0.480472\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 30150: 0.00090000004275\n",
      "Minibatch loss at step 30150: 0.482569\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 30200: 0.00090000004275\n",
      "Minibatch loss at step 30200: 0.513072\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 30250: 0.00090000004275\n",
      "Minibatch loss at step 30250: 0.351057\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 30300: 0.00090000004275\n",
      "Minibatch loss at step 30300: 0.232428\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 30350: 0.00090000004275\n",
      "Minibatch loss at step 30350: 0.546855\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 30400: 0.00090000004275\n",
      "Minibatch loss at step 30400: 0.676886\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 30450: 0.00090000004275\n",
      "Minibatch loss at step 30450: 0.503382\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 30500: 0.00090000004275\n",
      "Minibatch loss at step 30500: 0.923706\n",
      "Minibatch accuracy: 78.1%\n",
      "validation accuracy: 86.9%\n",
      "Learning rate at step 30550: 0.00090000004275\n",
      "Minibatch loss at step 30550: 0.856651\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 30600: 0.00090000004275\n",
      "Minibatch loss at step 30600: 1.092715\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 30650: 0.00090000004275\n",
      "Minibatch loss at step 30650: 0.939042\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 30700: 0.00090000004275\n",
      "Minibatch loss at step 30700: 0.394002\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 30750: 0.00090000004275\n",
      "Minibatch loss at step 30750: 0.392845\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 30800: 0.00090000004275\n",
      "Minibatch loss at step 30800: 0.341841\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 30850: 0.00090000004275\n",
      "Minibatch loss at step 30850: 0.176178\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 30900: 0.00090000004275\n",
      "Minibatch loss at step 30900: 0.534232\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 30950: 0.00090000004275\n",
      "Minibatch loss at step 30950: 0.430795\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 31000: 0.00090000004275\n",
      "Minibatch loss at step 31000: 0.624161\n",
      "Minibatch accuracy: 85.9%\n",
      "validation accuracy: 87.8%\n",
      "Learning rate at step 31050: 0.00090000004275\n",
      "Minibatch loss at step 31050: 0.229373\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 31100: 0.00090000004275\n",
      "Minibatch loss at step 31100: 0.336760\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 31150: 0.00090000004275\n",
      "Minibatch loss at step 31150: 0.230481\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 31200: 0.00090000004275\n",
      "Minibatch loss at step 31200: 0.690766\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 31250: 0.00090000004275\n",
      "Minibatch loss at step 31250: 0.399949\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 31300: 0.00090000004275\n",
      "Minibatch loss at step 31300: 0.771842\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 31350: 0.00090000004275\n",
      "Minibatch loss at step 31350: 1.140022\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 31400: 0.00090000004275\n",
      "Minibatch loss at step 31400: 1.278835\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 31450: 0.00090000004275\n",
      "Minibatch loss at step 31450: 0.544566\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 31500: 0.00090000004275\n",
      "Minibatch loss at step 31500: 0.367576\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 87.0%\n",
      "Learning rate at step 31550: 0.00090000004275\n",
      "Minibatch loss at step 31550: 1.010821\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 31600: 0.00090000004275\n",
      "Minibatch loss at step 31600: 0.614933\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 31650: 0.00090000004275\n",
      "Minibatch loss at step 31650: 0.925241\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 31700: 0.00090000004275\n",
      "Minibatch loss at step 31700: 0.904236\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 31750: 0.00090000004275\n",
      "Minibatch loss at step 31750: 1.129482\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 31800: 0.00090000004275\n",
      "Minibatch loss at step 31800: 0.762564\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 31850: 0.00090000004275\n",
      "Minibatch loss at step 31850: 0.591265\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 31900: 0.00090000004275\n",
      "Minibatch loss at step 31900: 0.766985\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 31950: 0.00090000004275\n",
      "Minibatch loss at step 31950: 1.220557\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 32000: 0.00090000004275\n",
      "Minibatch loss at step 32000: 0.820837\n",
      "Minibatch accuracy: 81.2%\n",
      "validation accuracy: 87.3%\n",
      "Learning rate at step 32050: 0.00090000004275\n",
      "Minibatch loss at step 32050: 0.414383\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 32100: 0.00090000004275\n",
      "Minibatch loss at step 32100: 0.448693\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 32150: 0.00090000004275\n",
      "Minibatch loss at step 32150: 0.433739\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 32200: 0.00090000004275\n",
      "Minibatch loss at step 32200: 0.386257\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 32250: 0.00090000004275\n",
      "Minibatch loss at step 32250: 0.483076\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 32300: 0.00090000004275\n",
      "Minibatch loss at step 32300: 0.639650\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 32350: 0.00090000004275\n",
      "Minibatch loss at step 32350: 0.640651\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 32400: 0.00090000004275\n",
      "Minibatch loss at step 32400: 0.355533\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 32450: 0.00090000004275\n",
      "Minibatch loss at step 32450: 0.763134\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 32500: 0.00090000004275\n",
      "Minibatch loss at step 32500: 0.868167\n",
      "Minibatch accuracy: 87.5%\n",
      "validation accuracy: 87.5%\n",
      "Learning rate at step 32550: 0.00090000004275\n",
      "Minibatch loss at step 32550: 0.498071\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 32600: 0.00090000004275\n",
      "Minibatch loss at step 32600: 0.398415\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 32650: 0.00090000004275\n",
      "Minibatch loss at step 32650: 0.594054\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 32700: 0.00090000004275\n",
      "Minibatch loss at step 32700: 0.577286\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 32750: 0.00090000004275\n",
      "Minibatch loss at step 32750: 0.539337\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 32800: 0.00090000004275\n",
      "Minibatch loss at step 32800: 0.287329\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 32850: 0.00090000004275\n",
      "Minibatch loss at step 32850: 0.259707\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 32900: 0.00090000004275\n",
      "Minibatch loss at step 32900: 0.783590\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 32950: 0.00090000004275\n",
      "Minibatch loss at step 32950: 0.403615\n",
      "Minibatch accuracy: 92.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 33000: 0.00090000004275\n",
      "Minibatch loss at step 33000: 0.745196\n",
      "Minibatch accuracy: 81.2%\n",
      "validation accuracy: 87.3%\n",
      "Learning rate at step 33050: 0.00090000004275\n",
      "Minibatch loss at step 33050: 0.499364\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 33100: 0.00090000004275\n",
      "Minibatch loss at step 33100: 0.536817\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 33150: 0.00090000004275\n",
      "Minibatch loss at step 33150: 0.408314\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 33200: 0.00090000004275\n",
      "Minibatch loss at step 33200: 0.806414\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 33250: 0.00090000004275\n",
      "Minibatch loss at step 33250: 0.289397\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 33300: 0.00090000004275\n",
      "Minibatch loss at step 33300: 0.709347\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 33350: 0.00090000004275\n",
      "Minibatch loss at step 33350: 0.596631\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 33400: 0.00090000004275\n",
      "Minibatch loss at step 33400: 0.893881\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 33450: 0.00090000004275\n",
      "Minibatch loss at step 33450: 0.493056\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 33500: 0.00090000004275\n",
      "Minibatch loss at step 33500: 1.094764\n",
      "Minibatch accuracy: 81.2%\n",
      "validation accuracy: 87.7%\n",
      "Learning rate at step 33550: 0.00090000004275\n",
      "Minibatch loss at step 33550: 0.256377\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 33600: 0.00090000004275\n",
      "Minibatch loss at step 33600: 0.729622\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 33650: 0.00090000004275\n",
      "Minibatch loss at step 33650: 0.591565\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 33700: 0.00090000004275\n",
      "Minibatch loss at step 33700: 0.483745\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 33750: 0.00090000004275\n",
      "Minibatch loss at step 33750: 0.451345\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 33800: 0.00090000004275\n",
      "Minibatch loss at step 33800: 0.279556\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 33850: 0.00090000004275\n",
      "Minibatch loss at step 33850: 0.263548\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 33900: 0.00090000004275\n",
      "Minibatch loss at step 33900: 0.355460\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 33950: 0.00090000004275\n",
      "Minibatch loss at step 33950: 0.297770\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 34000: 0.00090000004275\n",
      "Minibatch loss at step 34000: 0.162524\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 87.7%\n",
      "Learning rate at step 34050: 0.00090000004275\n",
      "Minibatch loss at step 34050: 0.802156\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 34100: 0.00090000004275\n",
      "Minibatch loss at step 34100: 0.512543\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 34150: 0.00090000004275\n",
      "Minibatch loss at step 34150: 0.893386\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 34200: 0.00090000004275\n",
      "Minibatch loss at step 34200: 0.190069\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 34250: 0.00090000004275\n",
      "Minibatch loss at step 34250: 0.179961\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 34300: 0.00090000004275\n",
      "Minibatch loss at step 34300: 0.457568\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 34350: 0.00090000004275\n",
      "Minibatch loss at step 34350: 0.999069\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 34400: 0.00090000004275\n",
      "Minibatch loss at step 34400: 0.677672\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 34450: 0.00090000004275\n",
      "Minibatch loss at step 34450: 0.793164\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 34500: 0.00090000004275\n",
      "Minibatch loss at step 34500: 1.041182\n",
      "Minibatch accuracy: 81.2%\n",
      "validation accuracy: 87.2%\n",
      "Learning rate at step 34550: 0.00090000004275\n",
      "Minibatch loss at step 34550: 0.839055\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 34600: 0.00090000004275\n",
      "Minibatch loss at step 34600: 0.868592\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 34650: 0.00090000004275\n",
      "Minibatch loss at step 34650: 1.029698\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 34700: 0.00090000004275\n",
      "Minibatch loss at step 34700: 0.810893\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 34750: 0.00090000004275\n",
      "Minibatch loss at step 34750: 0.500477\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 34800: 0.00090000004275\n",
      "Minibatch loss at step 34800: 0.793697\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 34850: 0.00090000004275\n",
      "Minibatch loss at step 34850: 0.487813\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 34900: 0.00090000004275\n",
      "Minibatch loss at step 34900: 0.932456\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 34950: 0.00090000004275\n",
      "Minibatch loss at step 34950: 0.390621\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 35000: 0.00090000004275\n",
      "Minibatch loss at step 35000: 0.216238\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 88.0%\n",
      "Learning rate at step 35050: 0.00090000004275\n",
      "Minibatch loss at step 35050: 0.334319\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 35100: 0.00090000004275\n",
      "Minibatch loss at step 35100: 0.236695\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 35150: 0.00090000004275\n",
      "Minibatch loss at step 35150: 0.442870\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 35200: 0.00090000004275\n",
      "Minibatch loss at step 35200: 0.307090\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 35250: 0.00090000004275\n",
      "Minibatch loss at step 35250: 0.454023\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 35300: 0.00090000004275\n",
      "Minibatch loss at step 35300: 0.643602\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 35350: 0.00090000004275\n",
      "Minibatch loss at step 35350: 0.152387\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 35400: 0.00090000004275\n",
      "Minibatch loss at step 35400: 0.332103\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 35450: 0.00090000004275\n",
      "Minibatch loss at step 35450: 0.856283\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 35500: 0.00090000004275\n",
      "Minibatch loss at step 35500: 0.604894\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 88.0%\n",
      "Learning rate at step 35550: 0.00090000004275\n",
      "Minibatch loss at step 35550: 0.646020\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 35600: 0.00090000004275\n",
      "Minibatch loss at step 35600: 0.496878\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 35650: 0.00090000004275\n",
      "Minibatch loss at step 35650: 0.791735\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 35700: 0.00090000004275\n",
      "Minibatch loss at step 35700: 0.602817\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 35750: 0.00090000004275\n",
      "Minibatch loss at step 35750: 0.540023\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 35800: 0.00090000004275\n",
      "Minibatch loss at step 35800: 0.318223\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 35850: 0.00090000004275\n",
      "Minibatch loss at step 35850: 0.872714\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 35900: 0.00090000004275\n",
      "Minibatch loss at step 35900: 0.308764\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 35950: 0.00090000004275\n",
      "Minibatch loss at step 35950: 0.305643\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 36000: 0.00090000004275\n",
      "Minibatch loss at step 36000: 0.264599\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 88.4%\n",
      "Learning rate at step 36050: 0.00090000004275\n",
      "Minibatch loss at step 36050: 0.512175\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 36100: 0.00090000004275\n",
      "Minibatch loss at step 36100: 0.672067\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 36150: 0.00090000004275\n",
      "Minibatch loss at step 36150: 0.721341\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 36200: 0.00090000004275\n",
      "Minibatch loss at step 36200: 0.390055\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 36250: 0.00090000004275\n",
      "Minibatch loss at step 36250: 0.312952\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 36300: 0.00090000004275\n",
      "Minibatch loss at step 36300: 0.412056\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 36350: 0.00090000004275\n",
      "Minibatch loss at step 36350: 0.318634\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 36400: 0.00090000004275\n",
      "Minibatch loss at step 36400: 0.257796\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 36450: 0.00090000004275\n",
      "Minibatch loss at step 36450: 0.425036\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 36500: 0.00090000004275\n",
      "Minibatch loss at step 36500: 1.091168\n",
      "Minibatch accuracy: 87.5%\n",
      "validation accuracy: 87.6%\n",
      "Learning rate at step 36550: 0.00090000004275\n",
      "Minibatch loss at step 36550: 0.511137\n",
      "Minibatch accuracy: 89.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 36600: 0.00090000004275\n",
      "Minibatch loss at step 36600: 0.267450\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 36650: 0.00090000004275\n",
      "Minibatch loss at step 36650: 0.353081\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 36700: 0.00090000004275\n",
      "Minibatch loss at step 36700: 0.440220\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 36750: 0.00090000004275\n",
      "Minibatch loss at step 36750: 0.474343\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 36800: 0.00090000004275\n",
      "Minibatch loss at step 36800: 0.312076\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 36850: 0.00090000004275\n",
      "Minibatch loss at step 36850: 0.087042\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 36900: 0.00090000004275\n",
      "Minibatch loss at step 36900: 0.267811\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 36950: 0.00090000004275\n",
      "Minibatch loss at step 36950: 0.886138\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 37000: 0.00090000004275\n",
      "Minibatch loss at step 37000: 0.374906\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 88.3%\n",
      "Learning rate at step 37050: 0.00090000004275\n",
      "Minibatch loss at step 37050: 0.324939\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 37100: 0.00090000004275\n",
      "Minibatch loss at step 37100: 0.818388\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 37150: 0.00090000004275\n",
      "Minibatch loss at step 37150: 0.449519\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 37200: 0.00090000004275\n",
      "Minibatch loss at step 37200: 0.097738\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 37250: 0.00090000004275\n",
      "Minibatch loss at step 37250: 0.730402\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 37300: 0.00090000004275\n",
      "Minibatch loss at step 37300: 0.631997\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 37350: 0.00090000004275\n",
      "Minibatch loss at step 37350: 1.218717\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 37400: 0.00090000004275\n",
      "Minibatch loss at step 37400: 0.911245\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 37450: 0.00090000004275\n",
      "Minibatch loss at step 37450: 1.004377\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 37500: 0.00090000004275\n",
      "Minibatch loss at step 37500: 0.698551\n",
      "Minibatch accuracy: 78.1%\n",
      "validation accuracy: 87.6%\n",
      "Learning rate at step 37550: 0.00090000004275\n",
      "Minibatch loss at step 37550: 0.906575\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 37600: 0.00090000004275\n",
      "Minibatch loss at step 37600: 0.952059\n",
      "Minibatch accuracy: 68.8%\n",
      "Learning rate at step 37650: 0.00090000004275\n",
      "Minibatch loss at step 37650: 0.649012\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 37700: 0.00090000004275\n",
      "Minibatch loss at step 37700: 0.565101\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 37750: 0.00090000004275\n",
      "Minibatch loss at step 37750: 0.381056\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 37800: 0.00090000004275\n",
      "Minibatch loss at step 37800: 0.485607\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 37850: 0.00090000004275\n",
      "Minibatch loss at step 37850: 0.194899\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 37900: 0.00090000004275\n",
      "Minibatch loss at step 37900: 0.619418\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 37950: 0.00090000004275\n",
      "Minibatch loss at step 37950: 0.070302\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 38000: 0.00090000004275\n",
      "Minibatch loss at step 38000: 0.404587\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 87.4%\n",
      "Learning rate at step 38050: 0.00090000004275\n",
      "Minibatch loss at step 38050: 0.453245\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 38100: 0.00090000004275\n",
      "Minibatch loss at step 38100: 0.358625\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 38150: 0.00090000004275\n",
      "Minibatch loss at step 38150: 0.310210\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 38200: 0.00090000004275\n",
      "Minibatch loss at step 38200: 0.351674\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 38250: 0.00090000004275\n",
      "Minibatch loss at step 38250: 0.609697\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 38300: 0.00090000004275\n",
      "Minibatch loss at step 38300: 0.370524\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 38350: 0.00090000004275\n",
      "Minibatch loss at step 38350: 0.749337\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 38400: 0.00090000004275\n",
      "Minibatch loss at step 38400: 0.612386\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 38450: 0.00090000004275\n",
      "Minibatch loss at step 38450: 0.337883\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 38500: 0.00090000004275\n",
      "Minibatch loss at step 38500: 0.257932\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 88.6%\n",
      "Learning rate at step 38550: 0.00090000004275\n",
      "Minibatch loss at step 38550: 0.388374\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 38600: 0.00090000004275\n",
      "Minibatch loss at step 38600: 0.834900\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 38650: 0.00090000004275\n",
      "Minibatch loss at step 38650: 0.472580\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 38700: 0.00090000004275\n",
      "Minibatch loss at step 38700: 0.426321\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 38750: 0.00090000004275\n",
      "Minibatch loss at step 38750: 0.433859\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 38800: 0.00090000004275\n",
      "Minibatch loss at step 38800: 0.477672\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 38850: 0.00090000004275\n",
      "Minibatch loss at step 38850: 0.445132\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 38900: 0.00090000004275\n",
      "Minibatch loss at step 38900: 0.291862\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 38950: 0.00090000004275\n",
      "Minibatch loss at step 38950: 0.331178\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 39000: 0.00090000004275\n",
      "Minibatch loss at step 39000: 0.283089\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 88.2%\n",
      "Learning rate at step 39050: 0.00090000004275\n",
      "Minibatch loss at step 39050: 0.443260\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 39100: 0.00090000004275\n",
      "Minibatch loss at step 39100: 0.267753\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 39150: 0.00090000004275\n",
      "Minibatch loss at step 39150: 0.605179\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 39200: 0.00090000004275\n",
      "Minibatch loss at step 39200: 0.449174\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 39250: 0.00090000004275\n",
      "Minibatch loss at step 39250: 0.297283\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 39300: 0.00090000004275\n",
      "Minibatch loss at step 39300: 0.191178\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 39350: 0.00090000004275\n",
      "Minibatch loss at step 39350: 0.772901\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 39400: 0.00090000004275\n",
      "Minibatch loss at step 39400: 0.511720\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 39450: 0.00090000004275\n",
      "Minibatch loss at step 39450: 0.510993\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 39500: 0.00090000004275\n",
      "Minibatch loss at step 39500: 0.366895\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 88.1%\n",
      "Learning rate at step 39550: 0.00090000004275\n",
      "Minibatch loss at step 39550: 0.231768\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 39600: 0.00090000004275\n",
      "Minibatch loss at step 39600: 0.567166\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 39650: 0.00090000004275\n",
      "Minibatch loss at step 39650: 0.364896\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 39700: 0.00090000004275\n",
      "Minibatch loss at step 39700: 0.438805\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 39750: 0.00090000004275\n",
      "Minibatch loss at step 39750: 0.142325\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 39800: 0.00090000004275\n",
      "Minibatch loss at step 39800: 0.592860\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 39850: 0.00090000004275\n",
      "Minibatch loss at step 39850: 0.247609\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 39900: 0.00090000004275\n",
      "Minibatch loss at step 39900: 0.370444\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 39950: 0.00090000004275\n",
      "Minibatch loss at step 39950: 0.337255\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 40000: 0.00080999999773\n",
      "Minibatch loss at step 40000: 0.693224\n",
      "Minibatch accuracy: 87.5%\n",
      "validation accuracy: 87.9%\n",
      "Learning rate at step 40050: 0.00080999999773\n",
      "Minibatch loss at step 40050: 0.554204\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 40100: 0.00080999999773\n",
      "Minibatch loss at step 40100: 0.149284\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 40150: 0.00080999999773\n",
      "Minibatch loss at step 40150: 0.722101\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 40200: 0.00080999999773\n",
      "Minibatch loss at step 40200: 0.829090\n",
      "Minibatch accuracy: 82.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 40250: 0.00080999999773\n",
      "Minibatch loss at step 40250: 0.634801\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 40300: 0.00080999999773\n",
      "Minibatch loss at step 40300: 0.653791\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 40350: 0.00080999999773\n",
      "Minibatch loss at step 40350: 0.348403\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 40400: 0.00080999999773\n",
      "Minibatch loss at step 40400: 0.748166\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 40450: 0.00080999999773\n",
      "Minibatch loss at step 40450: 0.899837\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 40500: 0.00080999999773\n",
      "Minibatch loss at step 40500: 0.852493\n",
      "Minibatch accuracy: 84.4%\n",
      "validation accuracy: 88.5%\n",
      "Learning rate at step 40550: 0.00080999999773\n",
      "Minibatch loss at step 40550: 0.496709\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 40600: 0.00080999999773\n",
      "Minibatch loss at step 40600: 0.669402\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 40650: 0.00080999999773\n",
      "Minibatch loss at step 40650: 0.531066\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 40700: 0.00080999999773\n",
      "Minibatch loss at step 40700: 0.526545\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 40750: 0.00080999999773\n",
      "Minibatch loss at step 40750: 0.516513\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 40800: 0.00080999999773\n",
      "Minibatch loss at step 40800: 0.183466\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 40850: 0.00080999999773\n",
      "Minibatch loss at step 40850: 0.213920\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 40900: 0.00080999999773\n",
      "Minibatch loss at step 40900: 0.484025\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 40950: 0.00080999999773\n",
      "Minibatch loss at step 40950: 0.412558\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 41000: 0.00080999999773\n",
      "Minibatch loss at step 41000: 0.288163\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 88.3%\n",
      "Learning rate at step 41050: 0.00080999999773\n",
      "Minibatch loss at step 41050: 0.472307\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 41100: 0.00080999999773\n",
      "Minibatch loss at step 41100: 0.074162\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 41150: 0.00080999999773\n",
      "Minibatch loss at step 41150: 0.600284\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 41200: 0.00080999999773\n",
      "Minibatch loss at step 41200: 0.570248\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 41250: 0.00080999999773\n",
      "Minibatch loss at step 41250: 0.693481\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 41300: 0.00080999999773\n",
      "Minibatch loss at step 41300: 0.612041\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 41350: 0.00080999999773\n",
      "Minibatch loss at step 41350: 0.445400\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 41400: 0.00080999999773\n",
      "Minibatch loss at step 41400: 0.309394\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 41450: 0.00080999999773\n",
      "Minibatch loss at step 41450: 0.355870\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 41500: 0.00080999999773\n",
      "Minibatch loss at step 41500: 0.326538\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 88.7%\n",
      "Learning rate at step 41550: 0.00080999999773\n",
      "Minibatch loss at step 41550: 0.702455\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 41600: 0.00080999999773\n",
      "Minibatch loss at step 41600: 1.124994\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 41650: 0.00080999999773\n",
      "Minibatch loss at step 41650: 0.166937\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 41700: 0.00080999999773\n",
      "Minibatch loss at step 41700: 0.813383\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 41750: 0.00080999999773\n",
      "Minibatch loss at step 41750: 0.515276\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 41800: 0.00080999999773\n",
      "Minibatch loss at step 41800: 0.394735\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 41850: 0.00080999999773\n",
      "Minibatch loss at step 41850: 0.942308\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 41900: 0.00080999999773\n",
      "Minibatch loss at step 41900: 0.691384\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 41950: 0.00080999999773\n",
      "Minibatch loss at step 41950: 0.473664\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 42000: 0.00080999999773\n",
      "Minibatch loss at step 42000: 0.219300\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 88.4%\n",
      "Learning rate at step 42050: 0.00080999999773\n",
      "Minibatch loss at step 42050: 0.706447\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 42100: 0.00080999999773\n",
      "Minibatch loss at step 42100: 0.910169\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 42150: 0.00080999999773\n",
      "Minibatch loss at step 42150: 0.579518\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 42200: 0.00080999999773\n",
      "Minibatch loss at step 42200: 0.358284\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 42250: 0.00080999999773\n",
      "Minibatch loss at step 42250: 0.305183\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 42300: 0.00080999999773\n",
      "Minibatch loss at step 42300: 0.222447\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 42350: 0.00080999999773\n",
      "Minibatch loss at step 42350: 0.202051\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 42400: 0.00080999999773\n",
      "Minibatch loss at step 42400: 0.281296\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 42450: 0.00080999999773\n",
      "Minibatch loss at step 42450: 0.267402\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 42500: 0.00080999999773\n",
      "Minibatch loss at step 42500: 0.518687\n",
      "Minibatch accuracy: 87.5%\n",
      "validation accuracy: 88.3%\n",
      "Learning rate at step 42550: 0.00080999999773\n",
      "Minibatch loss at step 42550: 0.315079\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 42600: 0.00080999999773\n",
      "Minibatch loss at step 42600: 0.390401\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 42650: 0.00080999999773\n",
      "Minibatch loss at step 42650: 0.822686\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 42700: 0.00080999999773\n",
      "Minibatch loss at step 42700: 0.418406\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 42750: 0.00080999999773\n",
      "Minibatch loss at step 42750: 0.327851\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 42800: 0.00080999999773\n",
      "Minibatch loss at step 42800: 0.216672\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 42850: 0.00080999999773\n",
      "Minibatch loss at step 42850: 0.232096\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 42900: 0.00080999999773\n",
      "Minibatch loss at step 42900: 0.232567\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 42950: 0.00080999999773\n",
      "Minibatch loss at step 42950: 0.205722\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 43000: 0.00080999999773\n",
      "Minibatch loss at step 43000: 0.636869\n",
      "Minibatch accuracy: 78.1%\n",
      "validation accuracy: 87.8%\n",
      "Learning rate at step 43050: 0.00080999999773\n",
      "Minibatch loss at step 43050: 0.613216\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 43100: 0.00080999999773\n",
      "Minibatch loss at step 43100: 1.215479\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 43150: 0.00080999999773\n",
      "Minibatch loss at step 43150: 0.410738\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 43200: 0.00080999999773\n",
      "Minibatch loss at step 43200: 1.767001\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 43250: 0.00080999999773\n",
      "Minibatch loss at step 43250: 0.799376\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 43300: 0.00080999999773\n",
      "Minibatch loss at step 43300: 0.501323\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 43350: 0.00080999999773\n",
      "Minibatch loss at step 43350: 0.701727\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 43400: 0.00080999999773\n",
      "Minibatch loss at step 43400: 0.540518\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 43450: 0.00080999999773\n",
      "Minibatch loss at step 43450: 0.766034\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 43500: 0.00080999999773\n",
      "Minibatch loss at step 43500: 0.560617\n",
      "Minibatch accuracy: 85.9%\n",
      "validation accuracy: 88.6%\n",
      "Learning rate at step 43550: 0.00080999999773\n",
      "Minibatch loss at step 43550: 0.587313\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 43600: 0.00080999999773\n",
      "Minibatch loss at step 43600: 0.068044\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 43650: 0.00080999999773\n",
      "Minibatch loss at step 43650: 0.340958\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 43700: 0.00080999999773\n",
      "Minibatch loss at step 43700: 0.472297\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 43750: 0.00080999999773\n",
      "Minibatch loss at step 43750: 0.276709\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 43800: 0.00080999999773\n",
      "Minibatch loss at step 43800: 0.171816\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 43850: 0.00080999999773\n",
      "Minibatch loss at step 43850: 0.495831\n",
      "Minibatch accuracy: 92.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 43900: 0.00080999999773\n",
      "Minibatch loss at step 43900: 0.306218\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 43950: 0.00080999999773\n",
      "Minibatch loss at step 43950: 0.204306\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 44000: 0.00080999999773\n",
      "Minibatch loss at step 44000: 0.636759\n",
      "Minibatch accuracy: 87.5%\n",
      "validation accuracy: 88.9%\n",
      "Learning rate at step 44050: 0.00080999999773\n",
      "Minibatch loss at step 44050: 0.227855\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 44100: 0.00080999999773\n",
      "Minibatch loss at step 44100: 0.249930\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 44150: 0.00080999999773\n",
      "Minibatch loss at step 44150: 0.113334\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 44200: 0.00080999999773\n",
      "Minibatch loss at step 44200: 0.278801\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 44250: 0.00080999999773\n",
      "Minibatch loss at step 44250: 0.262541\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 44300: 0.00080999999773\n",
      "Minibatch loss at step 44300: 0.322780\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 44350: 0.00080999999773\n",
      "Minibatch loss at step 44350: 0.229432\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 44400: 0.00080999999773\n",
      "Minibatch loss at step 44400: 0.254769\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 44450: 0.00080999999773\n",
      "Minibatch loss at step 44450: 0.719104\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 44500: 0.00080999999773\n",
      "Minibatch loss at step 44500: 0.261317\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 88.6%\n",
      "Learning rate at step 44550: 0.00080999999773\n",
      "Minibatch loss at step 44550: 0.219780\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 44600: 0.00080999999773\n",
      "Minibatch loss at step 44600: 0.209092\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 44650: 0.00080999999773\n",
      "Minibatch loss at step 44650: 0.200652\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 44700: 0.00080999999773\n",
      "Minibatch loss at step 44700: 0.085654\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 44750: 0.00080999999773\n",
      "Minibatch loss at step 44750: 0.739274\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 44800: 0.00080999999773\n",
      "Minibatch loss at step 44800: 0.298582\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 44850: 0.00080999999773\n",
      "Minibatch loss at step 44850: 0.237801\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 44900: 0.00080999999773\n",
      "Minibatch loss at step 44900: 0.287143\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 44950: 0.00080999999773\n",
      "Minibatch loss at step 44950: 0.119433\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 45000: 0.00080999999773\n",
      "Minibatch loss at step 45000: 0.707656\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 88.6%\n",
      "Learning rate at step 45050: 0.00080999999773\n",
      "Minibatch loss at step 45050: 0.269329\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 45100: 0.00080999999773\n",
      "Minibatch loss at step 45100: 0.641195\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 45150: 0.00080999999773\n",
      "Minibatch loss at step 45150: 0.489238\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 45200: 0.00080999999773\n",
      "Minibatch loss at step 45200: 0.230133\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 45250: 0.00080999999773\n",
      "Minibatch loss at step 45250: 0.402630\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 45300: 0.00080999999773\n",
      "Minibatch loss at step 45300: 0.297249\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 45350: 0.00080999999773\n",
      "Minibatch loss at step 45350: 0.172963\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 45400: 0.00080999999773\n",
      "Minibatch loss at step 45400: 0.163875\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 45450: 0.00080999999773\n",
      "Minibatch loss at step 45450: 0.205323\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 45500: 0.00080999999773\n",
      "Minibatch loss at step 45500: 0.134973\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 88.7%\n",
      "Learning rate at step 45550: 0.00080999999773\n",
      "Minibatch loss at step 45550: 0.680101\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 45600: 0.00080999999773\n",
      "Minibatch loss at step 45600: 0.266342\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 45650: 0.00080999999773\n",
      "Minibatch loss at step 45650: 0.208737\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 45700: 0.00080999999773\n",
      "Minibatch loss at step 45700: 0.226297\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 45750: 0.00080999999773\n",
      "Minibatch loss at step 45750: 0.274595\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 45800: 0.00080999999773\n",
      "Minibatch loss at step 45800: 0.170645\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 45850: 0.00080999999773\n",
      "Minibatch loss at step 45850: 0.907477\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 45900: 0.00080999999773\n",
      "Minibatch loss at step 45900: 0.879297\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 45950: 0.00080999999773\n",
      "Minibatch loss at step 45950: 0.650427\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 46000: 0.00080999999773\n",
      "Minibatch loss at step 46000: 0.600709\n",
      "Minibatch accuracy: 81.2%\n",
      "validation accuracy: 88.8%\n",
      "Learning rate at step 46050: 0.00080999999773\n",
      "Minibatch loss at step 46050: 0.814962\n",
      "Minibatch accuracy: 75.0%\n",
      "Learning rate at step 46100: 0.00080999999773\n",
      "Minibatch loss at step 46100: 0.781881\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 46150: 0.00080999999773\n",
      "Minibatch loss at step 46150: 0.861941\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 46200: 0.00080999999773\n",
      "Minibatch loss at step 46200: 0.589093\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 46250: 0.00080999999773\n",
      "Minibatch loss at step 46250: 0.684893\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 46300: 0.00080999999773\n",
      "Minibatch loss at step 46300: 0.528427\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 46350: 0.00080999999773\n",
      "Minibatch loss at step 46350: 0.335453\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 46400: 0.00080999999773\n",
      "Minibatch loss at step 46400: 0.217361\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 46450: 0.00080999999773\n",
      "Minibatch loss at step 46450: 0.264978\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 46500: 0.00080999999773\n",
      "Minibatch loss at step 46500: 0.296002\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 88.6%\n",
      "Learning rate at step 46550: 0.00080999999773\n",
      "Minibatch loss at step 46550: 0.302976\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 46600: 0.00080999999773\n",
      "Minibatch loss at step 46600: 0.340123\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 46650: 0.00080999999773\n",
      "Minibatch loss at step 46650: 0.220127\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 46700: 0.00080999999773\n",
      "Minibatch loss at step 46700: 0.187159\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 46750: 0.00080999999773\n",
      "Minibatch loss at step 46750: 0.349764\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 46800: 0.00080999999773\n",
      "Minibatch loss at step 46800: 1.387267\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 46850: 0.00080999999773\n",
      "Minibatch loss at step 46850: 0.248169\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 46900: 0.00080999999773\n",
      "Minibatch loss at step 46900: 0.392103\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 46950: 0.00080999999773\n",
      "Minibatch loss at step 46950: 0.275341\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 47000: 0.00080999999773\n",
      "Minibatch loss at step 47000: 0.444497\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 88.5%\n",
      "Learning rate at step 47050: 0.00080999999773\n",
      "Minibatch loss at step 47050: 0.239082\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 47100: 0.00080999999773\n",
      "Minibatch loss at step 47100: 0.384540\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 47150: 0.00080999999773\n",
      "Minibatch loss at step 47150: 0.372378\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 47200: 0.00080999999773\n",
      "Minibatch loss at step 47200: 0.298438\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 47250: 0.00080999999773\n",
      "Minibatch loss at step 47250: 0.314247\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 47300: 0.00080999999773\n",
      "Minibatch loss at step 47300: 0.264474\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 47350: 0.00080999999773\n",
      "Minibatch loss at step 47350: 0.056687\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 47400: 0.00080999999773\n",
      "Minibatch loss at step 47400: 0.349266\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 47450: 0.00080999999773\n",
      "Minibatch loss at step 47450: 0.154245\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 47500: 0.00080999999773\n",
      "Minibatch loss at step 47500: 0.225715\n",
      "Minibatch accuracy: 90.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 88.7%\n",
      "Learning rate at step 47550: 0.00080999999773\n",
      "Minibatch loss at step 47550: 0.549829\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 47600: 0.00080999999773\n",
      "Minibatch loss at step 47600: 0.519712\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 47650: 0.00080999999773\n",
      "Minibatch loss at step 47650: 0.210158\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 47700: 0.00080999999773\n",
      "Minibatch loss at step 47700: 0.254065\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 47750: 0.00080999999773\n",
      "Minibatch loss at step 47750: 0.435417\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 47800: 0.00080999999773\n",
      "Minibatch loss at step 47800: 0.750622\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 47850: 0.00080999999773\n",
      "Minibatch loss at step 47850: 0.170577\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 47900: 0.00080999999773\n",
      "Minibatch loss at step 47900: 0.497707\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 47950: 0.00080999999773\n",
      "Minibatch loss at step 47950: 0.890023\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 48000: 0.00080999999773\n",
      "Minibatch loss at step 48000: 0.186202\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.0%\n",
      "Learning rate at step 48050: 0.00080999999773\n",
      "Minibatch loss at step 48050: 0.508919\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 48100: 0.00080999999773\n",
      "Minibatch loss at step 48100: 0.428867\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 48150: 0.00080999999773\n",
      "Minibatch loss at step 48150: 0.143636\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 48200: 0.00080999999773\n",
      "Minibatch loss at step 48200: 0.290652\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 48250: 0.00080999999773\n",
      "Minibatch loss at step 48250: 0.448245\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 48300: 0.00080999999773\n",
      "Minibatch loss at step 48300: 0.434362\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 48350: 0.00080999999773\n",
      "Minibatch loss at step 48350: 0.438729\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 48400: 0.00080999999773\n",
      "Minibatch loss at step 48400: 0.101569\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 48450: 0.00080999999773\n",
      "Minibatch loss at step 48450: 0.579211\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 48500: 0.00080999999773\n",
      "Minibatch loss at step 48500: 0.319604\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 88.6%\n",
      "Learning rate at step 48550: 0.00080999999773\n",
      "Minibatch loss at step 48550: 0.253084\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 48600: 0.00080999999773\n",
      "Minibatch loss at step 48600: 0.293590\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 48650: 0.00080999999773\n",
      "Minibatch loss at step 48650: 0.157475\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 48700: 0.00080999999773\n",
      "Minibatch loss at step 48700: 0.504453\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 48750: 0.00080999999773\n",
      "Minibatch loss at step 48750: 0.481792\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 48800: 0.00080999999773\n",
      "Minibatch loss at step 48800: 0.822414\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 48850: 0.00080999999773\n",
      "Minibatch loss at step 48850: 0.852762\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 48900: 0.00080999999773\n",
      "Minibatch loss at step 48900: 0.348326\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 48950: 0.00080999999773\n",
      "Minibatch loss at step 48950: 0.612725\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 49000: 0.00080999999773\n",
      "Minibatch loss at step 49000: 0.685155\n",
      "Minibatch accuracy: 85.9%\n",
      "validation accuracy: 88.2%\n",
      "Learning rate at step 49050: 0.00080999999773\n",
      "Minibatch loss at step 49050: 1.152309\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 49100: 0.00080999999773\n",
      "Minibatch loss at step 49100: 0.460410\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 49150: 0.00080999999773\n",
      "Minibatch loss at step 49150: 0.755799\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 49200: 0.00080999999773\n",
      "Minibatch loss at step 49200: 0.633371\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 49250: 0.00080999999773\n",
      "Minibatch loss at step 49250: 0.449676\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 49300: 0.00080999999773\n",
      "Minibatch loss at step 49300: 0.631163\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 49350: 0.00080999999773\n",
      "Minibatch loss at step 49350: 0.099380\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 49400: 0.00080999999773\n",
      "Minibatch loss at step 49400: 0.337694\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 49450: 0.00080999999773\n",
      "Minibatch loss at step 49450: 0.599075\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 49500: 0.00080999999773\n",
      "Minibatch loss at step 49500: 0.071145\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 88.4%\n",
      "Learning rate at step 49550: 0.00080999999773\n",
      "Minibatch loss at step 49550: 0.150564\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 49600: 0.00080999999773\n",
      "Minibatch loss at step 49600: 0.298393\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 49650: 0.00080999999773\n",
      "Minibatch loss at step 49650: 0.277540\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 49700: 0.00080999999773\n",
      "Minibatch loss at step 49700: 0.293797\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 49750: 0.00080999999773\n",
      "Minibatch loss at step 49750: 0.241999\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 49800: 0.00080999999773\n",
      "Minibatch loss at step 49800: 0.224187\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 49850: 0.00080999999773\n",
      "Minibatch loss at step 49850: 0.306783\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 49900: 0.00080999999773\n",
      "Minibatch loss at step 49900: 0.298679\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 49950: 0.00080999999773\n",
      "Minibatch loss at step 49950: 0.426289\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 50000: 0.00080999999773\n",
      "Minibatch loss at step 50000: 0.424458\n",
      "Minibatch accuracy: 87.5%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 50050: 0.00080999999773\n",
      "Minibatch loss at step 50050: 0.196134\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 50100: 0.00080999999773\n",
      "Minibatch loss at step 50100: 0.204091\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 50150: 0.00080999999773\n",
      "Minibatch loss at step 50150: 0.316305\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 50200: 0.00080999999773\n",
      "Minibatch loss at step 50200: 0.048954\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 50250: 0.00080999999773\n",
      "Minibatch loss at step 50250: 0.321739\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 50300: 0.00080999999773\n",
      "Minibatch loss at step 50300: 0.038852\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 50350: 0.00080999999773\n",
      "Minibatch loss at step 50350: 0.209807\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 50400: 0.00080999999773\n",
      "Minibatch loss at step 50400: 0.351013\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 50450: 0.00080999999773\n",
      "Minibatch loss at step 50450: 0.143459\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 50500: 0.00080999999773\n",
      "Minibatch loss at step 50500: 0.290504\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 88.6%\n",
      "Learning rate at step 50550: 0.00080999999773\n",
      "Minibatch loss at step 50550: 0.649356\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 50600: 0.00080999999773\n",
      "Minibatch loss at step 50600: 0.238855\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 50650: 0.00080999999773\n",
      "Minibatch loss at step 50650: 0.196711\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 50700: 0.00080999999773\n",
      "Minibatch loss at step 50700: 0.526486\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 50750: 0.00080999999773\n",
      "Minibatch loss at step 50750: 0.471825\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 50800: 0.00080999999773\n",
      "Minibatch loss at step 50800: 0.221422\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 50850: 0.00080999999773\n",
      "Minibatch loss at step 50850: 0.135971\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 50900: 0.00080999999773\n",
      "Minibatch loss at step 50900: 0.248366\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 50950: 0.00080999999773\n",
      "Minibatch loss at step 50950: 0.233720\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 51000: 0.00080999999773\n",
      "Minibatch loss at step 51000: 0.124414\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 89.0%\n",
      "Learning rate at step 51050: 0.00080999999773\n",
      "Minibatch loss at step 51050: 0.388125\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 51100: 0.00080999999773\n",
      "Minibatch loss at step 51100: 0.734394\n",
      "Minibatch accuracy: 90.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 51150: 0.00080999999773\n",
      "Minibatch loss at step 51150: 0.340876\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 51200: 0.00080999999773\n",
      "Minibatch loss at step 51200: 0.321298\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 51250: 0.00080999999773\n",
      "Minibatch loss at step 51250: 0.162604\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 51300: 0.00080999999773\n",
      "Minibatch loss at step 51300: 0.488713\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 51350: 0.00080999999773\n",
      "Minibatch loss at step 51350: 0.077515\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 51400: 0.00080999999773\n",
      "Minibatch loss at step 51400: 0.320438\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 51450: 0.00080999999773\n",
      "Minibatch loss at step 51450: 0.319908\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 51500: 0.00080999999773\n",
      "Minibatch loss at step 51500: 0.212709\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 88.9%\n",
      "Learning rate at step 51550: 0.00080999999773\n",
      "Minibatch loss at step 51550: 0.425194\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 51600: 0.00080999999773\n",
      "Minibatch loss at step 51600: 1.133509\n",
      "Minibatch accuracy: 73.4%\n",
      "Learning rate at step 51650: 0.00080999999773\n",
      "Minibatch loss at step 51650: 0.196275\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 51700: 0.00080999999773\n",
      "Minibatch loss at step 51700: 0.779349\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 51750: 0.00080999999773\n",
      "Minibatch loss at step 51750: 0.405794\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 51800: 0.00080999999773\n",
      "Minibatch loss at step 51800: 0.724137\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 51850: 0.00080999999773\n",
      "Minibatch loss at step 51850: 0.472760\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 51900: 0.00080999999773\n",
      "Minibatch loss at step 51900: 0.445405\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 51950: 0.00080999999773\n",
      "Minibatch loss at step 51950: 0.670934\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 52000: 0.00080999999773\n",
      "Minibatch loss at step 52000: 0.346031\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.0%\n",
      "Learning rate at step 52050: 0.00080999999773\n",
      "Minibatch loss at step 52050: 0.884724\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 52100: 0.00080999999773\n",
      "Minibatch loss at step 52100: 0.147478\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 52150: 0.00080999999773\n",
      "Minibatch loss at step 52150: 0.269161\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 52200: 0.00080999999773\n",
      "Minibatch loss at step 52200: 0.573106\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 52250: 0.00080999999773\n",
      "Minibatch loss at step 52250: 0.568418\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 52300: 0.00080999999773\n",
      "Minibatch loss at step 52300: 0.323111\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 52350: 0.00080999999773\n",
      "Minibatch loss at step 52350: 0.139036\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 52400: 0.00080999999773\n",
      "Minibatch loss at step 52400: 0.148356\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 52450: 0.00080999999773\n",
      "Minibatch loss at step 52450: 0.217167\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 52500: 0.00080999999773\n",
      "Minibatch loss at step 52500: 0.236068\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 88.7%\n",
      "Learning rate at step 52550: 0.00080999999773\n",
      "Minibatch loss at step 52550: 0.458843\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 52600: 0.00080999999773\n",
      "Minibatch loss at step 52600: 0.352905\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 52650: 0.00080999999773\n",
      "Minibatch loss at step 52650: 0.134338\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 52700: 0.00080999999773\n",
      "Minibatch loss at step 52700: 0.209401\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 52750: 0.00080999999773\n",
      "Minibatch loss at step 52750: 0.547898\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 52800: 0.00080999999773\n",
      "Minibatch loss at step 52800: 0.388723\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 52850: 0.00080999999773\n",
      "Minibatch loss at step 52850: 0.116836\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 52900: 0.00080999999773\n",
      "Minibatch loss at step 52900: 0.086682\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 52950: 0.00080999999773\n",
      "Minibatch loss at step 52950: 0.380880\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 53000: 0.00080999999773\n",
      "Minibatch loss at step 53000: 0.594664\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 88.7%\n",
      "Learning rate at step 53050: 0.00080999999773\n",
      "Minibatch loss at step 53050: 0.589228\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 53100: 0.00080999999773\n",
      "Minibatch loss at step 53100: 0.407148\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 53150: 0.00080999999773\n",
      "Minibatch loss at step 53150: 0.374275\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 53200: 0.00080999999773\n",
      "Minibatch loss at step 53200: 0.161706\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 53250: 0.00080999999773\n",
      "Minibatch loss at step 53250: 0.295744\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 53300: 0.00080999999773\n",
      "Minibatch loss at step 53300: 0.173388\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 53350: 0.00080999999773\n",
      "Minibatch loss at step 53350: 0.276315\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 53400: 0.00080999999773\n",
      "Minibatch loss at step 53400: 0.209498\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 53450: 0.00080999999773\n",
      "Minibatch loss at step 53450: 0.635611\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 53500: 0.00080999999773\n",
      "Minibatch loss at step 53500: 0.256032\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.0%\n",
      "Learning rate at step 53550: 0.00080999999773\n",
      "Minibatch loss at step 53550: 0.241198\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 53600: 0.00080999999773\n",
      "Minibatch loss at step 53600: 0.384089\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 53650: 0.00080999999773\n",
      "Minibatch loss at step 53650: 0.247133\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 53700: 0.00080999999773\n",
      "Minibatch loss at step 53700: 0.363708\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 53750: 0.00080999999773\n",
      "Minibatch loss at step 53750: 0.174274\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 53800: 0.00080999999773\n",
      "Minibatch loss at step 53800: 0.214070\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 53850: 0.00080999999773\n",
      "Minibatch loss at step 53850: 0.238873\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 53900: 0.00080999999773\n",
      "Minibatch loss at step 53900: 0.201267\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 53950: 0.00080999999773\n",
      "Minibatch loss at step 53950: 0.272417\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 54000: 0.00080999999773\n",
      "Minibatch loss at step 54000: 0.463796\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 88.5%\n",
      "Learning rate at step 54050: 0.00080999999773\n",
      "Minibatch loss at step 54050: 0.317976\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 54100: 0.00080999999773\n",
      "Minibatch loss at step 54100: 0.258846\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 54150: 0.00080999999773\n",
      "Minibatch loss at step 54150: 0.648716\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 54200: 0.00080999999773\n",
      "Minibatch loss at step 54200: 0.437138\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 54250: 0.00080999999773\n",
      "Minibatch loss at step 54250: 0.383350\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 54300: 0.00080999999773\n",
      "Minibatch loss at step 54300: 0.148912\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 54350: 0.00080999999773\n",
      "Minibatch loss at step 54350: 0.114172\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 54400: 0.00080999999773\n",
      "Minibatch loss at step 54400: 0.448696\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 54450: 0.00080999999773\n",
      "Minibatch loss at step 54450: 0.722517\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 54500: 0.00080999999773\n",
      "Minibatch loss at step 54500: 0.203151\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.6%\n",
      "Learning rate at step 54550: 0.00080999999773\n",
      "Minibatch loss at step 54550: 0.198480\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 54600: 0.00080999999773\n",
      "Minibatch loss at step 54600: 0.847778\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 54650: 0.00080999999773\n",
      "Minibatch loss at step 54650: 0.659389\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 54700: 0.00080999999773\n",
      "Minibatch loss at step 54700: 0.573054\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 54750: 0.00080999999773\n",
      "Minibatch loss at step 54750: 0.273486\n",
      "Minibatch accuracy: 90.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 54800: 0.00080999999773\n",
      "Minibatch loss at step 54800: 0.258376\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 54850: 0.00080999999773\n",
      "Minibatch loss at step 54850: 0.841174\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 54900: 0.00080999999773\n",
      "Minibatch loss at step 54900: 0.606339\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 54950: 0.00080999999773\n",
      "Minibatch loss at step 54950: 0.119448\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 55000: 0.00080999999773\n",
      "Minibatch loss at step 55000: 0.236877\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 88.3%\n",
      "Learning rate at step 55050: 0.00080999999773\n",
      "Minibatch loss at step 55050: 0.252347\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 55100: 0.00080999999773\n",
      "Minibatch loss at step 55100: 0.297633\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 55150: 0.00080999999773\n",
      "Minibatch loss at step 55150: 0.202820\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 55200: 0.00080999999773\n",
      "Minibatch loss at step 55200: 0.151739\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 55250: 0.00080999999773\n",
      "Minibatch loss at step 55250: 0.119102\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 55300: 0.00080999999773\n",
      "Minibatch loss at step 55300: 0.218770\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 55350: 0.00080999999773\n",
      "Minibatch loss at step 55350: 0.298217\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 55400: 0.00080999999773\n",
      "Minibatch loss at step 55400: 0.275127\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 55450: 0.00080999999773\n",
      "Minibatch loss at step 55450: 0.201847\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 55500: 0.00080999999773\n",
      "Minibatch loss at step 55500: 0.450409\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 88.9%\n",
      "Learning rate at step 55550: 0.00080999999773\n",
      "Minibatch loss at step 55550: 0.165089\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 55600: 0.00080999999773\n",
      "Minibatch loss at step 55600: 0.275872\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 55650: 0.00080999999773\n",
      "Minibatch loss at step 55650: 0.100121\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 55700: 0.00080999999773\n",
      "Minibatch loss at step 55700: 0.232714\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 55750: 0.00080999999773\n",
      "Minibatch loss at step 55750: 0.103374\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 55800: 0.00080999999773\n",
      "Minibatch loss at step 55800: 0.334271\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 55850: 0.00080999999773\n",
      "Minibatch loss at step 55850: 0.357060\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 55900: 0.00080999999773\n",
      "Minibatch loss at step 55900: 0.225776\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 55950: 0.00080999999773\n",
      "Minibatch loss at step 55950: 0.282109\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 56000: 0.00080999999773\n",
      "Minibatch loss at step 56000: 0.413040\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 88.5%\n",
      "Learning rate at step 56050: 0.00080999999773\n",
      "Minibatch loss at step 56050: 0.347384\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 56100: 0.00080999999773\n",
      "Minibatch loss at step 56100: 0.168814\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 56150: 0.00080999999773\n",
      "Minibatch loss at step 56150: 0.309769\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 56200: 0.00080999999773\n",
      "Minibatch loss at step 56200: 0.337422\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 56250: 0.00080999999773\n",
      "Minibatch loss at step 56250: 0.834618\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 56300: 0.00080999999773\n",
      "Minibatch loss at step 56300: 0.260621\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 56350: 0.00080999999773\n",
      "Minibatch loss at step 56350: 0.155674\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 56400: 0.00080999999773\n",
      "Minibatch loss at step 56400: 0.470404\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 56450: 0.00080999999773\n",
      "Minibatch loss at step 56450: 0.295066\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 56500: 0.00080999999773\n",
      "Minibatch loss at step 56500: 0.523569\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 88.7%\n",
      "Learning rate at step 56550: 0.00080999999773\n",
      "Minibatch loss at step 56550: 0.236887\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 56600: 0.00080999999773\n",
      "Minibatch loss at step 56600: 0.428070\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 56650: 0.00080999999773\n",
      "Minibatch loss at step 56650: 0.260470\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 56700: 0.00080999999773\n",
      "Minibatch loss at step 56700: 0.117852\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 56750: 0.00080999999773\n",
      "Minibatch loss at step 56750: 0.960870\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 56800: 0.00080999999773\n",
      "Minibatch loss at step 56800: 0.407037\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 56850: 0.00080999999773\n",
      "Minibatch loss at step 56850: 0.468281\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 56900: 0.00080999999773\n",
      "Minibatch loss at step 56900: 0.300171\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 56950: 0.00080999999773\n",
      "Minibatch loss at step 56950: 0.521981\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 57000: 0.00080999999773\n",
      "Minibatch loss at step 57000: 0.083711\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 88.7%\n",
      "Learning rate at step 57050: 0.00080999999773\n",
      "Minibatch loss at step 57050: 0.311899\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 57100: 0.00080999999773\n",
      "Minibatch loss at step 57100: 0.493789\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 57150: 0.00080999999773\n",
      "Minibatch loss at step 57150: 0.445806\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 57200: 0.00080999999773\n",
      "Minibatch loss at step 57200: 0.076112\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 57250: 0.00080999999773\n",
      "Minibatch loss at step 57250: 0.149293\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 57300: 0.00080999999773\n",
      "Minibatch loss at step 57300: 0.427764\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 57350: 0.00080999999773\n",
      "Minibatch loss at step 57350: 0.934589\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 57400: 0.00080999999773\n",
      "Minibatch loss at step 57400: 0.485896\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 57450: 0.00080999999773\n",
      "Minibatch loss at step 57450: 0.516467\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 57500: 0.00080999999773\n",
      "Minibatch loss at step 57500: 0.588630\n",
      "Minibatch accuracy: 85.9%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 57550: 0.00080999999773\n",
      "Minibatch loss at step 57550: 0.188091\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 57600: 0.00080999999773\n",
      "Minibatch loss at step 57600: 0.327726\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 57650: 0.00080999999773\n",
      "Minibatch loss at step 57650: 0.565871\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 57700: 0.00080999999773\n",
      "Minibatch loss at step 57700: 0.756442\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 57750: 0.00080999999773\n",
      "Minibatch loss at step 57750: 0.418869\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 57800: 0.00080999999773\n",
      "Minibatch loss at step 57800: 0.379843\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 57850: 0.00080999999773\n",
      "Minibatch loss at step 57850: 0.409710\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 57900: 0.00080999999773\n",
      "Minibatch loss at step 57900: 0.102235\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 57950: 0.00080999999773\n",
      "Minibatch loss at step 57950: 0.115714\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 58000: 0.00080999999773\n",
      "Minibatch loss at step 58000: 0.333097\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.1%\n",
      "Learning rate at step 58050: 0.00080999999773\n",
      "Minibatch loss at step 58050: 0.198167\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 58100: 0.00080999999773\n",
      "Minibatch loss at step 58100: 0.212215\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 58150: 0.00080999999773\n",
      "Minibatch loss at step 58150: 0.077784\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 58200: 0.00080999999773\n",
      "Minibatch loss at step 58200: 0.288891\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 58250: 0.00080999999773\n",
      "Minibatch loss at step 58250: 0.143020\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 58300: 0.00080999999773\n",
      "Minibatch loss at step 58300: 0.307261\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 58350: 0.00080999999773\n",
      "Minibatch loss at step 58350: 0.196366\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 58400: 0.00080999999773\n",
      "Minibatch loss at step 58400: 0.272721\n",
      "Minibatch accuracy: 90.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 58450: 0.00080999999773\n",
      "Minibatch loss at step 58450: 0.142165\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 58500: 0.00080999999773\n",
      "Minibatch loss at step 58500: 0.454266\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 88.3%\n",
      "Learning rate at step 58550: 0.00080999999773\n",
      "Minibatch loss at step 58550: 0.473167\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 58600: 0.00080999999773\n",
      "Minibatch loss at step 58600: 0.264104\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 58650: 0.00080999999773\n",
      "Minibatch loss at step 58650: 1.177392\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 58700: 0.00080999999773\n",
      "Minibatch loss at step 58700: 0.303710\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 58750: 0.00080999999773\n",
      "Minibatch loss at step 58750: 0.111582\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 58800: 0.00080999999773\n",
      "Minibatch loss at step 58800: 0.288353\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 58850: 0.00080999999773\n",
      "Minibatch loss at step 58850: 0.541251\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 58900: 0.00080999999773\n",
      "Minibatch loss at step 58900: 0.080498\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 58950: 0.00080999999773\n",
      "Minibatch loss at step 58950: 0.552402\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 59000: 0.00080999999773\n",
      "Minibatch loss at step 59000: 0.261142\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 88.6%\n",
      "Learning rate at step 59050: 0.00080999999773\n",
      "Minibatch loss at step 59050: 0.137713\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 59100: 0.00080999999773\n",
      "Minibatch loss at step 59100: 0.305120\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 59150: 0.00080999999773\n",
      "Minibatch loss at step 59150: 0.103958\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 59200: 0.00080999999773\n",
      "Minibatch loss at step 59200: 0.168029\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 59250: 0.00080999999773\n",
      "Minibatch loss at step 59250: 0.281387\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 59300: 0.00080999999773\n",
      "Minibatch loss at step 59300: 0.394821\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 59350: 0.00080999999773\n",
      "Minibatch loss at step 59350: 0.186147\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 59400: 0.00080999999773\n",
      "Minibatch loss at step 59400: 0.212106\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 59450: 0.00080999999773\n",
      "Minibatch loss at step 59450: 0.359705\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 59500: 0.00080999999773\n",
      "Minibatch loss at step 59500: 0.142300\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 88.7%\n",
      "Learning rate at step 59550: 0.00080999999773\n",
      "Minibatch loss at step 59550: 0.201567\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 59600: 0.00080999999773\n",
      "Minibatch loss at step 59600: 0.281716\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 59650: 0.00080999999773\n",
      "Minibatch loss at step 59650: 0.287531\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 59700: 0.00080999999773\n",
      "Minibatch loss at step 59700: 0.769175\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 59750: 0.00080999999773\n",
      "Minibatch loss at step 59750: 0.419887\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 59800: 0.00080999999773\n",
      "Minibatch loss at step 59800: 0.214400\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 59850: 0.00080999999773\n",
      "Minibatch loss at step 59850: 0.282802\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 59900: 0.00080999999773\n",
      "Minibatch loss at step 59900: 0.440305\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 59950: 0.00080999999773\n",
      "Minibatch loss at step 59950: 0.427824\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 60000: 0.00072899996303\n",
      "Minibatch loss at step 60000: 0.308819\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.1%\n",
      "Learning rate at step 60050: 0.00072899996303\n",
      "Minibatch loss at step 60050: 0.103451\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 60100: 0.00072899996303\n",
      "Minibatch loss at step 60100: 0.274548\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 60150: 0.00072899996303\n",
      "Minibatch loss at step 60150: 0.162403\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 60200: 0.00072899996303\n",
      "Minibatch loss at step 60200: 0.597707\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 60250: 0.00072899996303\n",
      "Minibatch loss at step 60250: 0.397013\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 60300: 0.00072899996303\n",
      "Minibatch loss at step 60300: 0.338930\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 60350: 0.00072899996303\n",
      "Minibatch loss at step 60350: 0.453402\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 60400: 0.00072899996303\n",
      "Minibatch loss at step 60400: 0.388559\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 60450: 0.00072899996303\n",
      "Minibatch loss at step 60450: 0.224201\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 60500: 0.00072899996303\n",
      "Minibatch loss at step 60500: 0.604937\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.1%\n",
      "Learning rate at step 60550: 0.00072899996303\n",
      "Minibatch loss at step 60550: 0.972835\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 60600: 0.00072899996303\n",
      "Minibatch loss at step 60600: 0.315816\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 60650: 0.00072899996303\n",
      "Minibatch loss at step 60650: 0.525575\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 60700: 0.00072899996303\n",
      "Minibatch loss at step 60700: 0.139172\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 60750: 0.00072899996303\n",
      "Minibatch loss at step 60750: 0.391018\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 60800: 0.00072899996303\n",
      "Minibatch loss at step 60800: 0.263831\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 60850: 0.00072899996303\n",
      "Minibatch loss at step 60850: 0.278821\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 60900: 0.00072899996303\n",
      "Minibatch loss at step 60900: 0.149626\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 60950: 0.00072899996303\n",
      "Minibatch loss at step 60950: 0.410204\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 61000: 0.00072899996303\n",
      "Minibatch loss at step 61000: 0.469961\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.0%\n",
      "Learning rate at step 61050: 0.00072899996303\n",
      "Minibatch loss at step 61050: 0.088941\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 61100: 0.00072899996303\n",
      "Minibatch loss at step 61100: 0.166334\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 61150: 0.00072899996303\n",
      "Minibatch loss at step 61150: 0.338892\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 61200: 0.00072899996303\n",
      "Minibatch loss at step 61200: 0.644262\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 61250: 0.00072899996303\n",
      "Minibatch loss at step 61250: 0.155514\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 61300: 0.00072899996303\n",
      "Minibatch loss at step 61300: 0.254564\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 61350: 0.00072899996303\n",
      "Minibatch loss at step 61350: 0.186869\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 61400: 0.00072899996303\n",
      "Minibatch loss at step 61400: 0.329690\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 61450: 0.00072899996303\n",
      "Minibatch loss at step 61450: 0.212566\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 61500: 0.00072899996303\n",
      "Minibatch loss at step 61500: 0.194768\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 61550: 0.00072899996303\n",
      "Minibatch loss at step 61550: 0.257548\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 61600: 0.00072899996303\n",
      "Minibatch loss at step 61600: 0.318797\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 61650: 0.00072899996303\n",
      "Minibatch loss at step 61650: 0.044896\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 61700: 0.00072899996303\n",
      "Minibatch loss at step 61700: 0.231842\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 61750: 0.00072899996303\n",
      "Minibatch loss at step 61750: 0.439219\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 61800: 0.00072899996303\n",
      "Minibatch loss at step 61800: 0.270962\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 61850: 0.00072899996303\n",
      "Minibatch loss at step 61850: 0.366397\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 61900: 0.00072899996303\n",
      "Minibatch loss at step 61900: 0.188235\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 61950: 0.00072899996303\n",
      "Minibatch loss at step 61950: 0.256012\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 62000: 0.00072899996303\n",
      "Minibatch loss at step 62000: 0.257828\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 62050: 0.00072899996303\n",
      "Minibatch loss at step 62050: 0.521238\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 62100: 0.00072899996303\n",
      "Minibatch loss at step 62100: 0.083449\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 62150: 0.00072899996303\n",
      "Minibatch loss at step 62150: 0.218520\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 62200: 0.00072899996303\n",
      "Minibatch loss at step 62200: 0.093026\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 62250: 0.00072899996303\n",
      "Minibatch loss at step 62250: 0.142614\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 62300: 0.00072899996303\n",
      "Minibatch loss at step 62300: 0.246994\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 62350: 0.00072899996303\n",
      "Minibatch loss at step 62350: 0.115228\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 62400: 0.00072899996303\n",
      "Minibatch loss at step 62400: 0.159676\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 62450: 0.00072899996303\n",
      "Minibatch loss at step 62450: 0.101044\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 62500: 0.00072899996303\n",
      "Minibatch loss at step 62500: 0.153747\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.6%\n",
      "Learning rate at step 62550: 0.00072899996303\n",
      "Minibatch loss at step 62550: 0.059169\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 62600: 0.00072899996303\n",
      "Minibatch loss at step 62600: 0.186781\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 62650: 0.00072899996303\n",
      "Minibatch loss at step 62650: 0.093946\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 62700: 0.00072899996303\n",
      "Minibatch loss at step 62700: 0.117260\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 62750: 0.00072899996303\n",
      "Minibatch loss at step 62750: 0.259256\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 62800: 0.00072899996303\n",
      "Minibatch loss at step 62800: 0.096202\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 62850: 0.00072899996303\n",
      "Minibatch loss at step 62850: 0.244017\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 62900: 0.00072899996303\n",
      "Minibatch loss at step 62900: 0.191786\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 62950: 0.00072899996303\n",
      "Minibatch loss at step 62950: 0.333351\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 63000: 0.00072899996303\n",
      "Minibatch loss at step 63000: 0.231428\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.1%\n",
      "Learning rate at step 63050: 0.00072899996303\n",
      "Minibatch loss at step 63050: 0.428479\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 63100: 0.00072899996303\n",
      "Minibatch loss at step 63100: 0.272720\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 63150: 0.00072899996303\n",
      "Minibatch loss at step 63150: 0.239493\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 63200: 0.00072899996303\n",
      "Minibatch loss at step 63200: 0.380194\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 63250: 0.00072899996303\n",
      "Minibatch loss at step 63250: 0.516478\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 63300: 0.00072899996303\n",
      "Minibatch loss at step 63300: 0.419123\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 63350: 0.00072899996303\n",
      "Minibatch loss at step 63350: 0.668277\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 63400: 0.00072899996303\n",
      "Minibatch loss at step 63400: 0.236363\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 63450: 0.00072899996303\n",
      "Minibatch loss at step 63450: 0.392584\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 63500: 0.00072899996303\n",
      "Minibatch loss at step 63500: 0.297183\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 89.2%\n",
      "Learning rate at step 63550: 0.00072899996303\n",
      "Minibatch loss at step 63550: 0.361601\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 63600: 0.00072899996303\n",
      "Minibatch loss at step 63600: 0.267821\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 63650: 0.00072899996303\n",
      "Minibatch loss at step 63650: 0.666874\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 63700: 0.00072899996303\n",
      "Minibatch loss at step 63700: 0.205708\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 63750: 0.00072899996303\n",
      "Minibatch loss at step 63750: 0.339616\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 63800: 0.00072899996303\n",
      "Minibatch loss at step 63800: 0.126687\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 63850: 0.00072899996303\n",
      "Minibatch loss at step 63850: 0.153460\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 63900: 0.00072899996303\n",
      "Minibatch loss at step 63900: 0.186928\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 63950: 0.00072899996303\n",
      "Minibatch loss at step 63950: 0.422785\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 64000: 0.00072899996303\n",
      "Minibatch loss at step 64000: 0.579281\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 88.9%\n",
      "Learning rate at step 64050: 0.00072899996303\n",
      "Minibatch loss at step 64050: 0.487793\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 64100: 0.00072899996303\n",
      "Minibatch loss at step 64100: 0.396838\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 64150: 0.00072899996303\n",
      "Minibatch loss at step 64150: 0.118946\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 64200: 0.00072899996303\n",
      "Minibatch loss at step 64200: 0.133232\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 64250: 0.00072899996303\n",
      "Minibatch loss at step 64250: 0.129482\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 64300: 0.00072899996303\n",
      "Minibatch loss at step 64300: 0.214164\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 64350: 0.00072899996303\n",
      "Minibatch loss at step 64350: 0.297067\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 64400: 0.00072899996303\n",
      "Minibatch loss at step 64400: 0.197367\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 64450: 0.00072899996303\n",
      "Minibatch loss at step 64450: 0.235395\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 64500: 0.00072899996303\n",
      "Minibatch loss at step 64500: 0.200306\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 88.9%\n",
      "Learning rate at step 64550: 0.00072899996303\n",
      "Minibatch loss at step 64550: 0.272481\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 64600: 0.00072899996303\n",
      "Minibatch loss at step 64600: 0.433295\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 64650: 0.00072899996303\n",
      "Minibatch loss at step 64650: 0.232547\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 64700: 0.00072899996303\n",
      "Minibatch loss at step 64700: 0.262757\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 64750: 0.00072899996303\n",
      "Minibatch loss at step 64750: 0.043673\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 64800: 0.00072899996303\n",
      "Minibatch loss at step 64800: 0.264566\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 64850: 0.00072899996303\n",
      "Minibatch loss at step 64850: 0.365608\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 64900: 0.00072899996303\n",
      "Minibatch loss at step 64900: 0.080068\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 64950: 0.00072899996303\n",
      "Minibatch loss at step 64950: 0.240291\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 65000: 0.00072899996303\n",
      "Minibatch loss at step 65000: 0.314435\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 65050: 0.00072899996303\n",
      "Minibatch loss at step 65050: 0.355976\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 65100: 0.00072899996303\n",
      "Minibatch loss at step 65100: 0.146050\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 65150: 0.00072899996303\n",
      "Minibatch loss at step 65150: 0.067071\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 65200: 0.00072899996303\n",
      "Minibatch loss at step 65200: 0.113406\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 65250: 0.00072899996303\n",
      "Minibatch loss at step 65250: 0.535226\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 65300: 0.00072899996303\n",
      "Minibatch loss at step 65300: 0.560009\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 65350: 0.00072899996303\n",
      "Minibatch loss at step 65350: 0.146844\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 65400: 0.00072899996303\n",
      "Minibatch loss at step 65400: 0.345256\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 65450: 0.00072899996303\n",
      "Minibatch loss at step 65450: 0.327427\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 65500: 0.00072899996303\n",
      "Minibatch loss at step 65500: 0.081132\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.0%\n",
      "Learning rate at step 65550: 0.00072899996303\n",
      "Minibatch loss at step 65550: 0.168409\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 65600: 0.00072899996303\n",
      "Minibatch loss at step 65600: 0.084046\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 65650: 0.00072899996303\n",
      "Minibatch loss at step 65650: 0.077625\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 65700: 0.00072899996303\n",
      "Minibatch loss at step 65700: 0.092654\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 65750: 0.00072899996303\n",
      "Minibatch loss at step 65750: 0.303712\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 65800: 0.00072899996303\n",
      "Minibatch loss at step 65800: 0.412953\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 65850: 0.00072899996303\n",
      "Minibatch loss at step 65850: 0.120320\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 65900: 0.00072899996303\n",
      "Minibatch loss at step 65900: 0.606648\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 65950: 0.00072899996303\n",
      "Minibatch loss at step 65950: 0.582489\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 66000: 0.00072899996303\n",
      "Minibatch loss at step 66000: 1.034194\n",
      "Minibatch accuracy: 84.4%\n",
      "validation accuracy: 89.2%\n",
      "Learning rate at step 66050: 0.00072899996303\n",
      "Minibatch loss at step 66050: 0.297162\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 66100: 0.00072899996303\n",
      "Minibatch loss at step 66100: 0.318143\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 66150: 0.00072899996303\n",
      "Minibatch loss at step 66150: 0.343132\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 66200: 0.00072899996303\n",
      "Minibatch loss at step 66200: 0.542596\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 66250: 0.00072899996303\n",
      "Minibatch loss at step 66250: 0.534055\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 66300: 0.00072899996303\n",
      "Minibatch loss at step 66300: 0.493178\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 66350: 0.00072899996303\n",
      "Minibatch loss at step 66350: 0.400040\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 66400: 0.00072899996303\n",
      "Minibatch loss at step 66400: 0.462566\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 66450: 0.00072899996303\n",
      "Minibatch loss at step 66450: 0.140389\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 66500: 0.00072899996303\n",
      "Minibatch loss at step 66500: 0.272631\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.3%\n",
      "Learning rate at step 66550: 0.00072899996303\n",
      "Minibatch loss at step 66550: 0.095366\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 66600: 0.00072899996303\n",
      "Minibatch loss at step 66600: 0.508596\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 66650: 0.00072899996303\n",
      "Minibatch loss at step 66650: 0.186056\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 66700: 0.00072899996303\n",
      "Minibatch loss at step 66700: 0.224904\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 66750: 0.00072899996303\n",
      "Minibatch loss at step 66750: 0.037324\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 66800: 0.00072899996303\n",
      "Minibatch loss at step 66800: 0.131237\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 66850: 0.00072899996303\n",
      "Minibatch loss at step 66850: 0.506386\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 66900: 0.00072899996303\n",
      "Minibatch loss at step 66900: 0.259787\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 66950: 0.00072899996303\n",
      "Minibatch loss at step 66950: 0.221380\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 67000: 0.00072899996303\n",
      "Minibatch loss at step 67000: 0.237564\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 88.5%\n",
      "Learning rate at step 67050: 0.00072899996303\n",
      "Minibatch loss at step 67050: 0.351001\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 67100: 0.00072899996303\n",
      "Minibatch loss at step 67100: 0.081133\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 67150: 0.00072899996303\n",
      "Minibatch loss at step 67150: 0.352737\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 67200: 0.00072899996303\n",
      "Minibatch loss at step 67200: 0.041842\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 67250: 0.00072899996303\n",
      "Minibatch loss at step 67250: 0.113671\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 67300: 0.00072899996303\n",
      "Minibatch loss at step 67300: 0.159741\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 67350: 0.00072899996303\n",
      "Minibatch loss at step 67350: 0.254999\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 67400: 0.00072899996303\n",
      "Minibatch loss at step 67400: 0.237866\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 67450: 0.00072899996303\n",
      "Minibatch loss at step 67450: 0.093353\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 67500: 0.00072899996303\n",
      "Minibatch loss at step 67500: 0.224548\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.0%\n",
      "Learning rate at step 67550: 0.00072899996303\n",
      "Minibatch loss at step 67550: 0.449583\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 67600: 0.00072899996303\n",
      "Minibatch loss at step 67600: 0.382711\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 67650: 0.00072899996303\n",
      "Minibatch loss at step 67650: 0.149404\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 67700: 0.00072899996303\n",
      "Minibatch loss at step 67700: 0.201468\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 67750: 0.00072899996303\n",
      "Minibatch loss at step 67750: 0.086023\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 67800: 0.00072899996303\n",
      "Minibatch loss at step 67800: 0.046046\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 67850: 0.00072899996303\n",
      "Minibatch loss at step 67850: 0.361588\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 67900: 0.00072899996303\n",
      "Minibatch loss at step 67900: 0.301808\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 67950: 0.00072899996303\n",
      "Minibatch loss at step 67950: 0.271074\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 68000: 0.00072899996303\n",
      "Minibatch loss at step 68000: 0.059182\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 89.1%\n",
      "Learning rate at step 68050: 0.00072899996303\n",
      "Minibatch loss at step 68050: 0.254363\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 68100: 0.00072899996303\n",
      "Minibatch loss at step 68100: 0.130293\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 68150: 0.00072899996303\n",
      "Minibatch loss at step 68150: 0.187736\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 68200: 0.00072899996303\n",
      "Minibatch loss at step 68200: 0.770277\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 68250: 0.00072899996303\n",
      "Minibatch loss at step 68250: 0.225728\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 68300: 0.00072899996303\n",
      "Minibatch loss at step 68300: 0.148876\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 68350: 0.00072899996303\n",
      "Minibatch loss at step 68350: 0.242399\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 68400: 0.00072899996303\n",
      "Minibatch loss at step 68400: 0.168967\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 68450: 0.00072899996303\n",
      "Minibatch loss at step 68450: 0.267464\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 68500: 0.00072899996303\n",
      "Minibatch loss at step 68500: 0.315938\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.2%\n",
      "Learning rate at step 68550: 0.00072899996303\n",
      "Minibatch loss at step 68550: 0.099260\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 68600: 0.00072899996303\n",
      "Minibatch loss at step 68600: 0.224842\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 68650: 0.00072899996303\n",
      "Minibatch loss at step 68650: 0.353294\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 68700: 0.00072899996303\n",
      "Minibatch loss at step 68700: 0.044623\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 68750: 0.00072899996303\n",
      "Minibatch loss at step 68750: 0.156563\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 68800: 0.00072899996303\n",
      "Minibatch loss at step 68800: 0.696081\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 68850: 0.00072899996303\n",
      "Minibatch loss at step 68850: 0.726482\n",
      "Minibatch accuracy: 78.1%\n",
      "Learning rate at step 68900: 0.00072899996303\n",
      "Minibatch loss at step 68900: 0.956748\n",
      "Minibatch accuracy: 76.6%\n",
      "Learning rate at step 68950: 0.00072899996303\n",
      "Minibatch loss at step 68950: 0.292190\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 69000: 0.00072899996303\n",
      "Minibatch loss at step 69000: 0.638443\n",
      "Minibatch accuracy: 84.4%\n",
      "validation accuracy: 89.2%\n",
      "Learning rate at step 69050: 0.00072899996303\n",
      "Minibatch loss at step 69050: 0.348036\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 69100: 0.00072899996303\n",
      "Minibatch loss at step 69100: 0.171225\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 69150: 0.00072899996303\n",
      "Minibatch loss at step 69150: 0.312415\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 69200: 0.00072899996303\n",
      "Minibatch loss at step 69200: 0.169971\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 69250: 0.00072899996303\n",
      "Minibatch loss at step 69250: 0.472730\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 69300: 0.00072899996303\n",
      "Minibatch loss at step 69300: 0.252918\n",
      "Minibatch accuracy: 92.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 69350: 0.00072899996303\n",
      "Minibatch loss at step 69350: 0.200038\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 69400: 0.00072899996303\n",
      "Minibatch loss at step 69400: 0.118273\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 69450: 0.00072899996303\n",
      "Minibatch loss at step 69450: 0.640046\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 69500: 0.00072899996303\n",
      "Minibatch loss at step 69500: 0.375650\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 88.9%\n",
      "Learning rate at step 69550: 0.00072899996303\n",
      "Minibatch loss at step 69550: 0.590380\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 69600: 0.00072899996303\n",
      "Minibatch loss at step 69600: 0.106050\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 69650: 0.00072899996303\n",
      "Minibatch loss at step 69650: 0.293886\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 69700: 0.00072899996303\n",
      "Minibatch loss at step 69700: 0.107381\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 69750: 0.00072899996303\n",
      "Minibatch loss at step 69750: 0.104547\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 69800: 0.00072899996303\n",
      "Minibatch loss at step 69800: 0.122242\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 69850: 0.00072899996303\n",
      "Minibatch loss at step 69850: 0.115777\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 69900: 0.00072899996303\n",
      "Minibatch loss at step 69900: 0.142531\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 69950: 0.00072899996303\n",
      "Minibatch loss at step 69950: 0.264614\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 70000: 0.00072899996303\n",
      "Minibatch loss at step 70000: 0.039164\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 70050: 0.00072899996303\n",
      "Minibatch loss at step 70050: 0.113766\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 70100: 0.00072899996303\n",
      "Minibatch loss at step 70100: 0.473164\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 70150: 0.00072899996303\n",
      "Minibatch loss at step 70150: 0.371446\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 70200: 0.00072899996303\n",
      "Minibatch loss at step 70200: 0.037461\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 70250: 0.00072899996303\n",
      "Minibatch loss at step 70250: 0.232179\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 70300: 0.00072899996303\n",
      "Minibatch loss at step 70300: 0.346942\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 70350: 0.00072899996303\n",
      "Minibatch loss at step 70350: 0.222139\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 70400: 0.00072899996303\n",
      "Minibatch loss at step 70400: 0.210512\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 70450: 0.00072899996303\n",
      "Minibatch loss at step 70450: 0.274496\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 70500: 0.00072899996303\n",
      "Minibatch loss at step 70500: 0.384568\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 70550: 0.00072899996303\n",
      "Minibatch loss at step 70550: 0.115331\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 70600: 0.00072899996303\n",
      "Minibatch loss at step 70600: 0.463451\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 70650: 0.00072899996303\n",
      "Minibatch loss at step 70650: 0.342528\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 70700: 0.00072899996303\n",
      "Minibatch loss at step 70700: 0.468102\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 70750: 0.00072899996303\n",
      "Minibatch loss at step 70750: 0.175171\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 70800: 0.00072899996303\n",
      "Minibatch loss at step 70800: 0.109310\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 70850: 0.00072899996303\n",
      "Minibatch loss at step 70850: 0.068737\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 70900: 0.00072899996303\n",
      "Minibatch loss at step 70900: 0.084069\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 70950: 0.00072899996303\n",
      "Minibatch loss at step 70950: 0.248259\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 71000: 0.00072899996303\n",
      "Minibatch loss at step 71000: 0.124275\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 71050: 0.00072899996303\n",
      "Minibatch loss at step 71050: 0.113487\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 71100: 0.00072899996303\n",
      "Minibatch loss at step 71100: 0.270058\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 71150: 0.00072899996303\n",
      "Minibatch loss at step 71150: 0.323301\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 71200: 0.00072899996303\n",
      "Minibatch loss at step 71200: 0.327898\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 71250: 0.00072899996303\n",
      "Minibatch loss at step 71250: 0.327779\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 71300: 0.00072899996303\n",
      "Minibatch loss at step 71300: 0.305171\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 71350: 0.00072899996303\n",
      "Minibatch loss at step 71350: 0.214148\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 71400: 0.00072899996303\n",
      "Minibatch loss at step 71400: 0.183649\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 71450: 0.00072899996303\n",
      "Minibatch loss at step 71450: 0.265304\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 71500: 0.00072899996303\n",
      "Minibatch loss at step 71500: 0.149493\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 71550: 0.00072899996303\n",
      "Minibatch loss at step 71550: 0.256650\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 71600: 0.00072899996303\n",
      "Minibatch loss at step 71600: 0.071773\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 71650: 0.00072899996303\n",
      "Minibatch loss at step 71650: 0.260042\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 71700: 0.00072899996303\n",
      "Minibatch loss at step 71700: 0.692376\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 71750: 0.00072899996303\n",
      "Minibatch loss at step 71750: 0.392635\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 71800: 0.00072899996303\n",
      "Minibatch loss at step 71800: 0.484681\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 71850: 0.00072899996303\n",
      "Minibatch loss at step 71850: 0.431671\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 71900: 0.00072899996303\n",
      "Minibatch loss at step 71900: 0.268643\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 71950: 0.00072899996303\n",
      "Minibatch loss at step 71950: 0.468417\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 72000: 0.00072899996303\n",
      "Minibatch loss at step 72000: 0.260286\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.2%\n",
      "Learning rate at step 72050: 0.00072899996303\n",
      "Minibatch loss at step 72050: 0.249627\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 72100: 0.00072899996303\n",
      "Minibatch loss at step 72100: 0.301429\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 72150: 0.00072899996303\n",
      "Minibatch loss at step 72150: 0.046393\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 72200: 0.00072899996303\n",
      "Minibatch loss at step 72200: 0.180524\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 72250: 0.00072899996303\n",
      "Minibatch loss at step 72250: 0.186452\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 72300: 0.00072899996303\n",
      "Minibatch loss at step 72300: 0.055706\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 72350: 0.00072899996303\n",
      "Minibatch loss at step 72350: 0.118132\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 72400: 0.00072899996303\n",
      "Minibatch loss at step 72400: 0.108775\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 72450: 0.00072899996303\n",
      "Minibatch loss at step 72450: 0.356542\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 72500: 0.00072899996303\n",
      "Minibatch loss at step 72500: 0.087425\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 72550: 0.00072899996303\n",
      "Minibatch loss at step 72550: 0.449414\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 72600: 0.00072899996303\n",
      "Minibatch loss at step 72600: 0.225149\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 72650: 0.00072899996303\n",
      "Minibatch loss at step 72650: 0.073021\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 72700: 0.00072899996303\n",
      "Minibatch loss at step 72700: 0.279895\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 72750: 0.00072899996303\n",
      "Minibatch loss at step 72750: 0.275252\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 72800: 0.00072899996303\n",
      "Minibatch loss at step 72800: 0.421232\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 72850: 0.00072899996303\n",
      "Minibatch loss at step 72850: 0.260051\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 72900: 0.00072899996303\n",
      "Minibatch loss at step 72900: 0.107671\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 72950: 0.00072899996303\n",
      "Minibatch loss at step 72950: 0.047744\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 73000: 0.00072899996303\n",
      "Minibatch loss at step 73000: 0.221107\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.0%\n",
      "Learning rate at step 73050: 0.00072899996303\n",
      "Minibatch loss at step 73050: 0.297388\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 73100: 0.00072899996303\n",
      "Minibatch loss at step 73100: 0.059772\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 73150: 0.00072899996303\n",
      "Minibatch loss at step 73150: 0.045811\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 73200: 0.00072899996303\n",
      "Minibatch loss at step 73200: 0.199694\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 73250: 0.00072899996303\n",
      "Minibatch loss at step 73250: 0.128472\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 73300: 0.00072899996303\n",
      "Minibatch loss at step 73300: 0.476672\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 73350: 0.00072899996303\n",
      "Minibatch loss at step 73350: 0.349602\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 73400: 0.00072899996303\n",
      "Minibatch loss at step 73400: 0.353371\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 73450: 0.00072899996303\n",
      "Minibatch loss at step 73450: 0.307391\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 73500: 0.00072899996303\n",
      "Minibatch loss at step 73500: 0.289810\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.1%\n",
      "Learning rate at step 73550: 0.00072899996303\n",
      "Minibatch loss at step 73550: 0.217198\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 73600: 0.00072899996303\n",
      "Minibatch loss at step 73600: 0.303065\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 73650: 0.00072899996303\n",
      "Minibatch loss at step 73650: 0.132804\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 73700: 0.00072899996303\n",
      "Minibatch loss at step 73700: 0.300690\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 73750: 0.00072899996303\n",
      "Minibatch loss at step 73750: 0.168100\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 73800: 0.00072899996303\n",
      "Minibatch loss at step 73800: 0.245143\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 73850: 0.00072899996303\n",
      "Minibatch loss at step 73850: 0.155758\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 73900: 0.00072899996303\n",
      "Minibatch loss at step 73900: 0.446110\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 73950: 0.00072899996303\n",
      "Minibatch loss at step 73950: 0.249411\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 74000: 0.00072899996303\n",
      "Minibatch loss at step 74000: 0.335998\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 74050: 0.00072899996303\n",
      "Minibatch loss at step 74050: 0.237405\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 74100: 0.00072899996303\n",
      "Minibatch loss at step 74100: 0.106736\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 74150: 0.00072899996303\n",
      "Minibatch loss at step 74150: 0.175788\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 74200: 0.00072899996303\n",
      "Minibatch loss at step 74200: 0.108321\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 74250: 0.00072899996303\n",
      "Minibatch loss at step 74250: 0.296040\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 74300: 0.00072899996303\n",
      "Minibatch loss at step 74300: 0.072778\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 74350: 0.00072899996303\n",
      "Minibatch loss at step 74350: 0.236286\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 74400: 0.00072899996303\n",
      "Minibatch loss at step 74400: 0.121617\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 74450: 0.00072899996303\n",
      "Minibatch loss at step 74450: 0.083774\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 74500: 0.00072899996303\n",
      "Minibatch loss at step 74500: 0.392915\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 88.9%\n",
      "Learning rate at step 74550: 0.00072899996303\n",
      "Minibatch loss at step 74550: 0.305641\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 74600: 0.00072899996303\n",
      "Minibatch loss at step 74600: 0.285560\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 74650: 0.00072899996303\n",
      "Minibatch loss at step 74650: 0.200541\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 74700: 0.00072899996303\n",
      "Minibatch loss at step 74700: 0.290662\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 74750: 0.00072899996303\n",
      "Minibatch loss at step 74750: 0.513447\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 74800: 0.00072899996303\n",
      "Minibatch loss at step 74800: 0.230351\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 74850: 0.00072899996303\n",
      "Minibatch loss at step 74850: 0.492504\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 74900: 0.00072899996303\n",
      "Minibatch loss at step 74900: 0.362746\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 74950: 0.00072899996303\n",
      "Minibatch loss at step 74950: 0.416863\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 75000: 0.00072899996303\n",
      "Minibatch loss at step 75000: 0.200212\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 88.9%\n",
      "Learning rate at step 75050: 0.00072899996303\n",
      "Minibatch loss at step 75050: 0.219851\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 75100: 0.00072899996303\n",
      "Minibatch loss at step 75100: 0.147002\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 75150: 0.00072899996303\n",
      "Minibatch loss at step 75150: 0.045351\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 75200: 0.00072899996303\n",
      "Minibatch loss at step 75200: 0.123437\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 75250: 0.00072899996303\n",
      "Minibatch loss at step 75250: 0.193650\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 75300: 0.00072899996303\n",
      "Minibatch loss at step 75300: 0.098955\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 75350: 0.00072899996303\n",
      "Minibatch loss at step 75350: 0.718139\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 75400: 0.00072899996303\n",
      "Minibatch loss at step 75400: 0.193541\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 75450: 0.00072899996303\n",
      "Minibatch loss at step 75450: 0.156574\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 75500: 0.00072899996303\n",
      "Minibatch loss at step 75500: 0.114402\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 75550: 0.00072899996303\n",
      "Minibatch loss at step 75550: 0.106546\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 75600: 0.00072899996303\n",
      "Minibatch loss at step 75600: 0.095805\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 75650: 0.00072899996303\n",
      "Minibatch loss at step 75650: 0.113561\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 75700: 0.00072899996303\n",
      "Minibatch loss at step 75700: 0.401985\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 75750: 0.00072899996303\n",
      "Minibatch loss at step 75750: 0.065424\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 75800: 0.00072899996303\n",
      "Minibatch loss at step 75800: 0.298640\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 75850: 0.00072899996303\n",
      "Minibatch loss at step 75850: 0.129489\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 75900: 0.00072899996303\n",
      "Minibatch loss at step 75900: 0.348327\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 75950: 0.00072899996303\n",
      "Minibatch loss at step 75950: 0.568733\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 76000: 0.00072899996303\n",
      "Minibatch loss at step 76000: 0.180008\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 76050: 0.00072899996303\n",
      "Minibatch loss at step 76050: 0.194679\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 76100: 0.00072899996303\n",
      "Minibatch loss at step 76100: 0.382391\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 76150: 0.00072899996303\n",
      "Minibatch loss at step 76150: 0.180189\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 76200: 0.00072899996303\n",
      "Minibatch loss at step 76200: 0.288364\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 76250: 0.00072899996303\n",
      "Minibatch loss at step 76250: 0.169547\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 76300: 0.00072899996303\n",
      "Minibatch loss at step 76300: 0.286972\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 76350: 0.00072899996303\n",
      "Minibatch loss at step 76350: 0.505695\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 76400: 0.00072899996303\n",
      "Minibatch loss at step 76400: 0.172292\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 76450: 0.00072899996303\n",
      "Minibatch loss at step 76450: 0.053821\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 76500: 0.00072899996303\n",
      "Minibatch loss at step 76500: 0.317434\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 76550: 0.00072899996303\n",
      "Minibatch loss at step 76550: 0.091276\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 76600: 0.00072899996303\n",
      "Minibatch loss at step 76600: 0.196225\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 76650: 0.00072899996303\n",
      "Minibatch loss at step 76650: 0.366314\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 76700: 0.00072899996303\n",
      "Minibatch loss at step 76700: 0.098986\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 76750: 0.00072899996303\n",
      "Minibatch loss at step 76750: 0.160093\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 76800: 0.00072899996303\n",
      "Minibatch loss at step 76800: 0.139815\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 76850: 0.00072899996303\n",
      "Minibatch loss at step 76850: 0.280259\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 76900: 0.00072899996303\n",
      "Minibatch loss at step 76900: 0.151370\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 76950: 0.00072899996303\n",
      "Minibatch loss at step 76950: 0.129967\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 77000: 0.00072899996303\n",
      "Minibatch loss at step 77000: 0.437235\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 77050: 0.00072899996303\n",
      "Minibatch loss at step 77050: 0.152216\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 77100: 0.00072899996303\n",
      "Minibatch loss at step 77100: 0.367448\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 77150: 0.00072899996303\n",
      "Minibatch loss at step 77150: 0.174480\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 77200: 0.00072899996303\n",
      "Minibatch loss at step 77200: 0.343741\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 77250: 0.00072899996303\n",
      "Minibatch loss at step 77250: 0.246386\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 77300: 0.00072899996303\n",
      "Minibatch loss at step 77300: 0.139067\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 77350: 0.00072899996303\n",
      "Minibatch loss at step 77350: 0.955597\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 77400: 0.00072899996303\n",
      "Minibatch loss at step 77400: 0.483853\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 77450: 0.00072899996303\n",
      "Minibatch loss at step 77450: 0.556144\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 77500: 0.00072899996303\n",
      "Minibatch loss at step 77500: 0.550432\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 77550: 0.00072899996303\n",
      "Minibatch loss at step 77550: 0.365941\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 77600: 0.00072899996303\n",
      "Minibatch loss at step 77600: 0.245838\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 77650: 0.00072899996303\n",
      "Minibatch loss at step 77650: 0.193123\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 77700: 0.00072899996303\n",
      "Minibatch loss at step 77700: 0.689042\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 77750: 0.00072899996303\n",
      "Minibatch loss at step 77750: 0.641602\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 77800: 0.00072899996303\n",
      "Minibatch loss at step 77800: 0.262353\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 77850: 0.00072899996303\n",
      "Minibatch loss at step 77850: 0.470114\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 77900: 0.00072899996303\n",
      "Minibatch loss at step 77900: 0.252531\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 77950: 0.00072899996303\n",
      "Minibatch loss at step 77950: 0.147934\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 78000: 0.00072899996303\n",
      "Minibatch loss at step 78000: 0.085084\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.1%\n",
      "Learning rate at step 78050: 0.00072899996303\n",
      "Minibatch loss at step 78050: 0.078462\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 78100: 0.00072899996303\n",
      "Minibatch loss at step 78100: 0.051752\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 78150: 0.00072899996303\n",
      "Minibatch loss at step 78150: 0.379053\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 78200: 0.00072899996303\n",
      "Minibatch loss at step 78200: 0.164750\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 78250: 0.00072899996303\n",
      "Minibatch loss at step 78250: 0.370989\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 78300: 0.00072899996303\n",
      "Minibatch loss at step 78300: 0.089412\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 78350: 0.00072899996303\n",
      "Minibatch loss at step 78350: 0.234702\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 78400: 0.00072899996303\n",
      "Minibatch loss at step 78400: 0.112302\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 78450: 0.00072899996303\n",
      "Minibatch loss at step 78450: 0.261360\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 78500: 0.00072899996303\n",
      "Minibatch loss at step 78500: 0.183194\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 78550: 0.00072899996303\n",
      "Minibatch loss at step 78550: 0.369347\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 78600: 0.00072899996303\n",
      "Minibatch loss at step 78600: 0.062636\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 78650: 0.00072899996303\n",
      "Minibatch loss at step 78650: 0.070180\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 78700: 0.00072899996303\n",
      "Minibatch loss at step 78700: 0.057820\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 78750: 0.00072899996303\n",
      "Minibatch loss at step 78750: 0.140157\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 78800: 0.00072899996303\n",
      "Minibatch loss at step 78800: 0.081299\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 78850: 0.00072899996303\n",
      "Minibatch loss at step 78850: 0.264390\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 78900: 0.00072899996303\n",
      "Minibatch loss at step 78900: 0.180488\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 78950: 0.00072899996303\n",
      "Minibatch loss at step 78950: 0.204699\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 79000: 0.00072899996303\n",
      "Minibatch loss at step 79000: 0.298089\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.1%\n",
      "Learning rate at step 79050: 0.00072899996303\n",
      "Minibatch loss at step 79050: 0.227009\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 79100: 0.00072899996303\n",
      "Minibatch loss at step 79100: 0.343419\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 79150: 0.00072899996303\n",
      "Minibatch loss at step 79150: 0.280865\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 79200: 0.00072899996303\n",
      "Minibatch loss at step 79200: 0.310799\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 79250: 0.00072899996303\n",
      "Minibatch loss at step 79250: 0.378330\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 79300: 0.00072899996303\n",
      "Minibatch loss at step 79300: 0.441724\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 79350: 0.00072899996303\n",
      "Minibatch loss at step 79350: 0.250056\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 79400: 0.00072899996303\n",
      "Minibatch loss at step 79400: 0.072278\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 79450: 0.00072899996303\n",
      "Minibatch loss at step 79450: 0.302610\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 79500: 0.00072899996303\n",
      "Minibatch loss at step 79500: 0.172539\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 79550: 0.00072899996303\n",
      "Minibatch loss at step 79550: 0.027906\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 79600: 0.00072899996303\n",
      "Minibatch loss at step 79600: 0.558424\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 79650: 0.00072899996303\n",
      "Minibatch loss at step 79650: 0.221237\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 79700: 0.00072899996303\n",
      "Minibatch loss at step 79700: 0.167575\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 79750: 0.00072899996303\n",
      "Minibatch loss at step 79750: 0.052445\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 79800: 0.00072899996303\n",
      "Minibatch loss at step 79800: 0.483039\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 79850: 0.00072899996303\n",
      "Minibatch loss at step 79850: 0.131762\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 79900: 0.00072899996303\n",
      "Minibatch loss at step 79900: 0.173045\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 79950: 0.00072899996303\n",
      "Minibatch loss at step 79950: 0.143601\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 80000: 0.00065609993180\n",
      "Minibatch loss at step 80000: 0.518543\n",
      "Minibatch accuracy: 84.4%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 80050: 0.00065609993180\n",
      "Minibatch loss at step 80050: 0.237463\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 80100: 0.00065609993180\n",
      "Minibatch loss at step 80100: 0.235072\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 80150: 0.00065609993180\n",
      "Minibatch loss at step 80150: 0.401071\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 80200: 0.00065609993180\n",
      "Minibatch loss at step 80200: 0.099135\n",
      "Minibatch accuracy: 95.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 80250: 0.00065609993180\n",
      "Minibatch loss at step 80250: 0.179556\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 80300: 0.00065609993180\n",
      "Minibatch loss at step 80300: 0.207348\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 80350: 0.00065609993180\n",
      "Minibatch loss at step 80350: 0.194655\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 80400: 0.00065609993180\n",
      "Minibatch loss at step 80400: 0.665573\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 80450: 0.00065609993180\n",
      "Minibatch loss at step 80450: 0.756706\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 80500: 0.00065609993180\n",
      "Minibatch loss at step 80500: 0.423733\n",
      "Minibatch accuracy: 87.5%\n",
      "validation accuracy: 89.3%\n",
      "Learning rate at step 80550: 0.00065609993180\n",
      "Minibatch loss at step 80550: 0.553151\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 80600: 0.00065609993180\n",
      "Minibatch loss at step 80600: 0.376660\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 80650: 0.00065609993180\n",
      "Minibatch loss at step 80650: 0.162827\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 80700: 0.00065609993180\n",
      "Minibatch loss at step 80700: 0.251100\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 80750: 0.00065609993180\n",
      "Minibatch loss at step 80750: 0.128903\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 80800: 0.00065609993180\n",
      "Minibatch loss at step 80800: 0.087151\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 80850: 0.00065609993180\n",
      "Minibatch loss at step 80850: 0.106799\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 80900: 0.00065609993180\n",
      "Minibatch loss at step 80900: 0.083778\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 80950: 0.00065609993180\n",
      "Minibatch loss at step 80950: 0.125431\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 81000: 0.00065609993180\n",
      "Minibatch loss at step 81000: 0.083285\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.2%\n",
      "Learning rate at step 81050: 0.00065609993180\n",
      "Minibatch loss at step 81050: 0.178141\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 81100: 0.00065609993180\n",
      "Minibatch loss at step 81100: 0.160259\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 81150: 0.00065609993180\n",
      "Minibatch loss at step 81150: 0.175752\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 81200: 0.00065609993180\n",
      "Minibatch loss at step 81200: 0.196396\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 81250: 0.00065609993180\n",
      "Minibatch loss at step 81250: 0.226159\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 81300: 0.00065609993180\n",
      "Minibatch loss at step 81300: 0.094279\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 81350: 0.00065609993180\n",
      "Minibatch loss at step 81350: 0.206357\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 81400: 0.00065609993180\n",
      "Minibatch loss at step 81400: 0.120600\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 81450: 0.00065609993180\n",
      "Minibatch loss at step 81450: 0.155699\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 81500: 0.00065609993180\n",
      "Minibatch loss at step 81500: 0.277694\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 81550: 0.00065609993180\n",
      "Minibatch loss at step 81550: 0.266167\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 81600: 0.00065609993180\n",
      "Minibatch loss at step 81600: 0.377616\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 81650: 0.00065609993180\n",
      "Minibatch loss at step 81650: 0.094919\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 81700: 0.00065609993180\n",
      "Minibatch loss at step 81700: 0.019048\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 81750: 0.00065609993180\n",
      "Minibatch loss at step 81750: 0.281839\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 81800: 0.00065609993180\n",
      "Minibatch loss at step 81800: 0.111407\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 81850: 0.00065609993180\n",
      "Minibatch loss at step 81850: 0.186013\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 81900: 0.00065609993180\n",
      "Minibatch loss at step 81900: 0.045796\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 81950: 0.00065609993180\n",
      "Minibatch loss at step 81950: 0.372203\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 82000: 0.00065609993180\n",
      "Minibatch loss at step 82000: 0.133908\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.3%\n",
      "Learning rate at step 82050: 0.00065609993180\n",
      "Minibatch loss at step 82050: 0.335062\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 82100: 0.00065609993180\n",
      "Minibatch loss at step 82100: 0.224080\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 82150: 0.00065609993180\n",
      "Minibatch loss at step 82150: 0.126762\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 82200: 0.00065609993180\n",
      "Minibatch loss at step 82200: 0.154071\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 82250: 0.00065609993180\n",
      "Minibatch loss at step 82250: 0.105561\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 82300: 0.00065609993180\n",
      "Minibatch loss at step 82300: 0.086628\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 82350: 0.00065609993180\n",
      "Minibatch loss at step 82350: 0.138077\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 82400: 0.00065609993180\n",
      "Minibatch loss at step 82400: 0.696626\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 82450: 0.00065609993180\n",
      "Minibatch loss at step 82450: 0.169592\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 82500: 0.00065609993180\n",
      "Minibatch loss at step 82500: 0.207583\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 82550: 0.00065609993180\n",
      "Minibatch loss at step 82550: 0.577280\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 82600: 0.00065609993180\n",
      "Minibatch loss at step 82600: 0.609603\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 82650: 0.00065609993180\n",
      "Minibatch loss at step 82650: 0.024025\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 82700: 0.00065609993180\n",
      "Minibatch loss at step 82700: 0.165224\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 82750: 0.00065609993180\n",
      "Minibatch loss at step 82750: 0.074330\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 82800: 0.00065609993180\n",
      "Minibatch loss at step 82800: 0.119658\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 82850: 0.00065609993180\n",
      "Minibatch loss at step 82850: 0.226718\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 82900: 0.00065609993180\n",
      "Minibatch loss at step 82900: 0.314327\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 82950: 0.00065609993180\n",
      "Minibatch loss at step 82950: 0.111453\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 83000: 0.00065609993180\n",
      "Minibatch loss at step 83000: 0.106959\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.1%\n",
      "Learning rate at step 83050: 0.00065609993180\n",
      "Minibatch loss at step 83050: 0.700407\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 83100: 0.00065609993180\n",
      "Minibatch loss at step 83100: 0.266595\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 83150: 0.00065609993180\n",
      "Minibatch loss at step 83150: 0.422012\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 83200: 0.00065609993180\n",
      "Minibatch loss at step 83200: 0.679142\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 83250: 0.00065609993180\n",
      "Minibatch loss at step 83250: 0.246459\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 83300: 0.00065609993180\n",
      "Minibatch loss at step 83300: 0.104382\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 83350: 0.00065609993180\n",
      "Minibatch loss at step 83350: 0.485779\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 83400: 0.00065609993180\n",
      "Minibatch loss at step 83400: 0.454382\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 83450: 0.00065609993180\n",
      "Minibatch loss at step 83450: 0.218527\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 83500: 0.00065609993180\n",
      "Minibatch loss at step 83500: 0.316878\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 83550: 0.00065609993180\n",
      "Minibatch loss at step 83550: 0.424376\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 83600: 0.00065609993180\n",
      "Minibatch loss at step 83600: 0.110214\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 83650: 0.00065609993180\n",
      "Minibatch loss at step 83650: 0.281472\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 83700: 0.00065609993180\n",
      "Minibatch loss at step 83700: 0.156010\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 83750: 0.00065609993180\n",
      "Minibatch loss at step 83750: 0.053926\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 83800: 0.00065609993180\n",
      "Minibatch loss at step 83800: 0.046152\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 83850: 0.00065609993180\n",
      "Minibatch loss at step 83850: 0.171482\n",
      "Minibatch accuracy: 95.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 83900: 0.00065609993180\n",
      "Minibatch loss at step 83900: 0.774820\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 83950: 0.00065609993180\n",
      "Minibatch loss at step 83950: 0.156459\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 84000: 0.00065609993180\n",
      "Minibatch loss at step 84000: 0.023809\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 84050: 0.00065609993180\n",
      "Minibatch loss at step 84050: 0.373629\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 84100: 0.00065609993180\n",
      "Minibatch loss at step 84100: 0.344386\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 84150: 0.00065609993180\n",
      "Minibatch loss at step 84150: 0.108407\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 84200: 0.00065609993180\n",
      "Minibatch loss at step 84200: 0.614540\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 84250: 0.00065609993180\n",
      "Minibatch loss at step 84250: 0.091731\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 84300: 0.00065609993180\n",
      "Minibatch loss at step 84300: 0.142134\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 84350: 0.00065609993180\n",
      "Minibatch loss at step 84350: 0.121874\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 84400: 0.00065609993180\n",
      "Minibatch loss at step 84400: 0.037956\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 84450: 0.00065609993180\n",
      "Minibatch loss at step 84450: 0.048648\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 84500: 0.00065609993180\n",
      "Minibatch loss at step 84500: 0.233917\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.3%\n",
      "Learning rate at step 84550: 0.00065609993180\n",
      "Minibatch loss at step 84550: 0.109433\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 84600: 0.00065609993180\n",
      "Minibatch loss at step 84600: 0.266018\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 84650: 0.00065609993180\n",
      "Minibatch loss at step 84650: 0.103208\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 84700: 0.00065609993180\n",
      "Minibatch loss at step 84700: 0.139066\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 84750: 0.00065609993180\n",
      "Minibatch loss at step 84750: 0.224369\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 84800: 0.00065609993180\n",
      "Minibatch loss at step 84800: 0.104055\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 84850: 0.00065609993180\n",
      "Minibatch loss at step 84850: 0.221080\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 84900: 0.00065609993180\n",
      "Minibatch loss at step 84900: 0.323835\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 84950: 0.00065609993180\n",
      "Minibatch loss at step 84950: 0.238108\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 85000: 0.00065609993180\n",
      "Minibatch loss at step 85000: 0.204589\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 85050: 0.00065609993180\n",
      "Minibatch loss at step 85050: 0.157044\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 85100: 0.00065609993180\n",
      "Minibatch loss at step 85100: 0.267528\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 85150: 0.00065609993180\n",
      "Minibatch loss at step 85150: 0.102093\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 85200: 0.00065609993180\n",
      "Minibatch loss at step 85200: 0.233590\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 85250: 0.00065609993180\n",
      "Minibatch loss at step 85250: 0.064507\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 85300: 0.00065609993180\n",
      "Minibatch loss at step 85300: 0.173973\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 85350: 0.00065609993180\n",
      "Minibatch loss at step 85350: 0.130722\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 85400: 0.00065609993180\n",
      "Minibatch loss at step 85400: 0.175509\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 85450: 0.00065609993180\n",
      "Minibatch loss at step 85450: 0.062987\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 85500: 0.00065609993180\n",
      "Minibatch loss at step 85500: 0.411906\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.1%\n",
      "Learning rate at step 85550: 0.00065609993180\n",
      "Minibatch loss at step 85550: 0.115189\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 85600: 0.00065609993180\n",
      "Minibatch loss at step 85600: 0.067616\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 85650: 0.00065609993180\n",
      "Minibatch loss at step 85650: 0.084502\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 85700: 0.00065609993180\n",
      "Minibatch loss at step 85700: 0.273681\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 85750: 0.00065609993180\n",
      "Minibatch loss at step 85750: 0.035693\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 85800: 0.00065609993180\n",
      "Minibatch loss at step 85800: 0.222112\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 85850: 0.00065609993180\n",
      "Minibatch loss at step 85850: 0.076616\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 85900: 0.00065609993180\n",
      "Minibatch loss at step 85900: 0.053527\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 85950: 0.00065609993180\n",
      "Minibatch loss at step 85950: 0.612816\n",
      "Minibatch accuracy: 79.7%\n",
      "Learning rate at step 86000: 0.00065609993180\n",
      "Minibatch loss at step 86000: 0.699890\n",
      "Minibatch accuracy: 82.8%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 86050: 0.00065609993180\n",
      "Minibatch loss at step 86050: 0.276359\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 86100: 0.00065609993180\n",
      "Minibatch loss at step 86100: 0.215517\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 86150: 0.00065609993180\n",
      "Minibatch loss at step 86150: 0.260653\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 86200: 0.00065609993180\n",
      "Minibatch loss at step 86200: 0.142426\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 86250: 0.00065609993180\n",
      "Minibatch loss at step 86250: 0.351325\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 86300: 0.00065609993180\n",
      "Minibatch loss at step 86300: 0.185046\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 86350: 0.00065609993180\n",
      "Minibatch loss at step 86350: 0.667897\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 86400: 0.00065609993180\n",
      "Minibatch loss at step 86400: 0.232450\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 86450: 0.00065609993180\n",
      "Minibatch loss at step 86450: 0.137913\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 86500: 0.00065609993180\n",
      "Minibatch loss at step 86500: 0.085339\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 86550: 0.00065609993180\n",
      "Minibatch loss at step 86550: 0.198648\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 86600: 0.00065609993180\n",
      "Minibatch loss at step 86600: 0.236914\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 86650: 0.00065609993180\n",
      "Minibatch loss at step 86650: 0.116500\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 86700: 0.00065609993180\n",
      "Minibatch loss at step 86700: 0.092350\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 86750: 0.00065609993180\n",
      "Minibatch loss at step 86750: 0.487520\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 86800: 0.00065609993180\n",
      "Minibatch loss at step 86800: 0.038299\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 86850: 0.00065609993180\n",
      "Minibatch loss at step 86850: 0.032850\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 86900: 0.00065609993180\n",
      "Minibatch loss at step 86900: 0.147109\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 86950: 0.00065609993180\n",
      "Minibatch loss at step 86950: 0.240446\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 87000: 0.00065609993180\n",
      "Minibatch loss at step 87000: 0.188528\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 87050: 0.00065609993180\n",
      "Minibatch loss at step 87050: 0.031573\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 87100: 0.00065609993180\n",
      "Minibatch loss at step 87100: 0.268882\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 87150: 0.00065609993180\n",
      "Minibatch loss at step 87150: 0.290804\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 87200: 0.00065609993180\n",
      "Minibatch loss at step 87200: 0.094204\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 87250: 0.00065609993180\n",
      "Minibatch loss at step 87250: 0.265093\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 87300: 0.00065609993180\n",
      "Minibatch loss at step 87300: 0.391712\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 87350: 0.00065609993180\n",
      "Minibatch loss at step 87350: 0.297871\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 87400: 0.00065609993180\n",
      "Minibatch loss at step 87400: 0.116913\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 87450: 0.00065609993180\n",
      "Minibatch loss at step 87450: 0.256797\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 87500: 0.00065609993180\n",
      "Minibatch loss at step 87500: 0.117011\n",
      "Minibatch accuracy: 95.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 90.2%\n",
      "Learning rate at step 87550: 0.00065609993180\n",
      "Minibatch loss at step 87550: 0.143439\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 87600: 0.00065609993180\n",
      "Minibatch loss at step 87600: 0.058645\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 87650: 0.00065609993180\n",
      "Minibatch loss at step 87650: 0.112687\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 87700: 0.00065609993180\n",
      "Minibatch loss at step 87700: 0.072139\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 87750: 0.00065609993180\n",
      "Minibatch loss at step 87750: 0.057966\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 87800: 0.00065609993180\n",
      "Minibatch loss at step 87800: 0.096607\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 87850: 0.00065609993180\n",
      "Minibatch loss at step 87850: 0.212655\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 87900: 0.00065609993180\n",
      "Minibatch loss at step 87900: 0.278037\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 87950: 0.00065609993180\n",
      "Minibatch loss at step 87950: 0.210025\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 88000: 0.00065609993180\n",
      "Minibatch loss at step 88000: 0.321726\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 88050: 0.00065609993180\n",
      "Minibatch loss at step 88050: 0.072322\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 88100: 0.00065609993180\n",
      "Minibatch loss at step 88100: 0.104843\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 88150: 0.00065609993180\n",
      "Minibatch loss at step 88150: 0.326489\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 88200: 0.00065609993180\n",
      "Minibatch loss at step 88200: 0.416763\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 88250: 0.00065609993180\n",
      "Minibatch loss at step 88250: 0.074588\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 88300: 0.00065609993180\n",
      "Minibatch loss at step 88300: 0.205712\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 88350: 0.00065609993180\n",
      "Minibatch loss at step 88350: 0.111679\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 88400: 0.00065609993180\n",
      "Minibatch loss at step 88400: 0.094384\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 88450: 0.00065609993180\n",
      "Minibatch loss at step 88450: 0.216324\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 88500: 0.00065609993180\n",
      "Minibatch loss at step 88500: 0.081490\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 88550: 0.00065609993180\n",
      "Minibatch loss at step 88550: 0.058719\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 88600: 0.00065609993180\n",
      "Minibatch loss at step 88600: 0.128020\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 88650: 0.00065609993180\n",
      "Minibatch loss at step 88650: 0.170567\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 88700: 0.00065609993180\n",
      "Minibatch loss at step 88700: 0.014803\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 88750: 0.00065609993180\n",
      "Minibatch loss at step 88750: 0.027368\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 88800: 0.00065609993180\n",
      "Minibatch loss at step 88800: 0.295581\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 88850: 0.00065609993180\n",
      "Minibatch loss at step 88850: 0.349360\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 88900: 0.00065609993180\n",
      "Minibatch loss at step 88900: 0.416631\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 88950: 0.00065609993180\n",
      "Minibatch loss at step 88950: 0.317574\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 89000: 0.00065609993180\n",
      "Minibatch loss at step 89000: 0.120281\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 89050: 0.00065609993180\n",
      "Minibatch loss at step 89050: 0.158441\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 89100: 0.00065609993180\n",
      "Minibatch loss at step 89100: 0.354559\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 89150: 0.00065609993180\n",
      "Minibatch loss at step 89150: 0.211608\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 89200: 0.00065609993180\n",
      "Minibatch loss at step 89200: 0.675255\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 89250: 0.00065609993180\n",
      "Minibatch loss at step 89250: 0.453581\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 89300: 0.00065609993180\n",
      "Minibatch loss at step 89300: 0.265468\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 89350: 0.00065609993180\n",
      "Minibatch loss at step 89350: 0.162062\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 89400: 0.00065609993180\n",
      "Minibatch loss at step 89400: 0.250457\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 89450: 0.00065609993180\n",
      "Minibatch loss at step 89450: 0.150845\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 89500: 0.00065609993180\n",
      "Minibatch loss at step 89500: 0.240981\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 89550: 0.00065609993180\n",
      "Minibatch loss at step 89550: 0.212434\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 89600: 0.00065609993180\n",
      "Minibatch loss at step 89600: 0.145163\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 89650: 0.00065609993180\n",
      "Minibatch loss at step 89650: 0.014913\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 89700: 0.00065609993180\n",
      "Minibatch loss at step 89700: 0.351823\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 89750: 0.00065609993180\n",
      "Minibatch loss at step 89750: 0.539819\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 89800: 0.00065609993180\n",
      "Minibatch loss at step 89800: 0.139802\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 89850: 0.00065609993180\n",
      "Minibatch loss at step 89850: 0.025707\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 89900: 0.00065609993180\n",
      "Minibatch loss at step 89900: 0.220470\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 89950: 0.00065609993180\n",
      "Minibatch loss at step 89950: 0.094905\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 90000: 0.00065609993180\n",
      "Minibatch loss at step 90000: 0.085499\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 90050: 0.00065609993180\n",
      "Minibatch loss at step 90050: 0.053563\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 90100: 0.00065609993180\n",
      "Minibatch loss at step 90100: 0.154138\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 90150: 0.00065609993180\n",
      "Minibatch loss at step 90150: 0.715910\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 90200: 0.00065609993180\n",
      "Minibatch loss at step 90200: 0.144678\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 90250: 0.00065609993180\n",
      "Minibatch loss at step 90250: 0.117033\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 90300: 0.00065609993180\n",
      "Minibatch loss at step 90300: 0.114369\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 90350: 0.00065609993180\n",
      "Minibatch loss at step 90350: 0.020369\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 90400: 0.00065609993180\n",
      "Minibatch loss at step 90400: 0.065440\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 90450: 0.00065609993180\n",
      "Minibatch loss at step 90450: 0.234958\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 90500: 0.00065609993180\n",
      "Minibatch loss at step 90500: 0.083207\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 90550: 0.00065609993180\n",
      "Minibatch loss at step 90550: 0.238066\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 90600: 0.00065609993180\n",
      "Minibatch loss at step 90600: 0.103991\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 90650: 0.00065609993180\n",
      "Minibatch loss at step 90650: 0.164498\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 90700: 0.00065609993180\n",
      "Minibatch loss at step 90700: 0.179636\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 90750: 0.00065609993180\n",
      "Minibatch loss at step 90750: 0.003267\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 90800: 0.00065609993180\n",
      "Minibatch loss at step 90800: 0.158815\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 90850: 0.00065609993180\n",
      "Minibatch loss at step 90850: 0.520298\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 90900: 0.00065609993180\n",
      "Minibatch loss at step 90900: 0.105318\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 90950: 0.00065609993180\n",
      "Minibatch loss at step 90950: 0.172746\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 91000: 0.00065609993180\n",
      "Minibatch loss at step 91000: 0.103435\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 91050: 0.00065609993180\n",
      "Minibatch loss at step 91050: 0.108813\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 91100: 0.00065609993180\n",
      "Minibatch loss at step 91100: 0.169717\n",
      "Minibatch accuracy: 95.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 91150: 0.00065609993180\n",
      "Minibatch loss at step 91150: 0.501776\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 91200: 0.00065609993180\n",
      "Minibatch loss at step 91200: 0.079510\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 91250: 0.00065609993180\n",
      "Minibatch loss at step 91250: 0.015899\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 91300: 0.00065609993180\n",
      "Minibatch loss at step 91300: 0.076290\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 91350: 0.00065609993180\n",
      "Minibatch loss at step 91350: 0.039098\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 91400: 0.00065609993180\n",
      "Minibatch loss at step 91400: 0.402529\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 91450: 0.00065609993180\n",
      "Minibatch loss at step 91450: 0.242978\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 91500: 0.00065609993180\n",
      "Minibatch loss at step 91500: 0.134424\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 91550: 0.00065609993180\n",
      "Minibatch loss at step 91550: 0.163772\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 91600: 0.00065609993180\n",
      "Minibatch loss at step 91600: 0.265994\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 91650: 0.00065609993180\n",
      "Minibatch loss at step 91650: 0.163511\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 91700: 0.00065609993180\n",
      "Minibatch loss at step 91700: 0.240044\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 91750: 0.00065609993180\n",
      "Minibatch loss at step 91750: 0.293113\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 91800: 0.00065609993180\n",
      "Minibatch loss at step 91800: 0.043834\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 91850: 0.00065609993180\n",
      "Minibatch loss at step 91850: 0.120155\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 91900: 0.00065609993180\n",
      "Minibatch loss at step 91900: 0.359458\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 91950: 0.00065609993180\n",
      "Minibatch loss at step 91950: 0.144305\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 92000: 0.00065609993180\n",
      "Minibatch loss at step 92000: 0.349728\n",
      "Minibatch accuracy: 82.8%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 92050: 0.00065609993180\n",
      "Minibatch loss at step 92050: 0.379188\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 92100: 0.00065609993180\n",
      "Minibatch loss at step 92100: 0.321892\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 92150: 0.00065609993180\n",
      "Minibatch loss at step 92150: 0.584264\n",
      "Minibatch accuracy: 82.8%\n",
      "Learning rate at step 92200: 0.00065609993180\n",
      "Minibatch loss at step 92200: 0.061708\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 92250: 0.00065609993180\n",
      "Minibatch loss at step 92250: 0.138112\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 92300: 0.00065609993180\n",
      "Minibatch loss at step 92300: 0.308537\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 92350: 0.00065609993180\n",
      "Minibatch loss at step 92350: 0.108033\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 92400: 0.00065609993180\n",
      "Minibatch loss at step 92400: 0.121577\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 92450: 0.00065609993180\n",
      "Minibatch loss at step 92450: 0.057715\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 92500: 0.00065609993180\n",
      "Minibatch loss at step 92500: 0.132291\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 92550: 0.00065609993180\n",
      "Minibatch loss at step 92550: 0.146247\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 92600: 0.00065609993180\n",
      "Minibatch loss at step 92600: 0.111919\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 92650: 0.00065609993180\n",
      "Minibatch loss at step 92650: 0.447882\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 92700: 0.00065609993180\n",
      "Minibatch loss at step 92700: 0.140382\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 92750: 0.00065609993180\n",
      "Minibatch loss at step 92750: 0.172567\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 92800: 0.00065609993180\n",
      "Minibatch loss at step 92800: 0.051700\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 92850: 0.00065609993180\n",
      "Minibatch loss at step 92850: 0.227163\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 92900: 0.00065609993180\n",
      "Minibatch loss at step 92900: 0.242090\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 92950: 0.00065609993180\n",
      "Minibatch loss at step 92950: 0.101482\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 93000: 0.00065609993180\n",
      "Minibatch loss at step 93000: 0.097643\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 93050: 0.00065609993180\n",
      "Minibatch loss at step 93050: 0.131767\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 93100: 0.00065609993180\n",
      "Minibatch loss at step 93100: 0.092572\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 93150: 0.00065609993180\n",
      "Minibatch loss at step 93150: 0.154188\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 93200: 0.00065609993180\n",
      "Minibatch loss at step 93200: 0.066193\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 93250: 0.00065609993180\n",
      "Minibatch loss at step 93250: 0.094198\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 93300: 0.00065609993180\n",
      "Minibatch loss at step 93300: 0.038472\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 93350: 0.00065609993180\n",
      "Minibatch loss at step 93350: 0.119966\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 93400: 0.00065609993180\n",
      "Minibatch loss at step 93400: 0.088627\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 93450: 0.00065609993180\n",
      "Minibatch loss at step 93450: 0.091148\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 93500: 0.00065609993180\n",
      "Minibatch loss at step 93500: 0.241543\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 93550: 0.00065609993180\n",
      "Minibatch loss at step 93550: 0.109383\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 93600: 0.00065609993180\n",
      "Minibatch loss at step 93600: 0.328495\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 93650: 0.00065609993180\n",
      "Minibatch loss at step 93650: 0.047712\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 93700: 0.00065609993180\n",
      "Minibatch loss at step 93700: 0.306548\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 93750: 0.00065609993180\n",
      "Minibatch loss at step 93750: 0.198877\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 93800: 0.00065609993180\n",
      "Minibatch loss at step 93800: 0.143271\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 93850: 0.00065609993180\n",
      "Minibatch loss at step 93850: 0.124867\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 93900: 0.00065609993180\n",
      "Minibatch loss at step 93900: 0.040279\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 93950: 0.00065609993180\n",
      "Minibatch loss at step 93950: 0.246561\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 94000: 0.00065609993180\n",
      "Minibatch loss at step 94000: 0.119833\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 94050: 0.00065609993180\n",
      "Minibatch loss at step 94050: 0.156442\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 94100: 0.00065609993180\n",
      "Minibatch loss at step 94100: 0.144004\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 94150: 0.00065609993180\n",
      "Minibatch loss at step 94150: 0.041703\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 94200: 0.00065609993180\n",
      "Minibatch loss at step 94200: 0.121957\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 94250: 0.00065609993180\n",
      "Minibatch loss at step 94250: 0.189882\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 94300: 0.00065609993180\n",
      "Minibatch loss at step 94300: 0.718514\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 94350: 0.00065609993180\n",
      "Minibatch loss at step 94350: 0.165122\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 94400: 0.00065609993180\n",
      "Minibatch loss at step 94400: 0.079367\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 94450: 0.00065609993180\n",
      "Minibatch loss at step 94450: 0.108980\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 94500: 0.00065609993180\n",
      "Minibatch loss at step 94500: 0.213877\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 94550: 0.00065609993180\n",
      "Minibatch loss at step 94550: 0.995666\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 94600: 0.00065609993180\n",
      "Minibatch loss at step 94600: 0.484195\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 94650: 0.00065609993180\n",
      "Minibatch loss at step 94650: 0.300350\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 94700: 0.00065609993180\n",
      "Minibatch loss at step 94700: 0.161064\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 94750: 0.00065609993180\n",
      "Minibatch loss at step 94750: 0.738824\n",
      "Minibatch accuracy: 85.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 94800: 0.00065609993180\n",
      "Minibatch loss at step 94800: 0.188758\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 94850: 0.00065609993180\n",
      "Minibatch loss at step 94850: 0.630154\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 94900: 0.00065609993180\n",
      "Minibatch loss at step 94900: 0.404010\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 94950: 0.00065609993180\n",
      "Minibatch loss at step 94950: 0.319178\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 95000: 0.00065609993180\n",
      "Minibatch loss at step 95000: 0.048979\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 95050: 0.00065609993180\n",
      "Minibatch loss at step 95050: 0.320800\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 95100: 0.00065609993180\n",
      "Minibatch loss at step 95100: 0.209685\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 95150: 0.00065609993180\n",
      "Minibatch loss at step 95150: 0.214834\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 95200: 0.00065609993180\n",
      "Minibatch loss at step 95200: 0.091748\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 95250: 0.00065609993180\n",
      "Minibatch loss at step 95250: 0.193501\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 95300: 0.00065609993180\n",
      "Minibatch loss at step 95300: 0.158508\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 95350: 0.00065609993180\n",
      "Minibatch loss at step 95350: 0.135576\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 95400: 0.00065609993180\n",
      "Minibatch loss at step 95400: 0.258144\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 95450: 0.00065609993180\n",
      "Minibatch loss at step 95450: 0.142839\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 95500: 0.00065609993180\n",
      "Minibatch loss at step 95500: 0.143461\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.6%\n",
      "Learning rate at step 95550: 0.00065609993180\n",
      "Minibatch loss at step 95550: 0.077229\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 95600: 0.00065609993180\n",
      "Minibatch loss at step 95600: 0.268642\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 95650: 0.00065609993180\n",
      "Minibatch loss at step 95650: 0.264889\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 95700: 0.00065609993180\n",
      "Minibatch loss at step 95700: 0.139522\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 95750: 0.00065609993180\n",
      "Minibatch loss at step 95750: 0.176477\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 95800: 0.00065609993180\n",
      "Minibatch loss at step 95800: 0.069116\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 95850: 0.00065609993180\n",
      "Minibatch loss at step 95850: 0.158303\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 95900: 0.00065609993180\n",
      "Minibatch loss at step 95900: 0.048348\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 95950: 0.00065609993180\n",
      "Minibatch loss at step 95950: 0.192686\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 96000: 0.00065609993180\n",
      "Minibatch loss at step 96000: 0.334431\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 96050: 0.00065609993180\n",
      "Minibatch loss at step 96050: 0.030995\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 96100: 0.00065609993180\n",
      "Minibatch loss at step 96100: 0.100588\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 96150: 0.00065609993180\n",
      "Minibatch loss at step 96150: 0.216640\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 96200: 0.00065609993180\n",
      "Minibatch loss at step 96200: 0.233319\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 96250: 0.00065609993180\n",
      "Minibatch loss at step 96250: 0.102519\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 96300: 0.00065609993180\n",
      "Minibatch loss at step 96300: 0.140088\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 96350: 0.00065609993180\n",
      "Minibatch loss at step 96350: 0.121868\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 96400: 0.00065609993180\n",
      "Minibatch loss at step 96400: 0.127873\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 96450: 0.00065609993180\n",
      "Minibatch loss at step 96450: 0.111841\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 96500: 0.00065609993180\n",
      "Minibatch loss at step 96500: 0.091381\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 96550: 0.00065609993180\n",
      "Minibatch loss at step 96550: 0.067834\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 96600: 0.00065609993180\n",
      "Minibatch loss at step 96600: 0.038895\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 96650: 0.00065609993180\n",
      "Minibatch loss at step 96650: 0.251970\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 96700: 0.00065609993180\n",
      "Minibatch loss at step 96700: 0.148027\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 96750: 0.00065609993180\n",
      "Minibatch loss at step 96750: 0.098072\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 96800: 0.00065609993180\n",
      "Minibatch loss at step 96800: 0.103474\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 96850: 0.00065609993180\n",
      "Minibatch loss at step 96850: 0.192199\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 96900: 0.00065609993180\n",
      "Minibatch loss at step 96900: 0.209433\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 96950: 0.00065609993180\n",
      "Minibatch loss at step 96950: 0.127831\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 97000: 0.00065609993180\n",
      "Minibatch loss at step 97000: 0.123276\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 97050: 0.00065609993180\n",
      "Minibatch loss at step 97050: 0.053313\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 97100: 0.00065609993180\n",
      "Minibatch loss at step 97100: 0.270826\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 97150: 0.00065609993180\n",
      "Minibatch loss at step 97150: 0.181640\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 97200: 0.00065609993180\n",
      "Minibatch loss at step 97200: 0.090533\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 97250: 0.00065609993180\n",
      "Minibatch loss at step 97250: 0.043860\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 97300: 0.00065609993180\n",
      "Minibatch loss at step 97300: 0.078853\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 97350: 0.00065609993180\n",
      "Minibatch loss at step 97350: 0.132296\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 97400: 0.00065609993180\n",
      "Minibatch loss at step 97400: 0.184289\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 97450: 0.00065609993180\n",
      "Minibatch loss at step 97450: 0.320707\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 97500: 0.00065609993180\n",
      "Minibatch loss at step 97500: 0.208512\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 97550: 0.00065609993180\n",
      "Minibatch loss at step 97550: 0.555251\n",
      "Minibatch accuracy: 81.2%\n",
      "Learning rate at step 97600: 0.00065609993180\n",
      "Minibatch loss at step 97600: 0.175840\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 97650: 0.00065609993180\n",
      "Minibatch loss at step 97650: 0.286431\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 97700: 0.00065609993180\n",
      "Minibatch loss at step 97700: 0.267229\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 97750: 0.00065609993180\n",
      "Minibatch loss at step 97750: 0.360720\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 97800: 0.00065609993180\n",
      "Minibatch loss at step 97800: 0.283958\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 97850: 0.00065609993180\n",
      "Minibatch loss at step 97850: 0.271898\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 97900: 0.00065609993180\n",
      "Minibatch loss at step 97900: 0.322972\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 97950: 0.00065609993180\n",
      "Minibatch loss at step 97950: 0.017319\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 98000: 0.00065609993180\n",
      "Minibatch loss at step 98000: 0.130347\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.3%\n",
      "Learning rate at step 98050: 0.00065609993180\n",
      "Minibatch loss at step 98050: 0.085774\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 98100: 0.00065609993180\n",
      "Minibatch loss at step 98100: 0.027065\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 98150: 0.00065609993180\n",
      "Minibatch loss at step 98150: 0.352674\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 98200: 0.00065609993180\n",
      "Minibatch loss at step 98200: 0.094352\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 98250: 0.00065609993180\n",
      "Minibatch loss at step 98250: 0.055367\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 98300: 0.00065609993180\n",
      "Minibatch loss at step 98300: 0.046455\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 98350: 0.00065609993180\n",
      "Minibatch loss at step 98350: 0.129623\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 98400: 0.00065609993180\n",
      "Minibatch loss at step 98400: 0.213237\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 98450: 0.00065609993180\n",
      "Minibatch loss at step 98450: 0.444160\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 98500: 0.00065609993180\n",
      "Minibatch loss at step 98500: 0.192210\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 98550: 0.00065609993180\n",
      "Minibatch loss at step 98550: 0.057031\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 98600: 0.00065609993180\n",
      "Minibatch loss at step 98600: 0.107826\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 98650: 0.00065609993180\n",
      "Minibatch loss at step 98650: 0.090683\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 98700: 0.00065609993180\n",
      "Minibatch loss at step 98700: 0.070842\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 98750: 0.00065609993180\n",
      "Minibatch loss at step 98750: 0.135422\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 98800: 0.00065609993180\n",
      "Minibatch loss at step 98800: 0.148558\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 98850: 0.00065609993180\n",
      "Minibatch loss at step 98850: 0.023120\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 98900: 0.00065609993180\n",
      "Minibatch loss at step 98900: 0.075561\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 98950: 0.00065609993180\n",
      "Minibatch loss at step 98950: 0.050786\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 99000: 0.00065609993180\n",
      "Minibatch loss at step 99000: 0.116840\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 99050: 0.00065609993180\n",
      "Minibatch loss at step 99050: 0.217151\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 99100: 0.00065609993180\n",
      "Minibatch loss at step 99100: 0.330974\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 99150: 0.00065609993180\n",
      "Minibatch loss at step 99150: 0.306572\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 99200: 0.00065609993180\n",
      "Minibatch loss at step 99200: 0.044171\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 99250: 0.00065609993180\n",
      "Minibatch loss at step 99250: 0.145792\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 99300: 0.00065609993180\n",
      "Minibatch loss at step 99300: 0.275859\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 99350: 0.00065609993180\n",
      "Minibatch loss at step 99350: 0.178899\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 99400: 0.00065609993180\n",
      "Minibatch loss at step 99400: 0.447369\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 99450: 0.00065609993180\n",
      "Minibatch loss at step 99450: 0.014474\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 99500: 0.00065609993180\n",
      "Minibatch loss at step 99500: 0.033835\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 99550: 0.00065609993180\n",
      "Minibatch loss at step 99550: 0.150161\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 99600: 0.00065609993180\n",
      "Minibatch loss at step 99600: 0.233085\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 99650: 0.00065609993180\n",
      "Minibatch loss at step 99650: 0.073369\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 99700: 0.00065609993180\n",
      "Minibatch loss at step 99700: 0.252837\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 99750: 0.00065609993180\n",
      "Minibatch loss at step 99750: 0.219488\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 99800: 0.00065609993180\n",
      "Minibatch loss at step 99800: 0.181864\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 99850: 0.00065609993180\n",
      "Minibatch loss at step 99850: 0.083425\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 99900: 0.00065609993180\n",
      "Minibatch loss at step 99900: 0.190099\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 99950: 0.00065609993180\n",
      "Minibatch loss at step 99950: 0.046330\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 100000: 0.00059048994444\n",
      "Minibatch loss at step 100000: 0.119253\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 100050: 0.00059048994444\n",
      "Minibatch loss at step 100050: 0.163445\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 100100: 0.00059048994444\n",
      "Minibatch loss at step 100100: 0.098553\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 100150: 0.00059048994444\n",
      "Minibatch loss at step 100150: 0.228223\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 100200: 0.00059048994444\n",
      "Minibatch loss at step 100200: 0.168412\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 100250: 0.00059048994444\n",
      "Minibatch loss at step 100250: 0.564950\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 100300: 0.00059048994444\n",
      "Minibatch loss at step 100300: 0.271712\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 100350: 0.00059048994444\n",
      "Minibatch loss at step 100350: 0.416137\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 100400: 0.00059048994444\n",
      "Minibatch loss at step 100400: 0.350441\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 100450: 0.00059048994444\n",
      "Minibatch loss at step 100450: 0.463002\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 100500: 0.00059048994444\n",
      "Minibatch loss at step 100500: 0.145643\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.6%\n",
      "Learning rate at step 100550: 0.00059048994444\n",
      "Minibatch loss at step 100550: 0.665256\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 100600: 0.00059048994444\n",
      "Minibatch loss at step 100600: 0.330002\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 100650: 0.00059048994444\n",
      "Minibatch loss at step 100650: 0.290440\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 100700: 0.00059048994444\n",
      "Minibatch loss at step 100700: 0.171342\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 100750: 0.00059048994444\n",
      "Minibatch loss at step 100750: 0.294746\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 100800: 0.00059048994444\n",
      "Minibatch loss at step 100800: 0.019029\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 100850: 0.00059048994444\n",
      "Minibatch loss at step 100850: 0.129205\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 100900: 0.00059048994444\n",
      "Minibatch loss at step 100900: 0.136578\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 100950: 0.00059048994444\n",
      "Minibatch loss at step 100950: 0.069613\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 101000: 0.00059048994444\n",
      "Minibatch loss at step 101000: 0.201226\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 101050: 0.00059048994444\n",
      "Minibatch loss at step 101050: 0.068469\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 101100: 0.00059048994444\n",
      "Minibatch loss at step 101100: 0.085885\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 101150: 0.00059048994444\n",
      "Minibatch loss at step 101150: 0.124377\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 101200: 0.00059048994444\n",
      "Minibatch loss at step 101200: 0.154475\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 101250: 0.00059048994444\n",
      "Minibatch loss at step 101250: 0.032275\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 101300: 0.00059048994444\n",
      "Minibatch loss at step 101300: 0.182348\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 101350: 0.00059048994444\n",
      "Minibatch loss at step 101350: 0.047216\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 101400: 0.00059048994444\n",
      "Minibatch loss at step 101400: 0.157362\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 101450: 0.00059048994444\n",
      "Minibatch loss at step 101450: 0.010289\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 101500: 0.00059048994444\n",
      "Minibatch loss at step 101500: 0.101640\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 101550: 0.00059048994444\n",
      "Minibatch loss at step 101550: 0.151559\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 101600: 0.00059048994444\n",
      "Minibatch loss at step 101600: 0.055535\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 101650: 0.00059048994444\n",
      "Minibatch loss at step 101650: 0.174278\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 101700: 0.00059048994444\n",
      "Minibatch loss at step 101700: 0.035679\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 101750: 0.00059048994444\n",
      "Minibatch loss at step 101750: 0.032435\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 101800: 0.00059048994444\n",
      "Minibatch loss at step 101800: 0.058154\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 101850: 0.00059048994444\n",
      "Minibatch loss at step 101850: 0.039737\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 101900: 0.00059048994444\n",
      "Minibatch loss at step 101900: 0.276856\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 101950: 0.00059048994444\n",
      "Minibatch loss at step 101950: 0.119554\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 102000: 0.00059048994444\n",
      "Minibatch loss at step 102000: 0.250724\n",
      "Minibatch accuracy: 92.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 89.9%\n",
      "Learning rate at step 102050: 0.00059048994444\n",
      "Minibatch loss at step 102050: 0.239270\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 102100: 0.00059048994444\n",
      "Minibatch loss at step 102100: 0.041874\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 102150: 0.00059048994444\n",
      "Minibatch loss at step 102150: 0.133559\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 102200: 0.00059048994444\n",
      "Minibatch loss at step 102200: 0.126451\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 102250: 0.00059048994444\n",
      "Minibatch loss at step 102250: 0.139427\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 102300: 0.00059048994444\n",
      "Minibatch loss at step 102300: 0.187990\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 102350: 0.00059048994444\n",
      "Minibatch loss at step 102350: 0.039971\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 102400: 0.00059048994444\n",
      "Minibatch loss at step 102400: 0.141645\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 102450: 0.00059048994444\n",
      "Minibatch loss at step 102450: 0.126537\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 102500: 0.00059048994444\n",
      "Minibatch loss at step 102500: 0.052838\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 102550: 0.00059048994444\n",
      "Minibatch loss at step 102550: 0.050310\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 102600: 0.00059048994444\n",
      "Minibatch loss at step 102600: 0.209612\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 102650: 0.00059048994444\n",
      "Minibatch loss at step 102650: 0.106120\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 102700: 0.00059048994444\n",
      "Minibatch loss at step 102700: 0.123208\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 102750: 0.00059048994444\n",
      "Minibatch loss at step 102750: 0.141865\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 102800: 0.00059048994444\n",
      "Minibatch loss at step 102800: 0.233405\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 102850: 0.00059048994444\n",
      "Minibatch loss at step 102850: 0.072480\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 102900: 0.00059048994444\n",
      "Minibatch loss at step 102900: 0.115848\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 102950: 0.00059048994444\n",
      "Minibatch loss at step 102950: 0.098372\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 103000: 0.00059048994444\n",
      "Minibatch loss at step 103000: 0.063923\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 103050: 0.00059048994444\n",
      "Minibatch loss at step 103050: 0.101204\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 103100: 0.00059048994444\n",
      "Minibatch loss at step 103100: 0.066485\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 103150: 0.00059048994444\n",
      "Minibatch loss at step 103150: 0.504370\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 103200: 0.00059048994444\n",
      "Minibatch loss at step 103200: 0.428830\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 103250: 0.00059048994444\n",
      "Minibatch loss at step 103250: 0.497662\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 103300: 0.00059048994444\n",
      "Minibatch loss at step 103300: 0.405137\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 103350: 0.00059048994444\n",
      "Minibatch loss at step 103350: 0.112586\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 103400: 0.00059048994444\n",
      "Minibatch loss at step 103400: 0.423823\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 103450: 0.00059048994444\n",
      "Minibatch loss at step 103450: 0.322466\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 103500: 0.00059048994444\n",
      "Minibatch loss at step 103500: 0.400894\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 103550: 0.00059048994444\n",
      "Minibatch loss at step 103550: 0.220167\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 103600: 0.00059048994444\n",
      "Minibatch loss at step 103600: 0.130694\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 103650: 0.00059048994444\n",
      "Minibatch loss at step 103650: 0.100013\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 103700: 0.00059048994444\n",
      "Minibatch loss at step 103700: 0.179802\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 103750: 0.00059048994444\n",
      "Minibatch loss at step 103750: 0.034408\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 103800: 0.00059048994444\n",
      "Minibatch loss at step 103800: 0.274333\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 103850: 0.00059048994444\n",
      "Minibatch loss at step 103850: 0.021703\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 103900: 0.00059048994444\n",
      "Minibatch loss at step 103900: 0.023925\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 103950: 0.00059048994444\n",
      "Minibatch loss at step 103950: 0.030543\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 104000: 0.00059048994444\n",
      "Minibatch loss at step 104000: 0.049818\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 104050: 0.00059048994444\n",
      "Minibatch loss at step 104050: 0.139886\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 104100: 0.00059048994444\n",
      "Minibatch loss at step 104100: 0.025813\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 104150: 0.00059048994444\n",
      "Minibatch loss at step 104150: 0.064445\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 104200: 0.00059048994444\n",
      "Minibatch loss at step 104200: 0.015202\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 104250: 0.00059048994444\n",
      "Minibatch loss at step 104250: 0.043462\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 104300: 0.00059048994444\n",
      "Minibatch loss at step 104300: 0.055052\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 104350: 0.00059048994444\n",
      "Minibatch loss at step 104350: 0.140434\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 104400: 0.00059048994444\n",
      "Minibatch loss at step 104400: 0.216430\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 104450: 0.00059048994444\n",
      "Minibatch loss at step 104450: 0.033484\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 104500: 0.00059048994444\n",
      "Minibatch loss at step 104500: 0.122969\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 104550: 0.00059048994444\n",
      "Minibatch loss at step 104550: 0.031499\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 104600: 0.00059048994444\n",
      "Minibatch loss at step 104600: 0.049665\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 104650: 0.00059048994444\n",
      "Minibatch loss at step 104650: 0.138689\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 104700: 0.00059048994444\n",
      "Minibatch loss at step 104700: 0.133559\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 104750: 0.00059048994444\n",
      "Minibatch loss at step 104750: 0.021746\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 104800: 0.00059048994444\n",
      "Minibatch loss at step 104800: 0.062347\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 104850: 0.00059048994444\n",
      "Minibatch loss at step 104850: 0.091353\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 104900: 0.00059048994444\n",
      "Minibatch loss at step 104900: 0.232488\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 104950: 0.00059048994444\n",
      "Minibatch loss at step 104950: 0.588923\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 105000: 0.00059048994444\n",
      "Minibatch loss at step 105000: 0.041000\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 105050: 0.00059048994444\n",
      "Minibatch loss at step 105050: 0.110566\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 105100: 0.00059048994444\n",
      "Minibatch loss at step 105100: 0.285973\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 105150: 0.00059048994444\n",
      "Minibatch loss at step 105150: 0.193831\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 105200: 0.00059048994444\n",
      "Minibatch loss at step 105200: 0.100117\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 105250: 0.00059048994444\n",
      "Minibatch loss at step 105250: 0.314594\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 105300: 0.00059048994444\n",
      "Minibatch loss at step 105300: 0.337595\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 105350: 0.00059048994444\n",
      "Minibatch loss at step 105350: 0.021789\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 105400: 0.00059048994444\n",
      "Minibatch loss at step 105400: 0.133455\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 105450: 0.00059048994444\n",
      "Minibatch loss at step 105450: 0.134268\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 105500: 0.00059048994444\n",
      "Minibatch loss at step 105500: 0.410972\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 105550: 0.00059048994444\n",
      "Minibatch loss at step 105550: 0.312873\n",
      "Minibatch accuracy: 90.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 105600: 0.00059048994444\n",
      "Minibatch loss at step 105600: 0.221223\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 105650: 0.00059048994444\n",
      "Minibatch loss at step 105650: 0.058792\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 105700: 0.00059048994444\n",
      "Minibatch loss at step 105700: 0.072034\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 105750: 0.00059048994444\n",
      "Minibatch loss at step 105750: 0.015744\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 105800: 0.00059048994444\n",
      "Minibatch loss at step 105800: 0.248557\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 105850: 0.00059048994444\n",
      "Minibatch loss at step 105850: 0.056666\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 105900: 0.00059048994444\n",
      "Minibatch loss at step 105900: 0.041827\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 105950: 0.00059048994444\n",
      "Minibatch loss at step 105950: 0.039769\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 106000: 0.00059048994444\n",
      "Minibatch loss at step 106000: 0.321798\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 90.6%\n",
      "Learning rate at step 106050: 0.00059048994444\n",
      "Minibatch loss at step 106050: 0.288045\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 106100: 0.00059048994444\n",
      "Minibatch loss at step 106100: 0.527768\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 106150: 0.00059048994444\n",
      "Minibatch loss at step 106150: 0.292790\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 106200: 0.00059048994444\n",
      "Minibatch loss at step 106200: 0.123360\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 106250: 0.00059048994444\n",
      "Minibatch loss at step 106250: 0.350510\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 106300: 0.00059048994444\n",
      "Minibatch loss at step 106300: 0.201479\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 106350: 0.00059048994444\n",
      "Minibatch loss at step 106350: 0.437180\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 106400: 0.00059048994444\n",
      "Minibatch loss at step 106400: 0.724541\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 106450: 0.00059048994444\n",
      "Minibatch loss at step 106450: 0.254709\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 106500: 0.00059048994444\n",
      "Minibatch loss at step 106500: 0.689962\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 106550: 0.00059048994444\n",
      "Minibatch loss at step 106550: 0.037162\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 106600: 0.00059048994444\n",
      "Minibatch loss at step 106600: 0.055068\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 106650: 0.00059048994444\n",
      "Minibatch loss at step 106650: 0.004494\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 106700: 0.00059048994444\n",
      "Minibatch loss at step 106700: 0.149396\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 106750: 0.00059048994444\n",
      "Minibatch loss at step 106750: 0.198236\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 106800: 0.00059048994444\n",
      "Minibatch loss at step 106800: 0.037178\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 106850: 0.00059048994444\n",
      "Minibatch loss at step 106850: 0.050067\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 106900: 0.00059048994444\n",
      "Minibatch loss at step 106900: 0.072995\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 106950: 0.00059048994444\n",
      "Minibatch loss at step 106950: 0.041562\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 107000: 0.00059048994444\n",
      "Minibatch loss at step 107000: 0.142505\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.6%\n",
      "Learning rate at step 107050: 0.00059048994444\n",
      "Minibatch loss at step 107050: 0.070052\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 107100: 0.00059048994444\n",
      "Minibatch loss at step 107100: 0.155472\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 107150: 0.00059048994444\n",
      "Minibatch loss at step 107150: 0.250935\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 107200: 0.00059048994444\n",
      "Minibatch loss at step 107200: 0.075069\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 107250: 0.00059048994444\n",
      "Minibatch loss at step 107250: 0.038643\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 107300: 0.00059048994444\n",
      "Minibatch loss at step 107300: 0.366390\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 107350: 0.00059048994444\n",
      "Minibatch loss at step 107350: 0.109210\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 107400: 0.00059048994444\n",
      "Minibatch loss at step 107400: 0.046778\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 107450: 0.00059048994444\n",
      "Minibatch loss at step 107450: 0.034376\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 107500: 0.00059048994444\n",
      "Minibatch loss at step 107500: 0.329979\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 107550: 0.00059048994444\n",
      "Minibatch loss at step 107550: 0.043949\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 107600: 0.00059048994444\n",
      "Minibatch loss at step 107600: 0.232669\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 107650: 0.00059048994444\n",
      "Minibatch loss at step 107650: 0.044758\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 107700: 0.00059048994444\n",
      "Minibatch loss at step 107700: 0.094192\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 107750: 0.00059048994444\n",
      "Minibatch loss at step 107750: 0.220466\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 107800: 0.00059048994444\n",
      "Minibatch loss at step 107800: 0.036188\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 107850: 0.00059048994444\n",
      "Minibatch loss at step 107850: 0.115884\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 107900: 0.00059048994444\n",
      "Minibatch loss at step 107900: 0.088715\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 107950: 0.00059048994444\n",
      "Minibatch loss at step 107950: 0.077435\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 108000: 0.00059048994444\n",
      "Minibatch loss at step 108000: 0.309102\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 108050: 0.00059048994444\n",
      "Minibatch loss at step 108050: 0.140736\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 108100: 0.00059048994444\n",
      "Minibatch loss at step 108100: 0.188083\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 108150: 0.00059048994444\n",
      "Minibatch loss at step 108150: 0.090047\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 108200: 0.00059048994444\n",
      "Minibatch loss at step 108200: 0.332758\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 108250: 0.00059048994444\n",
      "Minibatch loss at step 108250: 0.304631\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 108300: 0.00059048994444\n",
      "Minibatch loss at step 108300: 0.109352\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 108350: 0.00059048994444\n",
      "Minibatch loss at step 108350: 0.215491\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 108400: 0.00059048994444\n",
      "Minibatch loss at step 108400: 0.214426\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 108450: 0.00059048994444\n",
      "Minibatch loss at step 108450: 0.020969\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 108500: 0.00059048994444\n",
      "Minibatch loss at step 108500: 0.131046\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 108550: 0.00059048994444\n",
      "Minibatch loss at step 108550: 0.476878\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 108600: 0.00059048994444\n",
      "Minibatch loss at step 108600: 0.075193\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 108650: 0.00059048994444\n",
      "Minibatch loss at step 108650: 0.057319\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 108700: 0.00059048994444\n",
      "Minibatch loss at step 108700: 0.255030\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 108750: 0.00059048994444\n",
      "Minibatch loss at step 108750: 0.126506\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 108800: 0.00059048994444\n",
      "Minibatch loss at step 108800: 0.036764\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 108850: 0.00059048994444\n",
      "Minibatch loss at step 108850: 0.082707\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 108900: 0.00059048994444\n",
      "Minibatch loss at step 108900: 0.563753\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 108950: 0.00059048994444\n",
      "Minibatch loss at step 108950: 0.359417\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 109000: 0.00059048994444\n",
      "Minibatch loss at step 109000: 0.360319\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 109050: 0.00059048994444\n",
      "Minibatch loss at step 109050: 0.341450\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 109100: 0.00059048994444\n",
      "Minibatch loss at step 109100: 0.245444\n",
      "Minibatch accuracy: 95.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 109150: 0.00059048994444\n",
      "Minibatch loss at step 109150: 0.541381\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 109200: 0.00059048994444\n",
      "Minibatch loss at step 109200: 0.391823\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 109250: 0.00059048994444\n",
      "Minibatch loss at step 109250: 0.435986\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 109300: 0.00059048994444\n",
      "Minibatch loss at step 109300: 0.242978\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 109350: 0.00059048994444\n",
      "Minibatch loss at step 109350: 0.243752\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 109400: 0.00059048994444\n",
      "Minibatch loss at step 109400: 0.015336\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 109450: 0.00059048994444\n",
      "Minibatch loss at step 109450: 0.050584\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 109500: 0.00059048994444\n",
      "Minibatch loss at step 109500: 0.166624\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 109550: 0.00059048994444\n",
      "Minibatch loss at step 109550: 0.544227\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 109600: 0.00059048994444\n",
      "Minibatch loss at step 109600: 0.041090\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 109650: 0.00059048994444\n",
      "Minibatch loss at step 109650: 0.268661\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 109700: 0.00059048994444\n",
      "Minibatch loss at step 109700: 0.138211\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 109750: 0.00059048994444\n",
      "Minibatch loss at step 109750: 0.022375\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 109800: 0.00059048994444\n",
      "Minibatch loss at step 109800: 0.072489\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 109850: 0.00059048994444\n",
      "Minibatch loss at step 109850: 0.022846\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 109900: 0.00059048994444\n",
      "Minibatch loss at step 109900: 0.058339\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 109950: 0.00059048994444\n",
      "Minibatch loss at step 109950: 0.041850\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 110000: 0.00059048994444\n",
      "Minibatch loss at step 110000: 0.067950\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 110050: 0.00059048994444\n",
      "Minibatch loss at step 110050: 0.167445\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 110100: 0.00059048994444\n",
      "Minibatch loss at step 110100: 0.030006\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 110150: 0.00059048994444\n",
      "Minibatch loss at step 110150: 0.030256\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 110200: 0.00059048994444\n",
      "Minibatch loss at step 110200: 0.011042\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 110250: 0.00059048994444\n",
      "Minibatch loss at step 110250: 0.340200\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 110300: 0.00059048994444\n",
      "Minibatch loss at step 110300: 0.204383\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 110350: 0.00059048994444\n",
      "Minibatch loss at step 110350: 0.029315\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 110400: 0.00059048994444\n",
      "Minibatch loss at step 110400: 0.075341\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 110450: 0.00059048994444\n",
      "Minibatch loss at step 110450: 0.231283\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 110500: 0.00059048994444\n",
      "Minibatch loss at step 110500: 0.166031\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.4%\n",
      "Learning rate at step 110550: 0.00059048994444\n",
      "Minibatch loss at step 110550: 0.175942\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 110600: 0.00059048994444\n",
      "Minibatch loss at step 110600: 0.106698\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 110650: 0.00059048994444\n",
      "Minibatch loss at step 110650: 0.037791\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 110700: 0.00059048994444\n",
      "Minibatch loss at step 110700: 0.026128\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 110750: 0.00059048994444\n",
      "Minibatch loss at step 110750: 0.159451\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 110800: 0.00059048994444\n",
      "Minibatch loss at step 110800: 0.085271\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 110850: 0.00059048994444\n",
      "Minibatch loss at step 110850: 0.097079\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 110900: 0.00059048994444\n",
      "Minibatch loss at step 110900: 0.086344\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 110950: 0.00059048994444\n",
      "Minibatch loss at step 110950: 0.153901\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 111000: 0.00059048994444\n",
      "Minibatch loss at step 111000: 0.047708\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 111050: 0.00059048994444\n",
      "Minibatch loss at step 111050: 0.027074\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 111100: 0.00059048994444\n",
      "Minibatch loss at step 111100: 0.106866\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 111150: 0.00059048994444\n",
      "Minibatch loss at step 111150: 0.178667\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 111200: 0.00059048994444\n",
      "Minibatch loss at step 111200: 0.042255\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 111250: 0.00059048994444\n",
      "Minibatch loss at step 111250: 0.031438\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 111300: 0.00059048994444\n",
      "Minibatch loss at step 111300: 0.068853\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 111350: 0.00059048994444\n",
      "Minibatch loss at step 111350: 0.024902\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 111400: 0.00059048994444\n",
      "Minibatch loss at step 111400: 0.104412\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 111450: 0.00059048994444\n",
      "Minibatch loss at step 111450: 0.056706\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 111500: 0.00059048994444\n",
      "Minibatch loss at step 111500: 0.115831\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 111550: 0.00059048994444\n",
      "Minibatch loss at step 111550: 0.182923\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 111600: 0.00059048994444\n",
      "Minibatch loss at step 111600: 0.031995\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 111650: 0.00059048994444\n",
      "Minibatch loss at step 111650: 0.485792\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 111700: 0.00059048994444\n",
      "Minibatch loss at step 111700: 0.307126\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 111750: 0.00059048994444\n",
      "Minibatch loss at step 111750: 0.303962\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 111800: 0.00059048994444\n",
      "Minibatch loss at step 111800: 0.082487\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 111850: 0.00059048994444\n",
      "Minibatch loss at step 111850: 0.116011\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 111900: 0.00059048994444\n",
      "Minibatch loss at step 111900: 0.439592\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 111950: 0.00059048994444\n",
      "Minibatch loss at step 111950: 0.189424\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 112000: 0.00059048994444\n",
      "Minibatch loss at step 112000: 0.133492\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.5%\n",
      "Learning rate at step 112050: 0.00059048994444\n",
      "Minibatch loss at step 112050: 0.232855\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 112100: 0.00059048994444\n",
      "Minibatch loss at step 112100: 0.261127\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 112150: 0.00059048994444\n",
      "Minibatch loss at step 112150: 0.504883\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 112200: 0.00059048994444\n",
      "Minibatch loss at step 112200: 0.276507\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 112250: 0.00059048994444\n",
      "Minibatch loss at step 112250: 0.133682\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 112300: 0.00059048994444\n",
      "Minibatch loss at step 112300: 0.159773\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 112350: 0.00059048994444\n",
      "Minibatch loss at step 112350: 0.101408\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 112400: 0.00059048994444\n",
      "Minibatch loss at step 112400: 0.150439\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 112450: 0.00059048994444\n",
      "Minibatch loss at step 112450: 0.094340\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 112500: 0.00059048994444\n",
      "Minibatch loss at step 112500: 0.117881\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 112550: 0.00059048994444\n",
      "Minibatch loss at step 112550: 0.032461\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 112600: 0.00059048994444\n",
      "Minibatch loss at step 112600: 0.226872\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 112650: 0.00059048994444\n",
      "Minibatch loss at step 112650: 0.078740\n",
      "Minibatch accuracy: 95.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 112700: 0.00059048994444\n",
      "Minibatch loss at step 112700: 0.080935\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 112750: 0.00059048994444\n",
      "Minibatch loss at step 112750: 0.018782\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 112800: 0.00059048994444\n",
      "Minibatch loss at step 112800: 0.046804\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 112850: 0.00059048994444\n",
      "Minibatch loss at step 112850: 0.143438\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 112900: 0.00059048994444\n",
      "Minibatch loss at step 112900: 0.043273\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 112950: 0.00059048994444\n",
      "Minibatch loss at step 112950: 0.135058\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 113000: 0.00059048994444\n",
      "Minibatch loss at step 113000: 0.091842\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 113050: 0.00059048994444\n",
      "Minibatch loss at step 113050: 0.024050\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 113100: 0.00059048994444\n",
      "Minibatch loss at step 113100: 0.064380\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 113150: 0.00059048994444\n",
      "Minibatch loss at step 113150: 0.064576\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 113200: 0.00059048994444\n",
      "Minibatch loss at step 113200: 0.063684\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 113250: 0.00059048994444\n",
      "Minibatch loss at step 113250: 0.107222\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 113300: 0.00059048994444\n",
      "Minibatch loss at step 113300: 0.102670\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 113350: 0.00059048994444\n",
      "Minibatch loss at step 113350: 0.005210\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 113400: 0.00059048994444\n",
      "Minibatch loss at step 113400: 0.057730\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 113450: 0.00059048994444\n",
      "Minibatch loss at step 113450: 0.122471\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 113500: 0.00059048994444\n",
      "Minibatch loss at step 113500: 0.234045\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.5%\n",
      "Learning rate at step 113550: 0.00059048994444\n",
      "Minibatch loss at step 113550: 0.014727\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 113600: 0.00059048994444\n",
      "Minibatch loss at step 113600: 0.279796\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 113650: 0.00059048994444\n",
      "Minibatch loss at step 113650: 0.027237\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 113700: 0.00059048994444\n",
      "Minibatch loss at step 113700: 0.024239\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 113750: 0.00059048994444\n",
      "Minibatch loss at step 113750: 0.070939\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 113800: 0.00059048994444\n",
      "Minibatch loss at step 113800: 0.113445\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 113850: 0.00059048994444\n",
      "Minibatch loss at step 113850: 0.227230\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 113900: 0.00059048994444\n",
      "Minibatch loss at step 113900: 0.352254\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 113950: 0.00059048994444\n",
      "Minibatch loss at step 113950: 0.089971\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 114000: 0.00059048994444\n",
      "Minibatch loss at step 114000: 0.063255\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 114050: 0.00059048994444\n",
      "Minibatch loss at step 114050: 0.421743\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 114100: 0.00059048994444\n",
      "Minibatch loss at step 114100: 0.045849\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 114150: 0.00059048994444\n",
      "Minibatch loss at step 114150: 0.025707\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 114200: 0.00059048994444\n",
      "Minibatch loss at step 114200: 0.008583\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 114250: 0.00059048994444\n",
      "Minibatch loss at step 114250: 0.012826\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 114300: 0.00059048994444\n",
      "Minibatch loss at step 114300: 0.331732\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 114350: 0.00059048994444\n",
      "Minibatch loss at step 114350: 0.043417\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 114400: 0.00059048994444\n",
      "Minibatch loss at step 114400: 0.117631\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 114450: 0.00059048994444\n",
      "Minibatch loss at step 114450: 0.094369\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 114500: 0.00059048994444\n",
      "Minibatch loss at step 114500: 0.279098\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 114550: 0.00059048994444\n",
      "Minibatch loss at step 114550: 0.048963\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 114600: 0.00059048994444\n",
      "Minibatch loss at step 114600: 0.831357\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 114650: 0.00059048994444\n",
      "Minibatch loss at step 114650: 0.131830\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 114700: 0.00059048994444\n",
      "Minibatch loss at step 114700: 0.468929\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 114750: 0.00059048994444\n",
      "Minibatch loss at step 114750: 0.096798\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 114800: 0.00059048994444\n",
      "Minibatch loss at step 114800: 0.252140\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 114850: 0.00059048994444\n",
      "Minibatch loss at step 114850: 0.068128\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 114900: 0.00059048994444\n",
      "Minibatch loss at step 114900: 0.264542\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 114950: 0.00059048994444\n",
      "Minibatch loss at step 114950: 0.238502\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 115000: 0.00059048994444\n",
      "Minibatch loss at step 115000: 0.553247\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 89.6%\n",
      "Learning rate at step 115050: 0.00059048994444\n",
      "Minibatch loss at step 115050: 0.156934\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 115100: 0.00059048994444\n",
      "Minibatch loss at step 115100: 0.184595\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 115150: 0.00059048994444\n",
      "Minibatch loss at step 115150: 0.233363\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 115200: 0.00059048994444\n",
      "Minibatch loss at step 115200: 0.101148\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 115250: 0.00059048994444\n",
      "Minibatch loss at step 115250: 0.256951\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 115300: 0.00059048994444\n",
      "Minibatch loss at step 115300: 0.139786\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 115350: 0.00059048994444\n",
      "Minibatch loss at step 115350: 0.132789\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 115400: 0.00059048994444\n",
      "Minibatch loss at step 115400: 0.047404\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 115450: 0.00059048994444\n",
      "Minibatch loss at step 115450: 0.168727\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 115500: 0.00059048994444\n",
      "Minibatch loss at step 115500: 0.034231\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 115550: 0.00059048994444\n",
      "Minibatch loss at step 115550: 0.252340\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 115600: 0.00059048994444\n",
      "Minibatch loss at step 115600: 0.030094\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 115650: 0.00059048994444\n",
      "Minibatch loss at step 115650: 0.112817\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 115700: 0.00059048994444\n",
      "Minibatch loss at step 115700: 0.085150\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 115750: 0.00059048994444\n",
      "Minibatch loss at step 115750: 0.176142\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 115800: 0.00059048994444\n",
      "Minibatch loss at step 115800: 0.169900\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 115850: 0.00059048994444\n",
      "Minibatch loss at step 115850: 0.027155\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 115900: 0.00059048994444\n",
      "Minibatch loss at step 115900: 0.111855\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 115950: 0.00059048994444\n",
      "Minibatch loss at step 115950: 0.172572\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 116000: 0.00059048994444\n",
      "Minibatch loss at step 116000: 0.028227\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 116050: 0.00059048994444\n",
      "Minibatch loss at step 116050: 0.145052\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 116100: 0.00059048994444\n",
      "Minibatch loss at step 116100: 0.009675\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 116150: 0.00059048994444\n",
      "Minibatch loss at step 116150: 0.035095\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 116200: 0.00059048994444\n",
      "Minibatch loss at step 116200: 0.165511\n",
      "Minibatch accuracy: 92.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 116250: 0.00059048994444\n",
      "Minibatch loss at step 116250: 0.008495\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 116300: 0.00059048994444\n",
      "Minibatch loss at step 116300: 0.293380\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 116350: 0.00059048994444\n",
      "Minibatch loss at step 116350: 0.088672\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 116400: 0.00059048994444\n",
      "Minibatch loss at step 116400: 0.056786\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 116450: 0.00059048994444\n",
      "Minibatch loss at step 116450: 0.110201\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 116500: 0.00059048994444\n",
      "Minibatch loss at step 116500: 0.317156\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 116550: 0.00059048994444\n",
      "Minibatch loss at step 116550: 0.015209\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 116600: 0.00059048994444\n",
      "Minibatch loss at step 116600: 0.077339\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 116650: 0.00059048994444\n",
      "Minibatch loss at step 116650: 0.190081\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 116700: 0.00059048994444\n",
      "Minibatch loss at step 116700: 0.109027\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 116750: 0.00059048994444\n",
      "Minibatch loss at step 116750: 0.111962\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 116800: 0.00059048994444\n",
      "Minibatch loss at step 116800: 0.162684\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 116850: 0.00059048994444\n",
      "Minibatch loss at step 116850: 0.114738\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 116900: 0.00059048994444\n",
      "Minibatch loss at step 116900: 0.258384\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 116950: 0.00059048994444\n",
      "Minibatch loss at step 116950: 0.014048\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 117000: 0.00059048994444\n",
      "Minibatch loss at step 117000: 0.156528\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 117050: 0.00059048994444\n",
      "Minibatch loss at step 117050: 0.063855\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 117100: 0.00059048994444\n",
      "Minibatch loss at step 117100: 0.072273\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 117150: 0.00059048994444\n",
      "Minibatch loss at step 117150: 0.134661\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 117200: 0.00059048994444\n",
      "Minibatch loss at step 117200: 0.436984\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 117250: 0.00059048994444\n",
      "Minibatch loss at step 117250: 0.041910\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 117300: 0.00059048994444\n",
      "Minibatch loss at step 117300: 0.190992\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 117350: 0.00059048994444\n",
      "Minibatch loss at step 117350: 0.196291\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 117400: 0.00059048994444\n",
      "Minibatch loss at step 117400: 0.128622\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 117450: 0.00059048994444\n",
      "Minibatch loss at step 117450: 1.052310\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 117500: 0.00059048994444\n",
      "Minibatch loss at step 117500: 0.230805\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 117550: 0.00059048994444\n",
      "Minibatch loss at step 117550: 0.402271\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 117600: 0.00059048994444\n",
      "Minibatch loss at step 117600: 0.106200\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 117650: 0.00059048994444\n",
      "Minibatch loss at step 117650: 0.098436\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 117700: 0.00059048994444\n",
      "Minibatch loss at step 117700: 0.179066\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 117750: 0.00059048994444\n",
      "Minibatch loss at step 117750: 0.158561\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 117800: 0.00059048994444\n",
      "Minibatch loss at step 117800: 0.378194\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 117850: 0.00059048994444\n",
      "Minibatch loss at step 117850: 0.304617\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 117900: 0.00059048994444\n",
      "Minibatch loss at step 117900: 0.327554\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 117950: 0.00059048994444\n",
      "Minibatch loss at step 117950: 0.209856\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 118000: 0.00059048994444\n",
      "Minibatch loss at step 118000: 0.145564\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.6%\n",
      "Learning rate at step 118050: 0.00059048994444\n",
      "Minibatch loss at step 118050: 0.085577\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 118100: 0.00059048994444\n",
      "Minibatch loss at step 118100: 0.078984\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 118150: 0.00059048994444\n",
      "Minibatch loss at step 118150: 0.154002\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 118200: 0.00059048994444\n",
      "Minibatch loss at step 118200: 0.436535\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 118250: 0.00059048994444\n",
      "Minibatch loss at step 118250: 0.330316\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 118300: 0.00059048994444\n",
      "Minibatch loss at step 118300: 0.198015\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 118350: 0.00059048994444\n",
      "Minibatch loss at step 118350: 0.117480\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 118400: 0.00059048994444\n",
      "Minibatch loss at step 118400: 0.100294\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 118450: 0.00059048994444\n",
      "Minibatch loss at step 118450: 0.060899\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 118500: 0.00059048994444\n",
      "Minibatch loss at step 118500: 0.046626\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 118550: 0.00059048994444\n",
      "Minibatch loss at step 118550: 0.038135\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 118600: 0.00059048994444\n",
      "Minibatch loss at step 118600: 0.237139\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 118650: 0.00059048994444\n",
      "Minibatch loss at step 118650: 0.072878\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 118700: 0.00059048994444\n",
      "Minibatch loss at step 118700: 0.146484\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 118750: 0.00059048994444\n",
      "Minibatch loss at step 118750: 0.074825\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 118800: 0.00059048994444\n",
      "Minibatch loss at step 118800: 0.065959\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 118850: 0.00059048994444\n",
      "Minibatch loss at step 118850: 0.081711\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 118900: 0.00059048994444\n",
      "Minibatch loss at step 118900: 0.089707\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 118950: 0.00059048994444\n",
      "Minibatch loss at step 118950: 0.275231\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 119000: 0.00059048994444\n",
      "Minibatch loss at step 119000: 0.285516\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 119050: 0.00059048994444\n",
      "Minibatch loss at step 119050: 0.116725\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 119100: 0.00059048994444\n",
      "Minibatch loss at step 119100: 0.324904\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 119150: 0.00059048994444\n",
      "Minibatch loss at step 119150: 0.121122\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 119200: 0.00059048994444\n",
      "Minibatch loss at step 119200: 0.038186\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 119250: 0.00059048994444\n",
      "Minibatch loss at step 119250: 0.205313\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 119300: 0.00059048994444\n",
      "Minibatch loss at step 119300: 0.058265\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 119350: 0.00059048994444\n",
      "Minibatch loss at step 119350: 0.263349\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 119400: 0.00059048994444\n",
      "Minibatch loss at step 119400: 0.086119\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 119450: 0.00059048994444\n",
      "Minibatch loss at step 119450: 0.100054\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 119500: 0.00059048994444\n",
      "Minibatch loss at step 119500: 0.137475\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 119550: 0.00059048994444\n",
      "Minibatch loss at step 119550: 0.073938\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 119600: 0.00059048994444\n",
      "Minibatch loss at step 119600: 0.141845\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 119650: 0.00059048994444\n",
      "Minibatch loss at step 119650: 0.101802\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 119700: 0.00059048994444\n",
      "Minibatch loss at step 119700: 0.043087\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 119750: 0.00059048994444\n",
      "Minibatch loss at step 119750: 0.035309\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 119800: 0.00059048994444\n",
      "Minibatch loss at step 119800: 0.008419\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 119850: 0.00059048994444\n",
      "Minibatch loss at step 119850: 0.032089\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 119900: 0.00059048994444\n",
      "Minibatch loss at step 119900: 0.177243\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 119950: 0.00059048994444\n",
      "Minibatch loss at step 119950: 0.157697\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 120000: 0.00053144095000\n",
      "Minibatch loss at step 120000: 0.064476\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 120050: 0.00053144095000\n",
      "Minibatch loss at step 120050: 0.097599\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 120100: 0.00053144095000\n",
      "Minibatch loss at step 120100: 0.031311\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 120150: 0.00053144095000\n",
      "Minibatch loss at step 120150: 0.020944\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 120200: 0.00053144095000\n",
      "Minibatch loss at step 120200: 0.120446\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 120250: 0.00053144095000\n",
      "Minibatch loss at step 120250: 0.067126\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 120300: 0.00053144095000\n",
      "Minibatch loss at step 120300: 0.033666\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 120350: 0.00053144095000\n",
      "Minibatch loss at step 120350: 0.153676\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 120400: 0.00053144095000\n",
      "Minibatch loss at step 120400: 0.052213\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 120450: 0.00053144095000\n",
      "Minibatch loss at step 120450: 0.108848\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 120500: 0.00053144095000\n",
      "Minibatch loss at step 120500: 0.182995\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 120550: 0.00053144095000\n",
      "Minibatch loss at step 120550: 0.081707\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 120600: 0.00053144095000\n",
      "Minibatch loss at step 120600: 0.154080\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 120650: 0.00053144095000\n",
      "Minibatch loss at step 120650: 0.410420\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 120700: 0.00053144095000\n",
      "Minibatch loss at step 120700: 0.526015\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 120750: 0.00053144095000\n",
      "Minibatch loss at step 120750: 0.468590\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 120800: 0.00053144095000\n",
      "Minibatch loss at step 120800: 0.180600\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 120850: 0.00053144095000\n",
      "Minibatch loss at step 120850: 0.308522\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 120900: 0.00053144095000\n",
      "Minibatch loss at step 120900: 0.116015\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 120950: 0.00053144095000\n",
      "Minibatch loss at step 120950: 0.054685\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 121000: 0.00053144095000\n",
      "Minibatch loss at step 121000: 0.205681\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 90.6%\n",
      "Learning rate at step 121050: 0.00053144095000\n",
      "Minibatch loss at step 121050: 0.068310\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 121100: 0.00053144095000\n",
      "Minibatch loss at step 121100: 0.068500\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 121150: 0.00053144095000\n",
      "Minibatch loss at step 121150: 0.025402\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 121200: 0.00053144095000\n",
      "Minibatch loss at step 121200: 0.040540\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 121250: 0.00053144095000\n",
      "Minibatch loss at step 121250: 0.048102\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 121300: 0.00053144095000\n",
      "Minibatch loss at step 121300: 0.042774\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 121350: 0.00053144095000\n",
      "Minibatch loss at step 121350: 0.027520\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 121400: 0.00053144095000\n",
      "Minibatch loss at step 121400: 0.137983\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 121450: 0.00053144095000\n",
      "Minibatch loss at step 121450: 0.009910\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 121500: 0.00053144095000\n",
      "Minibatch loss at step 121500: 0.125742\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 121550: 0.00053144095000\n",
      "Minibatch loss at step 121550: 0.086167\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 121600: 0.00053144095000\n",
      "Minibatch loss at step 121600: 0.139391\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 121650: 0.00053144095000\n",
      "Minibatch loss at step 121650: 0.086246\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 121700: 0.00053144095000\n",
      "Minibatch loss at step 121700: 0.066659\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 121750: 0.00053144095000\n",
      "Minibatch loss at step 121750: 0.353153\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 121800: 0.00053144095000\n",
      "Minibatch loss at step 121800: 0.125191\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 121850: 0.00053144095000\n",
      "Minibatch loss at step 121850: 0.033654\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 121900: 0.00053144095000\n",
      "Minibatch loss at step 121900: 0.276482\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 121950: 0.00053144095000\n",
      "Minibatch loss at step 121950: 0.002494\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 122000: 0.00053144095000\n",
      "Minibatch loss at step 122000: 0.553734\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 90.5%\n",
      "Learning rate at step 122050: 0.00053144095000\n",
      "Minibatch loss at step 122050: 0.147523\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 122100: 0.00053144095000\n",
      "Minibatch loss at step 122100: 0.048664\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 122150: 0.00053144095000\n",
      "Minibatch loss at step 122150: 0.026808\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 122200: 0.00053144095000\n",
      "Minibatch loss at step 122200: 0.067323\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 122250: 0.00053144095000\n",
      "Minibatch loss at step 122250: 0.106890\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 122300: 0.00053144095000\n",
      "Minibatch loss at step 122300: 0.054832\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 122350: 0.00053144095000\n",
      "Minibatch loss at step 122350: 0.005751\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 122400: 0.00053144095000\n",
      "Minibatch loss at step 122400: 0.090526\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 122450: 0.00053144095000\n",
      "Minibatch loss at step 122450: 0.166635\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 122500: 0.00053144095000\n",
      "Minibatch loss at step 122500: 0.022811\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 122550: 0.00053144095000\n",
      "Minibatch loss at step 122550: 0.367974\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 122600: 0.00053144095000\n",
      "Minibatch loss at step 122600: 0.022489\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 122650: 0.00053144095000\n",
      "Minibatch loss at step 122650: 0.039014\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 122700: 0.00053144095000\n",
      "Minibatch loss at step 122700: 0.342255\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 122750: 0.00053144095000\n",
      "Minibatch loss at step 122750: 0.186090\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 122800: 0.00053144095000\n",
      "Minibatch loss at step 122800: 0.120153\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 122850: 0.00053144095000\n",
      "Minibatch loss at step 122850: 0.007641\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 122900: 0.00053144095000\n",
      "Minibatch loss at step 122900: 0.019796\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 122950: 0.00053144095000\n",
      "Minibatch loss at step 122950: 0.024469\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 123000: 0.00053144095000\n",
      "Minibatch loss at step 123000: 0.205343\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 123050: 0.00053144095000\n",
      "Minibatch loss at step 123050: 0.069948\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 123100: 0.00053144095000\n",
      "Minibatch loss at step 123100: 0.096162\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 123150: 0.00053144095000\n",
      "Minibatch loss at step 123150: 0.102068\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 123200: 0.00053144095000\n",
      "Minibatch loss at step 123200: 0.783506\n",
      "Minibatch accuracy: 84.4%\n",
      "Learning rate at step 123250: 0.00053144095000\n",
      "Minibatch loss at step 123250: 0.058322\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 123300: 0.00053144095000\n",
      "Minibatch loss at step 123300: 0.519173\n",
      "Minibatch accuracy: 89.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 123350: 0.00053144095000\n",
      "Minibatch loss at step 123350: 0.397869\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 123400: 0.00053144095000\n",
      "Minibatch loss at step 123400: 0.198078\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 123450: 0.00053144095000\n",
      "Minibatch loss at step 123450: 0.361817\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 123500: 0.00053144095000\n",
      "Minibatch loss at step 123500: 0.398995\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 89.2%\n",
      "Learning rate at step 123550: 0.00053144095000\n",
      "Minibatch loss at step 123550: 0.178244\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 123600: 0.00053144095000\n",
      "Minibatch loss at step 123600: 0.167930\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 123650: 0.00053144095000\n",
      "Minibatch loss at step 123650: 0.206928\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 123700: 0.00053144095000\n",
      "Minibatch loss at step 123700: 0.102740\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 123750: 0.00053144095000\n",
      "Minibatch loss at step 123750: 0.022623\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 123800: 0.00053144095000\n",
      "Minibatch loss at step 123800: 0.024846\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 123850: 0.00053144095000\n",
      "Minibatch loss at step 123850: 0.061614\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 123900: 0.00053144095000\n",
      "Minibatch loss at step 123900: 0.102629\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 123950: 0.00053144095000\n",
      "Minibatch loss at step 123950: 0.027559\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 124000: 0.00053144095000\n",
      "Minibatch loss at step 124000: 0.285302\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 124050: 0.00053144095000\n",
      "Minibatch loss at step 124050: 0.182267\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 124100: 0.00053144095000\n",
      "Minibatch loss at step 124100: 0.093121\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 124150: 0.00053144095000\n",
      "Minibatch loss at step 124150: 0.038033\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 124200: 0.00053144095000\n",
      "Minibatch loss at step 124200: 0.168436\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 124250: 0.00053144095000\n",
      "Minibatch loss at step 124250: 0.020341\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 124300: 0.00053144095000\n",
      "Minibatch loss at step 124300: 0.014068\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 124350: 0.00053144095000\n",
      "Minibatch loss at step 124350: 0.089487\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 124400: 0.00053144095000\n",
      "Minibatch loss at step 124400: 0.234122\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 124450: 0.00053144095000\n",
      "Minibatch loss at step 124450: 0.051112\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 124500: 0.00053144095000\n",
      "Minibatch loss at step 124500: 0.034664\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 124550: 0.00053144095000\n",
      "Minibatch loss at step 124550: 0.289420\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 124600: 0.00053144095000\n",
      "Minibatch loss at step 124600: 0.016287\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 124650: 0.00053144095000\n",
      "Minibatch loss at step 124650: 0.086292\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 124700: 0.00053144095000\n",
      "Minibatch loss at step 124700: 0.103807\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 124750: 0.00053144095000\n",
      "Minibatch loss at step 124750: 0.232220\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 124800: 0.00053144095000\n",
      "Minibatch loss at step 124800: 0.499557\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 124850: 0.00053144095000\n",
      "Minibatch loss at step 124850: 0.275711\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 124900: 0.00053144095000\n",
      "Minibatch loss at step 124900: 0.288856\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 124950: 0.00053144095000\n",
      "Minibatch loss at step 124950: 0.065325\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 125000: 0.00053144095000\n",
      "Minibatch loss at step 125000: 0.068067\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 125050: 0.00053144095000\n",
      "Minibatch loss at step 125050: 0.111303\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 125100: 0.00053144095000\n",
      "Minibatch loss at step 125100: 0.141532\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 125150: 0.00053144095000\n",
      "Minibatch loss at step 125150: 0.044457\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 125200: 0.00053144095000\n",
      "Minibatch loss at step 125200: 0.047433\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 125250: 0.00053144095000\n",
      "Minibatch loss at step 125250: 0.069616\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 125300: 0.00053144095000\n",
      "Minibatch loss at step 125300: 0.030378\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 125350: 0.00053144095000\n",
      "Minibatch loss at step 125350: 0.025947\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 125400: 0.00053144095000\n",
      "Minibatch loss at step 125400: 0.010764\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 125450: 0.00053144095000\n",
      "Minibatch loss at step 125450: 0.067339\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 125500: 0.00053144095000\n",
      "Minibatch loss at step 125500: 0.275354\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 125550: 0.00053144095000\n",
      "Minibatch loss at step 125550: 0.067124\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 125600: 0.00053144095000\n",
      "Minibatch loss at step 125600: 0.061433\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 125650: 0.00053144095000\n",
      "Minibatch loss at step 125650: 0.102013\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 125700: 0.00053144095000\n",
      "Minibatch loss at step 125700: 0.068243\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 125750: 0.00053144095000\n",
      "Minibatch loss at step 125750: 0.021444\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 125800: 0.00053144095000\n",
      "Minibatch loss at step 125800: 0.188089\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 125850: 0.00053144095000\n",
      "Minibatch loss at step 125850: 0.045129\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 125900: 0.00053144095000\n",
      "Minibatch loss at step 125900: 0.029151\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 125950: 0.00053144095000\n",
      "Minibatch loss at step 125950: 0.011061\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 126000: 0.00053144095000\n",
      "Minibatch loss at step 126000: 0.028350\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 126050: 0.00053144095000\n",
      "Minibatch loss at step 126050: 0.129298\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 126100: 0.00053144095000\n",
      "Minibatch loss at step 126100: 0.178002\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 126150: 0.00053144095000\n",
      "Minibatch loss at step 126150: 0.186444\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 126200: 0.00053144095000\n",
      "Minibatch loss at step 126200: 0.170467\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 126250: 0.00053144095000\n",
      "Minibatch loss at step 126250: 0.535956\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 126300: 0.00053144095000\n",
      "Minibatch loss at step 126300: 0.328962\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 126350: 0.00053144095000\n",
      "Minibatch loss at step 126350: 0.120447\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 126400: 0.00053144095000\n",
      "Minibatch loss at step 126400: 0.200728\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 126450: 0.00053144095000\n",
      "Minibatch loss at step 126450: 0.090205\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 126500: 0.00053144095000\n",
      "Minibatch loss at step 126500: 0.471848\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 126550: 0.00053144095000\n",
      "Minibatch loss at step 126550: 0.165924\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 126600: 0.00053144095000\n",
      "Minibatch loss at step 126600: 0.081469\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 126650: 0.00053144095000\n",
      "Minibatch loss at step 126650: 0.059981\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 126700: 0.00053144095000\n",
      "Minibatch loss at step 126700: 0.144923\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 126750: 0.00053144095000\n",
      "Minibatch loss at step 126750: 0.057726\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 126800: 0.00053144095000\n",
      "Minibatch loss at step 126800: 0.013504\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 126850: 0.00053144095000\n",
      "Minibatch loss at step 126850: 0.224311\n",
      "Minibatch accuracy: 95.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 126900: 0.00053144095000\n",
      "Minibatch loss at step 126900: 0.054732\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 126950: 0.00053144095000\n",
      "Minibatch loss at step 126950: 0.068167\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 127000: 0.00053144095000\n",
      "Minibatch loss at step 127000: 0.156286\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.6%\n",
      "Learning rate at step 127050: 0.00053144095000\n",
      "Minibatch loss at step 127050: 0.021440\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 127100: 0.00053144095000\n",
      "Minibatch loss at step 127100: 0.052464\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 127150: 0.00053144095000\n",
      "Minibatch loss at step 127150: 0.018600\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 127200: 0.00053144095000\n",
      "Minibatch loss at step 127200: 0.126636\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 127250: 0.00053144095000\n",
      "Minibatch loss at step 127250: 0.048033\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 127300: 0.00053144095000\n",
      "Minibatch loss at step 127300: 0.047794\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 127350: 0.00053144095000\n",
      "Minibatch loss at step 127350: 0.001890\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 127400: 0.00053144095000\n",
      "Minibatch loss at step 127400: 0.136967\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 127450: 0.00053144095000\n",
      "Minibatch loss at step 127450: 0.121159\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 127500: 0.00053144095000\n",
      "Minibatch loss at step 127500: 0.188505\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 127550: 0.00053144095000\n",
      "Minibatch loss at step 127550: 0.160004\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 127600: 0.00053144095000\n",
      "Minibatch loss at step 127600: 0.313907\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 127650: 0.00053144095000\n",
      "Minibatch loss at step 127650: 0.160060\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 127700: 0.00053144095000\n",
      "Minibatch loss at step 127700: 0.414730\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 127750: 0.00053144095000\n",
      "Minibatch loss at step 127750: 0.085651\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 127800: 0.00053144095000\n",
      "Minibatch loss at step 127800: 0.059356\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 127850: 0.00053144095000\n",
      "Minibatch loss at step 127850: 0.041610\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 127900: 0.00053144095000\n",
      "Minibatch loss at step 127900: 0.135401\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 127950: 0.00053144095000\n",
      "Minibatch loss at step 127950: 0.030420\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 128000: 0.00053144095000\n",
      "Minibatch loss at step 128000: 0.021684\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 128050: 0.00053144095000\n",
      "Minibatch loss at step 128050: 0.057247\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 128100: 0.00053144095000\n",
      "Minibatch loss at step 128100: 0.277759\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 128150: 0.00053144095000\n",
      "Minibatch loss at step 128150: 0.197967\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 128200: 0.00053144095000\n",
      "Minibatch loss at step 128200: 0.024715\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 128250: 0.00053144095000\n",
      "Minibatch loss at step 128250: 0.103581\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 128300: 0.00053144095000\n",
      "Minibatch loss at step 128300: 0.064893\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 128350: 0.00053144095000\n",
      "Minibatch loss at step 128350: 0.033381\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 128400: 0.00053144095000\n",
      "Minibatch loss at step 128400: 0.238211\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 128450: 0.00053144095000\n",
      "Minibatch loss at step 128450: 0.051029\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 128500: 0.00053144095000\n",
      "Minibatch loss at step 128500: 0.208295\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 128550: 0.00053144095000\n",
      "Minibatch loss at step 128550: 0.159174\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 128600: 0.00053144095000\n",
      "Minibatch loss at step 128600: 0.151819\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 128650: 0.00053144095000\n",
      "Minibatch loss at step 128650: 0.258638\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 128700: 0.00053144095000\n",
      "Minibatch loss at step 128700: 0.084671\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 128750: 0.00053144095000\n",
      "Minibatch loss at step 128750: 0.372718\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 128800: 0.00053144095000\n",
      "Minibatch loss at step 128800: 0.310850\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 128850: 0.00053144095000\n",
      "Minibatch loss at step 128850: 0.012143\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 128900: 0.00053144095000\n",
      "Minibatch loss at step 128900: 0.235968\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 128950: 0.00053144095000\n",
      "Minibatch loss at step 128950: 0.240310\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 129000: 0.00053144095000\n",
      "Minibatch loss at step 129000: 0.255384\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 129050: 0.00053144095000\n",
      "Minibatch loss at step 129050: 0.073298\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 129100: 0.00053144095000\n",
      "Minibatch loss at step 129100: 0.216281\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 129150: 0.00053144095000\n",
      "Minibatch loss at step 129150: 0.127743\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 129200: 0.00053144095000\n",
      "Minibatch loss at step 129200: 0.272900\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 129250: 0.00053144095000\n",
      "Minibatch loss at step 129250: 0.117416\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 129300: 0.00053144095000\n",
      "Minibatch loss at step 129300: 0.127514\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 129350: 0.00053144095000\n",
      "Minibatch loss at step 129350: 0.208538\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 129400: 0.00053144095000\n",
      "Minibatch loss at step 129400: 0.226412\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 129450: 0.00053144095000\n",
      "Minibatch loss at step 129450: 0.153821\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 129500: 0.00053144095000\n",
      "Minibatch loss at step 129500: 0.282767\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 129550: 0.00053144095000\n",
      "Minibatch loss at step 129550: 0.004545\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 129600: 0.00053144095000\n",
      "Minibatch loss at step 129600: 0.101991\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 129650: 0.00053144095000\n",
      "Minibatch loss at step 129650: 0.041000\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 129700: 0.00053144095000\n",
      "Minibatch loss at step 129700: 0.187155\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 129750: 0.00053144095000\n",
      "Minibatch loss at step 129750: 0.022011\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 129800: 0.00053144095000\n",
      "Minibatch loss at step 129800: 0.136646\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 129850: 0.00053144095000\n",
      "Minibatch loss at step 129850: 0.033555\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 129900: 0.00053144095000\n",
      "Minibatch loss at step 129900: 0.143460\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 129950: 0.00053144095000\n",
      "Minibatch loss at step 129950: 0.083232\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 130000: 0.00053144095000\n",
      "Minibatch loss at step 130000: 0.118931\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 130050: 0.00053144095000\n",
      "Minibatch loss at step 130050: 0.209549\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 130100: 0.00053144095000\n",
      "Minibatch loss at step 130100: 0.055277\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 130150: 0.00053144095000\n",
      "Minibatch loss at step 130150: 0.438308\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 130200: 0.00053144095000\n",
      "Minibatch loss at step 130200: 0.088612\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 130250: 0.00053144095000\n",
      "Minibatch loss at step 130250: 0.039525\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 130300: 0.00053144095000\n",
      "Minibatch loss at step 130300: 0.027188\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 130350: 0.00053144095000\n",
      "Minibatch loss at step 130350: 0.147755\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 130400: 0.00053144095000\n",
      "Minibatch loss at step 130400: 0.009133\n",
      "Minibatch accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 130450: 0.00053144095000\n",
      "Minibatch loss at step 130450: 0.036624\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 130500: 0.00053144095000\n",
      "Minibatch loss at step 130500: 0.170660\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.6%\n",
      "Learning rate at step 130550: 0.00053144095000\n",
      "Minibatch loss at step 130550: 0.106748\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 130600: 0.00053144095000\n",
      "Minibatch loss at step 130600: 0.147782\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 130650: 0.00053144095000\n",
      "Minibatch loss at step 130650: 0.191453\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 130700: 0.00053144095000\n",
      "Minibatch loss at step 130700: 0.014667\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 130750: 0.00053144095000\n",
      "Minibatch loss at step 130750: 0.083586\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 130800: 0.00053144095000\n",
      "Minibatch loss at step 130800: 0.032697\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 130850: 0.00053144095000\n",
      "Minibatch loss at step 130850: 0.275627\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 130900: 0.00053144095000\n",
      "Minibatch loss at step 130900: 0.079588\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 130950: 0.00053144095000\n",
      "Minibatch loss at step 130950: 0.016106\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 131000: 0.00053144095000\n",
      "Minibatch loss at step 131000: 0.158161\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 131050: 0.00053144095000\n",
      "Minibatch loss at step 131050: 0.115974\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 131100: 0.00053144095000\n",
      "Minibatch loss at step 131100: 0.086542\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 131150: 0.00053144095000\n",
      "Minibatch loss at step 131150: 0.003371\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 131200: 0.00053144095000\n",
      "Minibatch loss at step 131200: 0.116581\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 131250: 0.00053144095000\n",
      "Minibatch loss at step 131250: 0.149589\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 131300: 0.00053144095000\n",
      "Minibatch loss at step 131300: 0.143921\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 131350: 0.00053144095000\n",
      "Minibatch loss at step 131350: 0.095416\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 131400: 0.00053144095000\n",
      "Minibatch loss at step 131400: 0.166240\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 131450: 0.00053144095000\n",
      "Minibatch loss at step 131450: 0.103418\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 131500: 0.00053144095000\n",
      "Minibatch loss at step 131500: 0.055083\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 131550: 0.00053144095000\n",
      "Minibatch loss at step 131550: 0.005088\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 131600: 0.00053144095000\n",
      "Minibatch loss at step 131600: 0.011866\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 131650: 0.00053144095000\n",
      "Minibatch loss at step 131650: 0.072513\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 131700: 0.00053144095000\n",
      "Minibatch loss at step 131700: 0.132549\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 131750: 0.00053144095000\n",
      "Minibatch loss at step 131750: 0.080641\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 131800: 0.00053144095000\n",
      "Minibatch loss at step 131800: 0.079154\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 131850: 0.00053144095000\n",
      "Minibatch loss at step 131850: 0.166469\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 131900: 0.00053144095000\n",
      "Minibatch loss at step 131900: 0.273667\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 131950: 0.00053144095000\n",
      "Minibatch loss at step 131950: 0.150348\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 132000: 0.00053144095000\n",
      "Minibatch loss at step 132000: 0.094991\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 132050: 0.00053144095000\n",
      "Minibatch loss at step 132050: 0.092494\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 132100: 0.00053144095000\n",
      "Minibatch loss at step 132100: 0.207026\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 132150: 0.00053144095000\n",
      "Minibatch loss at step 132150: 0.400320\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 132200: 0.00053144095000\n",
      "Minibatch loss at step 132200: 0.204844\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 132250: 0.00053144095000\n",
      "Minibatch loss at step 132250: 0.120097\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 132300: 0.00053144095000\n",
      "Minibatch loss at step 132300: 0.020798\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 132350: 0.00053144095000\n",
      "Minibatch loss at step 132350: 0.236559\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 132400: 0.00053144095000\n",
      "Minibatch loss at step 132400: 0.144315\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 132450: 0.00053144095000\n",
      "Minibatch loss at step 132450: 0.013428\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 132500: 0.00053144095000\n",
      "Minibatch loss at step 132500: 0.051382\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 132550: 0.00053144095000\n",
      "Minibatch loss at step 132550: 0.068045\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 132600: 0.00053144095000\n",
      "Minibatch loss at step 132600: 0.198369\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 132650: 0.00053144095000\n",
      "Minibatch loss at step 132650: 0.071471\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 132700: 0.00053144095000\n",
      "Minibatch loss at step 132700: 0.167243\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 132750: 0.00053144095000\n",
      "Minibatch loss at step 132750: 0.051409\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 132800: 0.00053144095000\n",
      "Minibatch loss at step 132800: 0.058187\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 132850: 0.00053144095000\n",
      "Minibatch loss at step 132850: 0.053873\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 132900: 0.00053144095000\n",
      "Minibatch loss at step 132900: 0.056712\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 132950: 0.00053144095000\n",
      "Minibatch loss at step 132950: 0.073898\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 133000: 0.00053144095000\n",
      "Minibatch loss at step 133000: 0.151275\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 133050: 0.00053144095000\n",
      "Minibatch loss at step 133050: 0.255813\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 133100: 0.00053144095000\n",
      "Minibatch loss at step 133100: 0.003766\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 133150: 0.00053144095000\n",
      "Minibatch loss at step 133150: 0.054310\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 133200: 0.00053144095000\n",
      "Minibatch loss at step 133200: 0.011602\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 133250: 0.00053144095000\n",
      "Minibatch loss at step 133250: 0.012030\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 133300: 0.00053144095000\n",
      "Minibatch loss at step 133300: 0.093355\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 133350: 0.00053144095000\n",
      "Minibatch loss at step 133350: 0.007843\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 133400: 0.00053144095000\n",
      "Minibatch loss at step 133400: 0.065783\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 133450: 0.00053144095000\n",
      "Minibatch loss at step 133450: 0.045809\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 133500: 0.00053144095000\n",
      "Minibatch loss at step 133500: 0.085164\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 133550: 0.00053144095000\n",
      "Minibatch loss at step 133550: 0.075435\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 133600: 0.00053144095000\n",
      "Minibatch loss at step 133600: 0.171225\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 133650: 0.00053144095000\n",
      "Minibatch loss at step 133650: 0.172384\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 133700: 0.00053144095000\n",
      "Minibatch loss at step 133700: 0.031902\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 133750: 0.00053144095000\n",
      "Minibatch loss at step 133750: 0.085979\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 133800: 0.00053144095000\n",
      "Minibatch loss at step 133800: 0.014758\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 133850: 0.00053144095000\n",
      "Minibatch loss at step 133850: 0.014431\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 133900: 0.00053144095000\n",
      "Minibatch loss at step 133900: 0.014264\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 133950: 0.00053144095000\n",
      "Minibatch loss at step 133950: 0.132189\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 134000: 0.00053144095000\n",
      "Minibatch loss at step 134000: 0.137675\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 134050: 0.00053144095000\n",
      "Minibatch loss at step 134050: 0.131862\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 134100: 0.00053144095000\n",
      "Minibatch loss at step 134100: 0.030347\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 134150: 0.00053144095000\n",
      "Minibatch loss at step 134150: 0.101049\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 134200: 0.00053144095000\n",
      "Minibatch loss at step 134200: 0.407717\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 134250: 0.00053144095000\n",
      "Minibatch loss at step 134250: 0.015605\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 134300: 0.00053144095000\n",
      "Minibatch loss at step 134300: 0.181905\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 134350: 0.00053144095000\n",
      "Minibatch loss at step 134350: 0.099836\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 134400: 0.00053144095000\n",
      "Minibatch loss at step 134400: 0.079775\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 134450: 0.00053144095000\n",
      "Minibatch loss at step 134450: 0.094637\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 134500: 0.00053144095000\n",
      "Minibatch loss at step 134500: 0.141149\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 134550: 0.00053144095000\n",
      "Minibatch loss at step 134550: 0.163204\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 134600: 0.00053144095000\n",
      "Minibatch loss at step 134600: 0.107809\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 134650: 0.00053144095000\n",
      "Minibatch loss at step 134650: 0.590251\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 134700: 0.00053144095000\n",
      "Minibatch loss at step 134700: 0.474152\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 134750: 0.00053144095000\n",
      "Minibatch loss at step 134750: 0.193683\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 134800: 0.00053144095000\n",
      "Minibatch loss at step 134800: 0.287472\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 134850: 0.00053144095000\n",
      "Minibatch loss at step 134850: 0.062348\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 134900: 0.00053144095000\n",
      "Minibatch loss at step 134900: 0.249366\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 134950: 0.00053144095000\n",
      "Minibatch loss at step 134950: 0.116415\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 135000: 0.00053144095000\n",
      "Minibatch loss at step 135000: 0.575773\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 90.7%\n",
      "Learning rate at step 135050: 0.00053144095000\n",
      "Minibatch loss at step 135050: 0.223898\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 135100: 0.00053144095000\n",
      "Minibatch loss at step 135100: 0.049546\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 135150: 0.00053144095000\n",
      "Minibatch loss at step 135150: 0.212613\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 135200: 0.00053144095000\n",
      "Minibatch loss at step 135200: 0.051221\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 135250: 0.00053144095000\n",
      "Minibatch loss at step 135250: 0.034035\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 135300: 0.00053144095000\n",
      "Minibatch loss at step 135300: 0.139342\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 135350: 0.00053144095000\n",
      "Minibatch loss at step 135350: 0.076271\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 135400: 0.00053144095000\n",
      "Minibatch loss at step 135400: 0.096154\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 135450: 0.00053144095000\n",
      "Minibatch loss at step 135450: 0.043567\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 135500: 0.00053144095000\n",
      "Minibatch loss at step 135500: 0.080749\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 89.7%\n",
      "Learning rate at step 135550: 0.00053144095000\n",
      "Minibatch loss at step 135550: 0.075626\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 135600: 0.00053144095000\n",
      "Minibatch loss at step 135600: 0.085918\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 135650: 0.00053144095000\n",
      "Minibatch loss at step 135650: 0.043419\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 135700: 0.00053144095000\n",
      "Minibatch loss at step 135700: 0.146045\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 135750: 0.00053144095000\n",
      "Minibatch loss at step 135750: 0.058765\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 135800: 0.00053144095000\n",
      "Minibatch loss at step 135800: 0.053450\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 135850: 0.00053144095000\n",
      "Minibatch loss at step 135850: 0.089384\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 135900: 0.00053144095000\n",
      "Minibatch loss at step 135900: 0.129989\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 135950: 0.00053144095000\n",
      "Minibatch loss at step 135950: 0.022764\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 136000: 0.00053144095000\n",
      "Minibatch loss at step 136000: 0.159800\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 136050: 0.00053144095000\n",
      "Minibatch loss at step 136050: 0.078396\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 136100: 0.00053144095000\n",
      "Minibatch loss at step 136100: 0.105331\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 136150: 0.00053144095000\n",
      "Minibatch loss at step 136150: 0.007067\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 136200: 0.00053144095000\n",
      "Minibatch loss at step 136200: 0.125461\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 136250: 0.00053144095000\n",
      "Minibatch loss at step 136250: 0.186836\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 136300: 0.00053144095000\n",
      "Minibatch loss at step 136300: 0.023980\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 136350: 0.00053144095000\n",
      "Minibatch loss at step 136350: 0.077621\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 136400: 0.00053144095000\n",
      "Minibatch loss at step 136400: 0.131138\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 136450: 0.00053144095000\n",
      "Minibatch loss at step 136450: 0.096105\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 136500: 0.00053144095000\n",
      "Minibatch loss at step 136500: 0.089962\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 136550: 0.00053144095000\n",
      "Minibatch loss at step 136550: 0.044469\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 136600: 0.00053144095000\n",
      "Minibatch loss at step 136600: 0.105323\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 136650: 0.00053144095000\n",
      "Minibatch loss at step 136650: 0.017693\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 136700: 0.00053144095000\n",
      "Minibatch loss at step 136700: 0.121711\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 136750: 0.00053144095000\n",
      "Minibatch loss at step 136750: 0.141941\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 136800: 0.00053144095000\n",
      "Minibatch loss at step 136800: 0.058888\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 136850: 0.00053144095000\n",
      "Minibatch loss at step 136850: 0.028288\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 136900: 0.00053144095000\n",
      "Minibatch loss at step 136900: 0.013625\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 136950: 0.00053144095000\n",
      "Minibatch loss at step 136950: 0.094094\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 137000: 0.00053144095000\n",
      "Minibatch loss at step 137000: 0.136728\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 137050: 0.00053144095000\n",
      "Minibatch loss at step 137050: 0.118995\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 137100: 0.00053144095000\n",
      "Minibatch loss at step 137100: 0.017827\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 137150: 0.00053144095000\n",
      "Minibatch loss at step 137150: 0.180340\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 137200: 0.00053144095000\n",
      "Minibatch loss at step 137200: 0.025414\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 137250: 0.00053144095000\n",
      "Minibatch loss at step 137250: 0.139765\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 137300: 0.00053144095000\n",
      "Minibatch loss at step 137300: 0.079387\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 137350: 0.00053144095000\n",
      "Minibatch loss at step 137350: 0.077057\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 137400: 0.00053144095000\n",
      "Minibatch loss at step 137400: 0.012497\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 137450: 0.00053144095000\n",
      "Minibatch loss at step 137450: 0.249230\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 137500: 0.00053144095000\n",
      "Minibatch loss at step 137500: 0.021901\n",
      "Minibatch accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 89.6%\n",
      "Learning rate at step 137550: 0.00053144095000\n",
      "Minibatch loss at step 137550: 0.154943\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 137600: 0.00053144095000\n",
      "Minibatch loss at step 137600: 0.186025\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 137650: 0.00053144095000\n",
      "Minibatch loss at step 137650: 0.045518\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 137700: 0.00053144095000\n",
      "Minibatch loss at step 137700: 0.158420\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 137750: 0.00053144095000\n",
      "Minibatch loss at step 137750: 0.309543\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 137800: 0.00053144095000\n",
      "Minibatch loss at step 137800: 0.120221\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 137850: 0.00053144095000\n",
      "Minibatch loss at step 137850: 0.497809\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 137900: 0.00053144095000\n",
      "Minibatch loss at step 137900: 0.317192\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 137950: 0.00053144095000\n",
      "Minibatch loss at step 137950: 0.090368\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 138000: 0.00053144095000\n",
      "Minibatch loss at step 138000: 0.173252\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 138050: 0.00053144095000\n",
      "Minibatch loss at step 138050: 0.012062\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 138100: 0.00053144095000\n",
      "Minibatch loss at step 138100: 0.087050\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 138150: 0.00053144095000\n",
      "Minibatch loss at step 138150: 0.008127\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 138200: 0.00053144095000\n",
      "Minibatch loss at step 138200: 0.026385\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 138250: 0.00053144095000\n",
      "Minibatch loss at step 138250: 0.199241\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 138300: 0.00053144095000\n",
      "Minibatch loss at step 138300: 0.138038\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 138350: 0.00053144095000\n",
      "Minibatch loss at step 138350: 0.120413\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 138400: 0.00053144095000\n",
      "Minibatch loss at step 138400: 0.087766\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 138450: 0.00053144095000\n",
      "Minibatch loss at step 138450: 0.022401\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 138500: 0.00053144095000\n",
      "Minibatch loss at step 138500: 0.034884\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 138550: 0.00053144095000\n",
      "Minibatch loss at step 138550: 0.075867\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 138600: 0.00053144095000\n",
      "Minibatch loss at step 138600: 0.038164\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 138650: 0.00053144095000\n",
      "Minibatch loss at step 138650: 0.010111\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 138700: 0.00053144095000\n",
      "Minibatch loss at step 138700: 0.017335\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 138750: 0.00053144095000\n",
      "Minibatch loss at step 138750: 0.076210\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 138800: 0.00053144095000\n",
      "Minibatch loss at step 138800: 0.077320\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 138850: 0.00053144095000\n",
      "Minibatch loss at step 138850: 0.267546\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 138900: 0.00053144095000\n",
      "Minibatch loss at step 138900: 0.080195\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 138950: 0.00053144095000\n",
      "Minibatch loss at step 138950: 0.153863\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 139000: 0.00053144095000\n",
      "Minibatch loss at step 139000: 0.129274\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 139050: 0.00053144095000\n",
      "Minibatch loss at step 139050: 0.194004\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 139100: 0.00053144095000\n",
      "Minibatch loss at step 139100: 0.018970\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 139150: 0.00053144095000\n",
      "Minibatch loss at step 139150: 0.071385\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 139200: 0.00053144095000\n",
      "Minibatch loss at step 139200: 0.398466\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 139250: 0.00053144095000\n",
      "Minibatch loss at step 139250: 0.085337\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 139300: 0.00053144095000\n",
      "Minibatch loss at step 139300: 0.034955\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 139350: 0.00053144095000\n",
      "Minibatch loss at step 139350: 0.020025\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 139400: 0.00053144095000\n",
      "Minibatch loss at step 139400: 0.108130\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 139450: 0.00053144095000\n",
      "Minibatch loss at step 139450: 0.046779\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 139500: 0.00053144095000\n",
      "Minibatch loss at step 139500: 0.451921\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 139550: 0.00053144095000\n",
      "Minibatch loss at step 139550: 0.118856\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 139600: 0.00053144095000\n",
      "Minibatch loss at step 139600: 0.035625\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 139650: 0.00053144095000\n",
      "Minibatch loss at step 139650: 0.154534\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 139700: 0.00053144095000\n",
      "Minibatch loss at step 139700: 0.033246\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 139750: 0.00053144095000\n",
      "Minibatch loss at step 139750: 0.497460\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 139800: 0.00053144095000\n",
      "Minibatch loss at step 139800: 0.091646\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 139850: 0.00053144095000\n",
      "Minibatch loss at step 139850: 0.093101\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 139900: 0.00053144095000\n",
      "Minibatch loss at step 139900: 0.078068\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 139950: 0.00053144095000\n",
      "Minibatch loss at step 139950: 0.032361\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 140000: 0.00047829683172\n",
      "Minibatch loss at step 140000: 0.307299\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.5%\n",
      "Learning rate at step 140050: 0.00047829683172\n",
      "Minibatch loss at step 140050: 0.429324\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 140100: 0.00047829683172\n",
      "Minibatch loss at step 140100: 0.168746\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 140150: 0.00047829683172\n",
      "Minibatch loss at step 140150: 0.016903\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 140200: 0.00047829683172\n",
      "Minibatch loss at step 140200: 0.079141\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 140250: 0.00047829683172\n",
      "Minibatch loss at step 140250: 0.191852\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 140300: 0.00047829683172\n",
      "Minibatch loss at step 140300: 0.115506\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 140350: 0.00047829683172\n",
      "Minibatch loss at step 140350: 0.039222\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 140400: 0.00047829683172\n",
      "Minibatch loss at step 140400: 0.114186\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 140450: 0.00047829683172\n",
      "Minibatch loss at step 140450: 0.281713\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 140500: 0.00047829683172\n",
      "Minibatch loss at step 140500: 0.073441\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 140550: 0.00047829683172\n",
      "Minibatch loss at step 140550: 0.503267\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 140600: 0.00047829683172\n",
      "Minibatch loss at step 140600: 0.107181\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 140650: 0.00047829683172\n",
      "Minibatch loss at step 140650: 0.150432\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 140700: 0.00047829683172\n",
      "Minibatch loss at step 140700: 0.207804\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 140750: 0.00047829683172\n",
      "Minibatch loss at step 140750: 0.031402\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 140800: 0.00047829683172\n",
      "Minibatch loss at step 140800: 0.204681\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 140850: 0.00047829683172\n",
      "Minibatch loss at step 140850: 0.063166\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 140900: 0.00047829683172\n",
      "Minibatch loss at step 140900: 0.067123\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 140950: 0.00047829683172\n",
      "Minibatch loss at step 140950: 0.153933\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 141000: 0.00047829683172\n",
      "Minibatch loss at step 141000: 0.058039\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 141050: 0.00047829683172\n",
      "Minibatch loss at step 141050: 0.091320\n",
      "Minibatch accuracy: 95.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 141100: 0.00047829683172\n",
      "Minibatch loss at step 141100: 0.024140\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 141150: 0.00047829683172\n",
      "Minibatch loss at step 141150: 0.305339\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 141200: 0.00047829683172\n",
      "Minibatch loss at step 141200: 0.030076\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 141250: 0.00047829683172\n",
      "Minibatch loss at step 141250: 0.042167\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 141300: 0.00047829683172\n",
      "Minibatch loss at step 141300: 0.033923\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 141350: 0.00047829683172\n",
      "Minibatch loss at step 141350: 0.121819\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 141400: 0.00047829683172\n",
      "Minibatch loss at step 141400: 0.157719\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 141450: 0.00047829683172\n",
      "Minibatch loss at step 141450: 0.038864\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 141500: 0.00047829683172\n",
      "Minibatch loss at step 141500: 0.055480\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 141550: 0.00047829683172\n",
      "Minibatch loss at step 141550: 0.090375\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 141600: 0.00047829683172\n",
      "Minibatch loss at step 141600: 0.013871\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 141650: 0.00047829683172\n",
      "Minibatch loss at step 141650: 0.021289\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 141700: 0.00047829683172\n",
      "Minibatch loss at step 141700: 0.230980\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 141750: 0.00047829683172\n",
      "Minibatch loss at step 141750: 0.007562\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 141800: 0.00047829683172\n",
      "Minibatch loss at step 141800: 0.233617\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 141850: 0.00047829683172\n",
      "Minibatch loss at step 141850: 0.175581\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 141900: 0.00047829683172\n",
      "Minibatch loss at step 141900: 0.088130\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 141950: 0.00047829683172\n",
      "Minibatch loss at step 141950: 0.034269\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 142000: 0.00047829683172\n",
      "Minibatch loss at step 142000: 0.097977\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 142050: 0.00047829683172\n",
      "Minibatch loss at step 142050: 0.089324\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 142100: 0.00047829683172\n",
      "Minibatch loss at step 142100: 0.132711\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 142150: 0.00047829683172\n",
      "Minibatch loss at step 142150: 0.055582\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 142200: 0.00047829683172\n",
      "Minibatch loss at step 142200: 0.011920\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 142250: 0.00047829683172\n",
      "Minibatch loss at step 142250: 0.014586\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 142300: 0.00047829683172\n",
      "Minibatch loss at step 142300: 0.023524\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 142350: 0.00047829683172\n",
      "Minibatch loss at step 142350: 0.078980\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 142400: 0.00047829683172\n",
      "Minibatch loss at step 142400: 0.020858\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 142450: 0.00047829683172\n",
      "Minibatch loss at step 142450: 0.151014\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 142500: 0.00047829683172\n",
      "Minibatch loss at step 142500: 0.063216\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 142550: 0.00047829683172\n",
      "Minibatch loss at step 142550: 0.063437\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 142600: 0.00047829683172\n",
      "Minibatch loss at step 142600: 0.104193\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 142650: 0.00047829683172\n",
      "Minibatch loss at step 142650: 0.029474\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 142700: 0.00047829683172\n",
      "Minibatch loss at step 142700: 0.061721\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 142750: 0.00047829683172\n",
      "Minibatch loss at step 142750: 0.033388\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 142800: 0.00047829683172\n",
      "Minibatch loss at step 142800: 0.036070\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 142850: 0.00047829683172\n",
      "Minibatch loss at step 142850: 0.075032\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 142900: 0.00047829683172\n",
      "Minibatch loss at step 142900: 0.047957\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 142950: 0.00047829683172\n",
      "Minibatch loss at step 142950: 0.081340\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 143000: 0.00047829683172\n",
      "Minibatch loss at step 143000: 0.051669\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 143050: 0.00047829683172\n",
      "Minibatch loss at step 143050: 0.221939\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 143100: 0.00047829683172\n",
      "Minibatch loss at step 143100: 0.243250\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 143150: 0.00047829683172\n",
      "Minibatch loss at step 143150: 0.009114\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 143200: 0.00047829683172\n",
      "Minibatch loss at step 143200: 0.026820\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 143250: 0.00047829683172\n",
      "Minibatch loss at step 143250: 0.104406\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 143300: 0.00047829683172\n",
      "Minibatch loss at step 143300: 0.190571\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 143350: 0.00047829683172\n",
      "Minibatch loss at step 143350: 0.245882\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 143400: 0.00047829683172\n",
      "Minibatch loss at step 143400: 0.418065\n",
      "Minibatch accuracy: 85.9%\n",
      "Learning rate at step 143450: 0.00047829683172\n",
      "Minibatch loss at step 143450: 0.053108\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 143500: 0.00047829683172\n",
      "Minibatch loss at step 143500: 0.504003\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 143550: 0.00047829683172\n",
      "Minibatch loss at step 143550: 0.239334\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 143600: 0.00047829683172\n",
      "Minibatch loss at step 143600: 0.335971\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 143650: 0.00047829683172\n",
      "Minibatch loss at step 143650: 0.186197\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 143700: 0.00047829683172\n",
      "Minibatch loss at step 143700: 0.118086\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 143750: 0.00047829683172\n",
      "Minibatch loss at step 143750: 0.102439\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 143800: 0.00047829683172\n",
      "Minibatch loss at step 143800: 0.186718\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 143850: 0.00047829683172\n",
      "Minibatch loss at step 143850: 0.005151\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 143900: 0.00047829683172\n",
      "Minibatch loss at step 143900: 0.055841\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 143950: 0.00047829683172\n",
      "Minibatch loss at step 143950: 0.069301\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 144000: 0.00047829683172\n",
      "Minibatch loss at step 144000: 0.136589\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 144050: 0.00047829683172\n",
      "Minibatch loss at step 144050: 0.097225\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 144100: 0.00047829683172\n",
      "Minibatch loss at step 144100: 0.009257\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 144150: 0.00047829683172\n",
      "Minibatch loss at step 144150: 0.122974\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 144200: 0.00047829683172\n",
      "Minibatch loss at step 144200: 0.022556\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 144250: 0.00047829683172\n",
      "Minibatch loss at step 144250: 0.232749\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 144300: 0.00047829683172\n",
      "Minibatch loss at step 144300: 0.067532\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 144350: 0.00047829683172\n",
      "Minibatch loss at step 144350: 0.014451\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 144400: 0.00047829683172\n",
      "Minibatch loss at step 144400: 0.017567\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 144450: 0.00047829683172\n",
      "Minibatch loss at step 144450: 0.234514\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 144500: 0.00047829683172\n",
      "Minibatch loss at step 144500: 0.084800\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 144550: 0.00047829683172\n",
      "Minibatch loss at step 144550: 0.150677\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 144600: 0.00047829683172\n",
      "Minibatch loss at step 144600: 0.019718\n",
      "Minibatch accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 144650: 0.00047829683172\n",
      "Minibatch loss at step 144650: 0.007518\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 144700: 0.00047829683172\n",
      "Minibatch loss at step 144700: 0.026576\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 144750: 0.00047829683172\n",
      "Minibatch loss at step 144750: 0.025577\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 144800: 0.00047829683172\n",
      "Minibatch loss at step 144800: 0.163844\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 144850: 0.00047829683172\n",
      "Minibatch loss at step 144850: 0.045808\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 144900: 0.00047829683172\n",
      "Minibatch loss at step 144900: 0.051161\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 144950: 0.00047829683172\n",
      "Minibatch loss at step 144950: 0.009375\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 145000: 0.00047829683172\n",
      "Minibatch loss at step 145000: 0.005260\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 145050: 0.00047829683172\n",
      "Minibatch loss at step 145050: 0.144402\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 145100: 0.00047829683172\n",
      "Minibatch loss at step 145100: 0.013062\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 145150: 0.00047829683172\n",
      "Minibatch loss at step 145150: 0.118822\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 145200: 0.00047829683172\n",
      "Minibatch loss at step 145200: 0.052259\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 145250: 0.00047829683172\n",
      "Minibatch loss at step 145250: 0.055557\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 145300: 0.00047829683172\n",
      "Minibatch loss at step 145300: 0.029278\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 145350: 0.00047829683172\n",
      "Minibatch loss at step 145350: 0.069198\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 145400: 0.00047829683172\n",
      "Minibatch loss at step 145400: 0.048391\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 145450: 0.00047829683172\n",
      "Minibatch loss at step 145450: 0.110730\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 145500: 0.00047829683172\n",
      "Minibatch loss at step 145500: 0.296243\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 145550: 0.00047829683172\n",
      "Minibatch loss at step 145550: 0.017636\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 145600: 0.00047829683172\n",
      "Minibatch loss at step 145600: 0.028608\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 145650: 0.00047829683172\n",
      "Minibatch loss at step 145650: 0.390986\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 145700: 0.00047829683172\n",
      "Minibatch loss at step 145700: 0.095080\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 145750: 0.00047829683172\n",
      "Minibatch loss at step 145750: 0.005354\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 145800: 0.00047829683172\n",
      "Minibatch loss at step 145800: 0.157257\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 145850: 0.00047829683172\n",
      "Minibatch loss at step 145850: 0.217229\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 145900: 0.00047829683172\n",
      "Minibatch loss at step 145900: 0.012468\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 145950: 0.00047829683172\n",
      "Minibatch loss at step 145950: 0.047081\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 146000: 0.00047829683172\n",
      "Minibatch loss at step 146000: 0.168855\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 146050: 0.00047829683172\n",
      "Minibatch loss at step 146050: 0.153745\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 146100: 0.00047829683172\n",
      "Minibatch loss at step 146100: 0.101094\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 146150: 0.00047829683172\n",
      "Minibatch loss at step 146150: 0.153889\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 146200: 0.00047829683172\n",
      "Minibatch loss at step 146200: 0.046874\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 146250: 0.00047829683172\n",
      "Minibatch loss at step 146250: 0.138952\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 146300: 0.00047829683172\n",
      "Minibatch loss at step 146300: 0.069091\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 146350: 0.00047829683172\n",
      "Minibatch loss at step 146350: 0.272787\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 146400: 0.00047829683172\n",
      "Minibatch loss at step 146400: 0.138057\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 146450: 0.00047829683172\n",
      "Minibatch loss at step 146450: 0.330997\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 146500: 0.00047829683172\n",
      "Minibatch loss at step 146500: 0.189554\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.6%\n",
      "Learning rate at step 146550: 0.00047829683172\n",
      "Minibatch loss at step 146550: 0.043206\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 146600: 0.00047829683172\n",
      "Minibatch loss at step 146600: 0.245591\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 146650: 0.00047829683172\n",
      "Minibatch loss at step 146650: 0.064494\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 146700: 0.00047829683172\n",
      "Minibatch loss at step 146700: 0.033142\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 146750: 0.00047829683172\n",
      "Minibatch loss at step 146750: 0.098734\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 146800: 0.00047829683172\n",
      "Minibatch loss at step 146800: 0.022488\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 146850: 0.00047829683172\n",
      "Minibatch loss at step 146850: 0.035140\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 146900: 0.00047829683172\n",
      "Minibatch loss at step 146900: 0.137179\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 146950: 0.00047829683172\n",
      "Minibatch loss at step 146950: 0.115295\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 147000: 0.00047829683172\n",
      "Minibatch loss at step 147000: 0.210528\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 147050: 0.00047829683172\n",
      "Minibatch loss at step 147050: 0.321669\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 147100: 0.00047829683172\n",
      "Minibatch loss at step 147100: 0.148121\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 147150: 0.00047829683172\n",
      "Minibatch loss at step 147150: 0.060569\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 147200: 0.00047829683172\n",
      "Minibatch loss at step 147200: 0.056728\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 147250: 0.00047829683172\n",
      "Minibatch loss at step 147250: 0.045375\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 147300: 0.00047829683172\n",
      "Minibatch loss at step 147300: 0.032887\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 147350: 0.00047829683172\n",
      "Minibatch loss at step 147350: 0.020816\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 147400: 0.00047829683172\n",
      "Minibatch loss at step 147400: 0.026539\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 147450: 0.00047829683172\n",
      "Minibatch loss at step 147450: 0.062628\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 147500: 0.00047829683172\n",
      "Minibatch loss at step 147500: 0.138620\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 147550: 0.00047829683172\n",
      "Minibatch loss at step 147550: 0.103487\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 147600: 0.00047829683172\n",
      "Minibatch loss at step 147600: 0.019401\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 147650: 0.00047829683172\n",
      "Minibatch loss at step 147650: 0.083162\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 147700: 0.00047829683172\n",
      "Minibatch loss at step 147700: 0.003665\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 147750: 0.00047829683172\n",
      "Minibatch loss at step 147750: 0.011561\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 147800: 0.00047829683172\n",
      "Minibatch loss at step 147800: 0.010695\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 147850: 0.00047829683172\n",
      "Minibatch loss at step 147850: 0.159978\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 147900: 0.00047829683172\n",
      "Minibatch loss at step 147900: 0.082706\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 147950: 0.00047829683172\n",
      "Minibatch loss at step 147950: 0.044799\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 148000: 0.00047829683172\n",
      "Minibatch loss at step 148000: 0.050108\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 148050: 0.00047829683172\n",
      "Minibatch loss at step 148050: 0.168540\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 148100: 0.00047829683172\n",
      "Minibatch loss at step 148100: 0.057002\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 148150: 0.00047829683172\n",
      "Minibatch loss at step 148150: 0.028380\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 148200: 0.00047829683172\n",
      "Minibatch loss at step 148200: 0.080472\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 148250: 0.00047829683172\n",
      "Minibatch loss at step 148250: 0.064840\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 148300: 0.00047829683172\n",
      "Minibatch loss at step 148300: 0.079101\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 148350: 0.00047829683172\n",
      "Minibatch loss at step 148350: 0.068727\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 148400: 0.00047829683172\n",
      "Minibatch loss at step 148400: 0.019630\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 148450: 0.00047829683172\n",
      "Minibatch loss at step 148450: 0.008837\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 148500: 0.00047829683172\n",
      "Minibatch loss at step 148500: 0.203072\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 148550: 0.00047829683172\n",
      "Minibatch loss at step 148550: 0.059011\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 148600: 0.00047829683172\n",
      "Minibatch loss at step 148600: 0.286696\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 148650: 0.00047829683172\n",
      "Minibatch loss at step 148650: 0.006572\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 148700: 0.00047829683172\n",
      "Minibatch loss at step 148700: 0.023937\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 148750: 0.00047829683172\n",
      "Minibatch loss at step 148750: 0.073105\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 148800: 0.00047829683172\n",
      "Minibatch loss at step 148800: 0.259199\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 148850: 0.00047829683172\n",
      "Minibatch loss at step 148850: 0.273265\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 148900: 0.00047829683172\n",
      "Minibatch loss at step 148900: 0.052416\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 148950: 0.00047829683172\n",
      "Minibatch loss at step 148950: 0.007178\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 149000: 0.00047829683172\n",
      "Minibatch loss at step 149000: 0.166438\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 149050: 0.00047829683172\n",
      "Minibatch loss at step 149050: 0.144870\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 149100: 0.00047829683172\n",
      "Minibatch loss at step 149100: 0.095663\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 149150: 0.00047829683172\n",
      "Minibatch loss at step 149150: 0.056584\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 149200: 0.00047829683172\n",
      "Minibatch loss at step 149200: 0.298639\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 149250: 0.00047829683172\n",
      "Minibatch loss at step 149250: 0.091994\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 149300: 0.00047829683172\n",
      "Minibatch loss at step 149300: 0.141158\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 149350: 0.00047829683172\n",
      "Minibatch loss at step 149350: 0.246563\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 149400: 0.00047829683172\n",
      "Minibatch loss at step 149400: 0.458767\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 149450: 0.00047829683172\n",
      "Minibatch loss at step 149450: 0.383732\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 149500: 0.00047829683172\n",
      "Minibatch loss at step 149500: 0.007362\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 149550: 0.00047829683172\n",
      "Minibatch loss at step 149550: 0.060001\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 149600: 0.00047829683172\n",
      "Minibatch loss at step 149600: 0.010677\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 149650: 0.00047829683172\n",
      "Minibatch loss at step 149650: 0.029816\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 149700: 0.00047829683172\n",
      "Minibatch loss at step 149700: 0.146992\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 149750: 0.00047829683172\n",
      "Minibatch loss at step 149750: 0.222668\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 149800: 0.00047829683172\n",
      "Minibatch loss at step 149800: 0.003993\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 149850: 0.00047829683172\n",
      "Minibatch loss at step 149850: 0.139880\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 149900: 0.00047829683172\n",
      "Minibatch loss at step 149900: 0.047474\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 149950: 0.00047829683172\n",
      "Minibatch loss at step 149950: 0.075541\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 150000: 0.00047829683172\n",
      "Minibatch loss at step 150000: 0.072628\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 150050: 0.00047829683172\n",
      "Minibatch loss at step 150050: 0.056824\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 150100: 0.00047829683172\n",
      "Minibatch loss at step 150100: 0.136963\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 150150: 0.00047829683172\n",
      "Minibatch loss at step 150150: 0.124701\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 150200: 0.00047829683172\n",
      "Minibatch loss at step 150200: 0.018685\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 150250: 0.00047829683172\n",
      "Minibatch loss at step 150250: 0.022142\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 150300: 0.00047829683172\n",
      "Minibatch loss at step 150300: 0.082081\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 150350: 0.00047829683172\n",
      "Minibatch loss at step 150350: 0.047477\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 150400: 0.00047829683172\n",
      "Minibatch loss at step 150400: 0.006137\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 150450: 0.00047829683172\n",
      "Minibatch loss at step 150450: 0.150390\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 150500: 0.00047829683172\n",
      "Minibatch loss at step 150500: 0.012377\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 150550: 0.00047829683172\n",
      "Minibatch loss at step 150550: 0.112367\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 150600: 0.00047829683172\n",
      "Minibatch loss at step 150600: 0.027340\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 150650: 0.00047829683172\n",
      "Minibatch loss at step 150650: 0.075127\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 150700: 0.00047829683172\n",
      "Minibatch loss at step 150700: 0.031423\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 150750: 0.00047829683172\n",
      "Minibatch loss at step 150750: 0.137890\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 150800: 0.00047829683172\n",
      "Minibatch loss at step 150800: 0.041195\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 150850: 0.00047829683172\n",
      "Minibatch loss at step 150850: 0.133387\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 150900: 0.00047829683172\n",
      "Minibatch loss at step 150900: 0.033027\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 150950: 0.00047829683172\n",
      "Minibatch loss at step 150950: 0.099098\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 151000: 0.00047829683172\n",
      "Minibatch loss at step 151000: 0.114716\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 151050: 0.00047829683172\n",
      "Minibatch loss at step 151050: 0.148356\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 151100: 0.00047829683172\n",
      "Minibatch loss at step 151100: 0.040476\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 151150: 0.00047829683172\n",
      "Minibatch loss at step 151150: 0.115320\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 151200: 0.00047829683172\n",
      "Minibatch loss at step 151200: 0.049166\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 151250: 0.00047829683172\n",
      "Minibatch loss at step 151250: 0.062446\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 151300: 0.00047829683172\n",
      "Minibatch loss at step 151300: 0.071092\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 151350: 0.00047829683172\n",
      "Minibatch loss at step 151350: 0.091573\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 151400: 0.00047829683172\n",
      "Minibatch loss at step 151400: 0.009681\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 151450: 0.00047829683172\n",
      "Minibatch loss at step 151450: 0.128786\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 151500: 0.00047829683172\n",
      "Minibatch loss at step 151500: 0.151833\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 151550: 0.00047829683172\n",
      "Minibatch loss at step 151550: 0.039235\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 151600: 0.00047829683172\n",
      "Minibatch loss at step 151600: 0.091287\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 151650: 0.00047829683172\n",
      "Minibatch loss at step 151650: 0.003043\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 151700: 0.00047829683172\n",
      "Minibatch loss at step 151700: 0.112307\n",
      "Minibatch accuracy: 96.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 151750: 0.00047829683172\n",
      "Minibatch loss at step 151750: 0.024010\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 151800: 0.00047829683172\n",
      "Minibatch loss at step 151800: 0.098550\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 151850: 0.00047829683172\n",
      "Minibatch loss at step 151850: 0.285015\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 151900: 0.00047829683172\n",
      "Minibatch loss at step 151900: 0.019841\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 151950: 0.00047829683172\n",
      "Minibatch loss at step 151950: 0.460030\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 152000: 0.00047829683172\n",
      "Minibatch loss at step 152000: 0.087866\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 152050: 0.00047829683172\n",
      "Minibatch loss at step 152050: 0.136756\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 152100: 0.00047829683172\n",
      "Minibatch loss at step 152100: 0.400378\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 152150: 0.00047829683172\n",
      "Minibatch loss at step 152150: 0.092489\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 152200: 0.00047829683172\n",
      "Minibatch loss at step 152200: 0.122102\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 152250: 0.00047829683172\n",
      "Minibatch loss at step 152250: 0.224410\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 152300: 0.00047829683172\n",
      "Minibatch loss at step 152300: 0.044543\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 152350: 0.00047829683172\n",
      "Minibatch loss at step 152350: 0.128761\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 152400: 0.00047829683172\n",
      "Minibatch loss at step 152400: 0.073182\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 152450: 0.00047829683172\n",
      "Minibatch loss at step 152450: 0.023394\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 152500: 0.00047829683172\n",
      "Minibatch loss at step 152500: 0.049479\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 152550: 0.00047829683172\n",
      "Minibatch loss at step 152550: 0.061821\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 152600: 0.00047829683172\n",
      "Minibatch loss at step 152600: 0.148699\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 152650: 0.00047829683172\n",
      "Minibatch loss at step 152650: 0.018745\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 152700: 0.00047829683172\n",
      "Minibatch loss at step 152700: 0.171684\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 152750: 0.00047829683172\n",
      "Minibatch loss at step 152750: 0.010308\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 152800: 0.00047829683172\n",
      "Minibatch loss at step 152800: 0.001548\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 152850: 0.00047829683172\n",
      "Minibatch loss at step 152850: 0.086365\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 152900: 0.00047829683172\n",
      "Minibatch loss at step 152900: 0.140585\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 152950: 0.00047829683172\n",
      "Minibatch loss at step 152950: 0.089620\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 153000: 0.00047829683172\n",
      "Minibatch loss at step 153000: 0.288485\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 153050: 0.00047829683172\n",
      "Minibatch loss at step 153050: 0.115388\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 153100: 0.00047829683172\n",
      "Minibatch loss at step 153100: 0.025246\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 153150: 0.00047829683172\n",
      "Minibatch loss at step 153150: 0.010786\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 153200: 0.00047829683172\n",
      "Minibatch loss at step 153200: 0.011364\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 153250: 0.00047829683172\n",
      "Minibatch loss at step 153250: 0.100847\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 153300: 0.00047829683172\n",
      "Minibatch loss at step 153300: 0.136844\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 153350: 0.00047829683172\n",
      "Minibatch loss at step 153350: 0.073883\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 153400: 0.00047829683172\n",
      "Minibatch loss at step 153400: 0.106526\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 153450: 0.00047829683172\n",
      "Minibatch loss at step 153450: 0.072647\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 153500: 0.00047829683172\n",
      "Minibatch loss at step 153500: 0.008178\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 153550: 0.00047829683172\n",
      "Minibatch loss at step 153550: 0.069782\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 153600: 0.00047829683172\n",
      "Minibatch loss at step 153600: 0.020195\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 153650: 0.00047829683172\n",
      "Minibatch loss at step 153650: 0.171387\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 153700: 0.00047829683172\n",
      "Minibatch loss at step 153700: 0.088581\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 153750: 0.00047829683172\n",
      "Minibatch loss at step 153750: 0.026752\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 153800: 0.00047829683172\n",
      "Minibatch loss at step 153800: 0.018263\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 153850: 0.00047829683172\n",
      "Minibatch loss at step 153850: 0.011838\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 153900: 0.00047829683172\n",
      "Minibatch loss at step 153900: 0.113860\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 153950: 0.00047829683172\n",
      "Minibatch loss at step 153950: 0.052095\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 154000: 0.00047829683172\n",
      "Minibatch loss at step 154000: 0.170374\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 154050: 0.00047829683172\n",
      "Minibatch loss at step 154050: 0.004029\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 154100: 0.00047829683172\n",
      "Minibatch loss at step 154100: 0.060048\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 154150: 0.00047829683172\n",
      "Minibatch loss at step 154150: 0.049258\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 154200: 0.00047829683172\n",
      "Minibatch loss at step 154200: 0.029207\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 154250: 0.00047829683172\n",
      "Minibatch loss at step 154250: 0.091963\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 154300: 0.00047829683172\n",
      "Minibatch loss at step 154300: 0.034054\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 154350: 0.00047829683172\n",
      "Minibatch loss at step 154350: 0.060503\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 154400: 0.00047829683172\n",
      "Minibatch loss at step 154400: 0.102946\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 154450: 0.00047829683172\n",
      "Minibatch loss at step 154450: 0.033130\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 154500: 0.00047829683172\n",
      "Minibatch loss at step 154500: 0.044557\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 154550: 0.00047829683172\n",
      "Minibatch loss at step 154550: 0.203837\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 154600: 0.00047829683172\n",
      "Minibatch loss at step 154600: 0.122303\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 154650: 0.00047829683172\n",
      "Minibatch loss at step 154650: 0.134581\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 154700: 0.00047829683172\n",
      "Minibatch loss at step 154700: 0.068726\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 154750: 0.00047829683172\n",
      "Minibatch loss at step 154750: 0.161601\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 154800: 0.00047829683172\n",
      "Minibatch loss at step 154800: 0.128368\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 154850: 0.00047829683172\n",
      "Minibatch loss at step 154850: 0.188802\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 154900: 0.00047829683172\n",
      "Minibatch loss at step 154900: 0.167425\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 154950: 0.00047829683172\n",
      "Minibatch loss at step 154950: 0.222902\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 155000: 0.00047829683172\n",
      "Minibatch loss at step 155000: 0.089260\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 155050: 0.00047829683172\n",
      "Minibatch loss at step 155050: 0.525943\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 155100: 0.00047829683172\n",
      "Minibatch loss at step 155100: 0.116224\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 155150: 0.00047829683172\n",
      "Minibatch loss at step 155150: 0.009218\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 155200: 0.00047829683172\n",
      "Minibatch loss at step 155200: 0.089941\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 155250: 0.00047829683172\n",
      "Minibatch loss at step 155250: 0.045769\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 155300: 0.00047829683172\n",
      "Minibatch loss at step 155300: 0.026962\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 155350: 0.00047829683172\n",
      "Minibatch loss at step 155350: 0.232843\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 155400: 0.00047829683172\n",
      "Minibatch loss at step 155400: 0.010234\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 155450: 0.00047829683172\n",
      "Minibatch loss at step 155450: 0.023912\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 155500: 0.00047829683172\n",
      "Minibatch loss at step 155500: 0.138015\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 155550: 0.00047829683172\n",
      "Minibatch loss at step 155550: 0.016592\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 155600: 0.00047829683172\n",
      "Minibatch loss at step 155600: 0.275559\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 155650: 0.00047829683172\n",
      "Minibatch loss at step 155650: 0.073945\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 155700: 0.00047829683172\n",
      "Minibatch loss at step 155700: 0.065630\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 155750: 0.00047829683172\n",
      "Minibatch loss at step 155750: 0.045075\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 155800: 0.00047829683172\n",
      "Minibatch loss at step 155800: 0.225535\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 155850: 0.00047829683172\n",
      "Minibatch loss at step 155850: 0.069833\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 155900: 0.00047829683172\n",
      "Minibatch loss at step 155900: 0.169020\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 155950: 0.00047829683172\n",
      "Minibatch loss at step 155950: 0.119687\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 156000: 0.00047829683172\n",
      "Minibatch loss at step 156000: 0.028366\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 156050: 0.00047829683172\n",
      "Minibatch loss at step 156050: 0.092198\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 156100: 0.00047829683172\n",
      "Minibatch loss at step 156100: 0.307199\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 156150: 0.00047829683172\n",
      "Minibatch loss at step 156150: 0.258668\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 156200: 0.00047829683172\n",
      "Minibatch loss at step 156200: 0.018123\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 156250: 0.00047829683172\n",
      "Minibatch loss at step 156250: 0.118409\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 156300: 0.00047829683172\n",
      "Minibatch loss at step 156300: 0.092722\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 156350: 0.00047829683172\n",
      "Minibatch loss at step 156350: 0.062417\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 156400: 0.00047829683172\n",
      "Minibatch loss at step 156400: 0.020172\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 156450: 0.00047829683172\n",
      "Minibatch loss at step 156450: 0.071390\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 156500: 0.00047829683172\n",
      "Minibatch loss at step 156500: 0.024225\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 156550: 0.00047829683172\n",
      "Minibatch loss at step 156550: 0.004856\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 156600: 0.00047829683172\n",
      "Minibatch loss at step 156600: 0.263090\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 156650: 0.00047829683172\n",
      "Minibatch loss at step 156650: 0.122929\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 156700: 0.00047829683172\n",
      "Minibatch loss at step 156700: 0.035644\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 156750: 0.00047829683172\n",
      "Minibatch loss at step 156750: 0.019163\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 156800: 0.00047829683172\n",
      "Minibatch loss at step 156800: 0.060687\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 156850: 0.00047829683172\n",
      "Minibatch loss at step 156850: 0.182828\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 156900: 0.00047829683172\n",
      "Minibatch loss at step 156900: 0.168236\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 156950: 0.00047829683172\n",
      "Minibatch loss at step 156950: 0.064310\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 157000: 0.00047829683172\n",
      "Minibatch loss at step 157000: 0.076241\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 157050: 0.00047829683172\n",
      "Minibatch loss at step 157050: 0.018117\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 157100: 0.00047829683172\n",
      "Minibatch loss at step 157100: 0.054771\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 157150: 0.00047829683172\n",
      "Minibatch loss at step 157150: 0.010216\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 157200: 0.00047829683172\n",
      "Minibatch loss at step 157200: 0.011905\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 157250: 0.00047829683172\n",
      "Minibatch loss at step 157250: 0.014727\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 157300: 0.00047829683172\n",
      "Minibatch loss at step 157300: 0.002192\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 157350: 0.00047829683172\n",
      "Minibatch loss at step 157350: 0.008442\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 157400: 0.00047829683172\n",
      "Minibatch loss at step 157400: 0.015453\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 157450: 0.00047829683172\n",
      "Minibatch loss at step 157450: 0.024202\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 157500: 0.00047829683172\n",
      "Minibatch loss at step 157500: 0.033200\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 89.6%\n",
      "Learning rate at step 157550: 0.00047829683172\n",
      "Minibatch loss at step 157550: 0.004885\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 157600: 0.00047829683172\n",
      "Minibatch loss at step 157600: 0.194521\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 157650: 0.00047829683172\n",
      "Minibatch loss at step 157650: 0.200526\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 157700: 0.00047829683172\n",
      "Minibatch loss at step 157700: 0.146036\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 157750: 0.00047829683172\n",
      "Minibatch loss at step 157750: 0.049353\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 157800: 0.00047829683172\n",
      "Minibatch loss at step 157800: 0.080801\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 157850: 0.00047829683172\n",
      "Minibatch loss at step 157850: 0.058558\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 157900: 0.00047829683172\n",
      "Minibatch loss at step 157900: 0.242822\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 157950: 0.00047829683172\n",
      "Minibatch loss at step 157950: 0.296385\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 158000: 0.00047829683172\n",
      "Minibatch loss at step 158000: 0.081218\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 158050: 0.00047829683172\n",
      "Minibatch loss at step 158050: 0.166979\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 158100: 0.00047829683172\n",
      "Minibatch loss at step 158100: 0.003940\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 158150: 0.00047829683172\n",
      "Minibatch loss at step 158150: 0.050890\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 158200: 0.00047829683172\n",
      "Minibatch loss at step 158200: 0.102240\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 158250: 0.00047829683172\n",
      "Minibatch loss at step 158250: 0.014591\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 158300: 0.00047829683172\n",
      "Minibatch loss at step 158300: 0.125611\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 158350: 0.00047829683172\n",
      "Minibatch loss at step 158350: 0.036601\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 158400: 0.00047829683172\n",
      "Minibatch loss at step 158400: 0.116349\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 158450: 0.00047829683172\n",
      "Minibatch loss at step 158450: 0.003800\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 158500: 0.00047829683172\n",
      "Minibatch loss at step 158500: 0.043589\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 158550: 0.00047829683172\n",
      "Minibatch loss at step 158550: 0.073019\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 158600: 0.00047829683172\n",
      "Minibatch loss at step 158600: 0.114233\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 158650: 0.00047829683172\n",
      "Minibatch loss at step 158650: 0.180331\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 158700: 0.00047829683172\n",
      "Minibatch loss at step 158700: 0.069801\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 158750: 0.00047829683172\n",
      "Minibatch loss at step 158750: 0.241283\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 158800: 0.00047829683172\n",
      "Minibatch loss at step 158800: 0.007831\n",
      "Minibatch accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 158850: 0.00047829683172\n",
      "Minibatch loss at step 158850: 0.008688\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 158900: 0.00047829683172\n",
      "Minibatch loss at step 158900: 0.031401\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 158950: 0.00047829683172\n",
      "Minibatch loss at step 158950: 0.065562\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 159000: 0.00047829683172\n",
      "Minibatch loss at step 159000: 0.004373\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 159050: 0.00047829683172\n",
      "Minibatch loss at step 159050: 0.059919\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 159100: 0.00047829683172\n",
      "Minibatch loss at step 159100: 0.153451\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 159150: 0.00047829683172\n",
      "Minibatch loss at step 159150: 0.087967\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 159200: 0.00047829683172\n",
      "Minibatch loss at step 159200: 0.066833\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 159250: 0.00047829683172\n",
      "Minibatch loss at step 159250: 0.337519\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 159300: 0.00047829683172\n",
      "Minibatch loss at step 159300: 0.184936\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 159350: 0.00047829683172\n",
      "Minibatch loss at step 159350: 0.381470\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 159400: 0.00047829683172\n",
      "Minibatch loss at step 159400: 0.119632\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 159450: 0.00047829683172\n",
      "Minibatch loss at step 159450: 0.017604\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 159500: 0.00047829683172\n",
      "Minibatch loss at step 159500: 0.069519\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.5%\n",
      "Learning rate at step 159550: 0.00047829683172\n",
      "Minibatch loss at step 159550: 0.404219\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 159600: 0.00047829683172\n",
      "Minibatch loss at step 159600: 0.198323\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 159650: 0.00047829683172\n",
      "Minibatch loss at step 159650: 0.105578\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 159700: 0.00047829683172\n",
      "Minibatch loss at step 159700: 0.119461\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 159750: 0.00047829683172\n",
      "Minibatch loss at step 159750: 0.012385\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 159800: 0.00047829683172\n",
      "Minibatch loss at step 159800: 0.011604\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 159850: 0.00047829683172\n",
      "Minibatch loss at step 159850: 0.105761\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 159900: 0.00047829683172\n",
      "Minibatch loss at step 159900: 0.228085\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 159950: 0.00047829683172\n",
      "Minibatch loss at step 159950: 0.062954\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 160000: 0.00043046716019\n",
      "Minibatch loss at step 160000: 0.012796\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 160050: 0.00043046716019\n",
      "Minibatch loss at step 160050: 0.008590\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 160100: 0.00043046716019\n",
      "Minibatch loss at step 160100: 0.187046\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 160150: 0.00043046716019\n",
      "Minibatch loss at step 160150: 0.116572\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 160200: 0.00043046716019\n",
      "Minibatch loss at step 160200: 0.140608\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 160250: 0.00043046716019\n",
      "Minibatch loss at step 160250: 0.191783\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 160300: 0.00043046716019\n",
      "Minibatch loss at step 160300: 0.064086\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 160350: 0.00043046716019\n",
      "Minibatch loss at step 160350: 0.113778\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 160400: 0.00043046716019\n",
      "Minibatch loss at step 160400: 0.030361\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 160450: 0.00043046716019\n",
      "Minibatch loss at step 160450: 0.031317\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 160500: 0.00043046716019\n",
      "Minibatch loss at step 160500: 0.160265\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 160550: 0.00043046716019\n",
      "Minibatch loss at step 160550: 0.361780\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 160600: 0.00043046716019\n",
      "Minibatch loss at step 160600: 0.052140\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 160650: 0.00043046716019\n",
      "Minibatch loss at step 160650: 0.427382\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 160700: 0.00043046716019\n",
      "Minibatch loss at step 160700: 0.315674\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 160750: 0.00043046716019\n",
      "Minibatch loss at step 160750: 0.178907\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 160800: 0.00043046716019\n",
      "Minibatch loss at step 160800: 0.449783\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 160850: 0.00043046716019\n",
      "Minibatch loss at step 160850: 0.179443\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 160900: 0.00043046716019\n",
      "Minibatch loss at step 160900: 0.218599\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 160950: 0.00043046716019\n",
      "Minibatch loss at step 160950: 0.138376\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 161000: 0.00043046716019\n",
      "Minibatch loss at step 161000: 0.087894\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 161050: 0.00043046716019\n",
      "Minibatch loss at step 161050: 0.030067\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 161100: 0.00043046716019\n",
      "Minibatch loss at step 161100: 0.234101\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 161150: 0.00043046716019\n",
      "Minibatch loss at step 161150: 0.076481\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 161200: 0.00043046716019\n",
      "Minibatch loss at step 161200: 0.015017\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 161250: 0.00043046716019\n",
      "Minibatch loss at step 161250: 0.071041\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 161300: 0.00043046716019\n",
      "Minibatch loss at step 161300: 0.009407\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 161350: 0.00043046716019\n",
      "Minibatch loss at step 161350: 0.164161\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 161400: 0.00043046716019\n",
      "Minibatch loss at step 161400: 0.059758\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 161450: 0.00043046716019\n",
      "Minibatch loss at step 161450: 0.187894\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 161500: 0.00043046716019\n",
      "Minibatch loss at step 161500: 0.079539\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 161550: 0.00043046716019\n",
      "Minibatch loss at step 161550: 0.006518\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 161600: 0.00043046716019\n",
      "Minibatch loss at step 161600: 0.170239\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 161650: 0.00043046716019\n",
      "Minibatch loss at step 161650: 0.004885\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 161700: 0.00043046716019\n",
      "Minibatch loss at step 161700: 0.024374\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 161750: 0.00043046716019\n",
      "Minibatch loss at step 161750: 0.118928\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 161800: 0.00043046716019\n",
      "Minibatch loss at step 161800: 0.050638\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 161850: 0.00043046716019\n",
      "Minibatch loss at step 161850: 0.129004\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 161900: 0.00043046716019\n",
      "Minibatch loss at step 161900: 0.065809\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 161950: 0.00043046716019\n",
      "Minibatch loss at step 161950: 0.077880\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 162000: 0.00043046716019\n",
      "Minibatch loss at step 162000: 0.195246\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 162050: 0.00043046716019\n",
      "Minibatch loss at step 162050: 0.061827\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 162100: 0.00043046716019\n",
      "Minibatch loss at step 162100: 0.026691\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 162150: 0.00043046716019\n",
      "Minibatch loss at step 162150: 0.022421\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 162200: 0.00043046716019\n",
      "Minibatch loss at step 162200: 0.068821\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 162250: 0.00043046716019\n",
      "Minibatch loss at step 162250: 0.042807\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 162300: 0.00043046716019\n",
      "Minibatch loss at step 162300: 0.011810\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 162350: 0.00043046716019\n",
      "Minibatch loss at step 162350: 0.091893\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 162400: 0.00043046716019\n",
      "Minibatch loss at step 162400: 0.029065\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 162450: 0.00043046716019\n",
      "Minibatch loss at step 162450: 0.022932\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 162500: 0.00043046716019\n",
      "Minibatch loss at step 162500: 0.054186\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.5%\n",
      "Learning rate at step 162550: 0.00043046716019\n",
      "Minibatch loss at step 162550: 0.112781\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 162600: 0.00043046716019\n",
      "Minibatch loss at step 162600: 0.111586\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 162650: 0.00043046716019\n",
      "Minibatch loss at step 162650: 0.020605\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 162700: 0.00043046716019\n",
      "Minibatch loss at step 162700: 0.036831\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 162750: 0.00043046716019\n",
      "Minibatch loss at step 162750: 0.075845\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 162800: 0.00043046716019\n",
      "Minibatch loss at step 162800: 0.016551\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 162850: 0.00043046716019\n",
      "Minibatch loss at step 162850: 0.007537\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 162900: 0.00043046716019\n",
      "Minibatch loss at step 162900: 0.027244\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 162950: 0.00043046716019\n",
      "Minibatch loss at step 162950: 0.063247\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 163000: 0.00043046716019\n",
      "Minibatch loss at step 163000: 0.203391\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 163050: 0.00043046716019\n",
      "Minibatch loss at step 163050: 0.013498\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 163100: 0.00043046716019\n",
      "Minibatch loss at step 163100: 0.134152\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 163150: 0.00043046716019\n",
      "Minibatch loss at step 163150: 0.003238\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 163200: 0.00043046716019\n",
      "Minibatch loss at step 163200: 0.040302\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 163250: 0.00043046716019\n",
      "Minibatch loss at step 163250: 0.071256\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 163300: 0.00043046716019\n",
      "Minibatch loss at step 163300: 0.274756\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 163350: 0.00043046716019\n",
      "Minibatch loss at step 163350: 0.072512\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 163400: 0.00043046716019\n",
      "Minibatch loss at step 163400: 0.325988\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 163450: 0.00043046716019\n",
      "Minibatch loss at step 163450: 0.278288\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 163500: 0.00043046716019\n",
      "Minibatch loss at step 163500: 0.217221\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 163550: 0.00043046716019\n",
      "Minibatch loss at step 163550: 0.333368\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 163600: 0.00043046716019\n",
      "Minibatch loss at step 163600: 0.338362\n",
      "Minibatch accuracy: 87.5%\n",
      "Learning rate at step 163650: 0.00043046716019\n",
      "Minibatch loss at step 163650: 0.005988\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 163700: 0.00043046716019\n",
      "Minibatch loss at step 163700: 0.231060\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 163750: 0.00043046716019\n",
      "Minibatch loss at step 163750: 0.025688\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 163800: 0.00043046716019\n",
      "Minibatch loss at step 163800: 0.213751\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 163850: 0.00043046716019\n",
      "Minibatch loss at step 163850: 0.008178\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 163900: 0.00043046716019\n",
      "Minibatch loss at step 163900: 0.111237\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 163950: 0.00043046716019\n",
      "Minibatch loss at step 163950: 0.013951\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 164000: 0.00043046716019\n",
      "Minibatch loss at step 164000: 0.077643\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 164050: 0.00043046716019\n",
      "Minibatch loss at step 164050: 0.233936\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 164100: 0.00043046716019\n",
      "Minibatch loss at step 164100: 0.229162\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 164150: 0.00043046716019\n",
      "Minibatch loss at step 164150: 0.095107\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 164200: 0.00043046716019\n",
      "Minibatch loss at step 164200: 0.009412\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 164250: 0.00043046716019\n",
      "Minibatch loss at step 164250: 0.040758\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 164300: 0.00043046716019\n",
      "Minibatch loss at step 164300: 0.004839\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 164350: 0.00043046716019\n",
      "Minibatch loss at step 164350: 0.124136\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 164400: 0.00043046716019\n",
      "Minibatch loss at step 164400: 0.095179\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 164450: 0.00043046716019\n",
      "Minibatch loss at step 164450: 0.227323\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 164500: 0.00043046716019\n",
      "Minibatch loss at step 164500: 0.042422\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 164550: 0.00043046716019\n",
      "Minibatch loss at step 164550: 0.083384\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 164600: 0.00043046716019\n",
      "Minibatch loss at step 164600: 0.081450\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 164650: 0.00043046716019\n",
      "Minibatch loss at step 164650: 0.049660\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 164700: 0.00043046716019\n",
      "Minibatch loss at step 164700: 0.008871\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 164750: 0.00043046716019\n",
      "Minibatch loss at step 164750: 0.026513\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 164800: 0.00043046716019\n",
      "Minibatch loss at step 164800: 0.010211\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 164850: 0.00043046716019\n",
      "Minibatch loss at step 164850: 0.099613\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 164900: 0.00043046716019\n",
      "Minibatch loss at step 164900: 0.006323\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 164950: 0.00043046716019\n",
      "Minibatch loss at step 164950: 0.034644\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 165000: 0.00043046716019\n",
      "Minibatch loss at step 165000: 0.052324\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 165050: 0.00043046716019\n",
      "Minibatch loss at step 165050: 0.088499\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 165100: 0.00043046716019\n",
      "Minibatch loss at step 165100: 0.097450\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 165150: 0.00043046716019\n",
      "Minibatch loss at step 165150: 0.004149\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 165200: 0.00043046716019\n",
      "Minibatch loss at step 165200: 0.067952\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 165250: 0.00043046716019\n",
      "Minibatch loss at step 165250: 0.088093\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 165300: 0.00043046716019\n",
      "Minibatch loss at step 165300: 0.154562\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 165350: 0.00043046716019\n",
      "Minibatch loss at step 165350: 0.124330\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 165400: 0.00043046716019\n",
      "Minibatch loss at step 165400: 0.186773\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 165450: 0.00043046716019\n",
      "Minibatch loss at step 165450: 0.048164\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 165500: 0.00043046716019\n",
      "Minibatch loss at step 165500: 0.518609\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 165550: 0.00043046716019\n",
      "Minibatch loss at step 165550: 0.010523\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 165600: 0.00043046716019\n",
      "Minibatch loss at step 165600: 0.015465\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 165650: 0.00043046716019\n",
      "Minibatch loss at step 165650: 0.167746\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 165700: 0.00043046716019\n",
      "Minibatch loss at step 165700: 0.002821\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 165750: 0.00043046716019\n",
      "Minibatch loss at step 165750: 0.153086\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 165800: 0.00043046716019\n",
      "Minibatch loss at step 165800: 0.132202\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 165850: 0.00043046716019\n",
      "Minibatch loss at step 165850: 0.016004\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 165900: 0.00043046716019\n",
      "Minibatch loss at step 165900: 0.033866\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 165950: 0.00043046716019\n",
      "Minibatch loss at step 165950: 0.085821\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 166000: 0.00043046716019\n",
      "Minibatch loss at step 166000: 0.027615\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 166050: 0.00043046716019\n",
      "Minibatch loss at step 166050: 0.077978\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 166100: 0.00043046716019\n",
      "Minibatch loss at step 166100: 0.013428\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 166150: 0.00043046716019\n",
      "Minibatch loss at step 166150: 0.055668\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 166200: 0.00043046716019\n",
      "Minibatch loss at step 166200: 0.078606\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 166250: 0.00043046716019\n",
      "Minibatch loss at step 166250: 0.179710\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 166300: 0.00043046716019\n",
      "Minibatch loss at step 166300: 0.124966\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 166350: 0.00043046716019\n",
      "Minibatch loss at step 166350: 0.150874\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 166400: 0.00043046716019\n",
      "Minibatch loss at step 166400: 0.248475\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 166450: 0.00043046716019\n",
      "Minibatch loss at step 166450: 0.164701\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 166500: 0.00043046716019\n",
      "Minibatch loss at step 166500: 0.639231\n",
      "Minibatch accuracy: 89.1%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 166550: 0.00043046716019\n",
      "Minibatch loss at step 166550: 0.444591\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 166600: 0.00043046716019\n",
      "Minibatch loss at step 166600: 0.261226\n",
      "Minibatch accuracy: 89.1%\n",
      "Learning rate at step 166650: 0.00043046716019\n",
      "Minibatch loss at step 166650: 0.105967\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 166700: 0.00043046716019\n",
      "Minibatch loss at step 166700: 0.059950\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 166750: 0.00043046716019\n",
      "Minibatch loss at step 166750: 0.044899\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 166800: 0.00043046716019\n",
      "Minibatch loss at step 166800: 0.092605\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 166850: 0.00043046716019\n",
      "Minibatch loss at step 166850: 0.062272\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 166900: 0.00043046716019\n",
      "Minibatch loss at step 166900: 0.051228\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 166950: 0.00043046716019\n",
      "Minibatch loss at step 166950: 0.010243\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 167000: 0.00043046716019\n",
      "Minibatch loss at step 167000: 0.091409\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 167050: 0.00043046716019\n",
      "Minibatch loss at step 167050: 0.047486\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 167100: 0.00043046716019\n",
      "Minibatch loss at step 167100: 0.091796\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 167150: 0.00043046716019\n",
      "Minibatch loss at step 167150: 0.162999\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 167200: 0.00043046716019\n",
      "Minibatch loss at step 167200: 0.009428\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 167250: 0.00043046716019\n",
      "Minibatch loss at step 167250: 0.002766\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 167300: 0.00043046716019\n",
      "Minibatch loss at step 167300: 0.055504\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 167350: 0.00043046716019\n",
      "Minibatch loss at step 167350: 0.296324\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 167400: 0.00043046716019\n",
      "Minibatch loss at step 167400: 0.022941\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 167450: 0.00043046716019\n",
      "Minibatch loss at step 167450: 0.202116\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 167500: 0.00043046716019\n",
      "Minibatch loss at step 167500: 0.048756\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 167550: 0.00043046716019\n",
      "Minibatch loss at step 167550: 0.023886\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 167600: 0.00043046716019\n",
      "Minibatch loss at step 167600: 0.002908\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 167650: 0.00043046716019\n",
      "Minibatch loss at step 167650: 0.045068\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 167700: 0.00043046716019\n",
      "Minibatch loss at step 167700: 0.132806\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 167750: 0.00043046716019\n",
      "Minibatch loss at step 167750: 0.033868\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 167800: 0.00043046716019\n",
      "Minibatch loss at step 167800: 0.043342\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 167850: 0.00043046716019\n",
      "Minibatch loss at step 167850: 0.153134\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 167900: 0.00043046716019\n",
      "Minibatch loss at step 167900: 0.030249\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 167950: 0.00043046716019\n",
      "Minibatch loss at step 167950: 0.005513\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 168000: 0.00043046716019\n",
      "Minibatch loss at step 168000: 0.207068\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.7%\n",
      "Learning rate at step 168050: 0.00043046716019\n",
      "Minibatch loss at step 168050: 0.016511\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 168100: 0.00043046716019\n",
      "Minibatch loss at step 168100: 0.170790\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 168150: 0.00043046716019\n",
      "Minibatch loss at step 168150: 0.105879\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 168200: 0.00043046716019\n",
      "Minibatch loss at step 168200: 0.031484\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 168250: 0.00043046716019\n",
      "Minibatch loss at step 168250: 0.004849\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 168300: 0.00043046716019\n",
      "Minibatch loss at step 168300: 0.016287\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 168350: 0.00043046716019\n",
      "Minibatch loss at step 168350: 0.051696\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 168400: 0.00043046716019\n",
      "Minibatch loss at step 168400: 0.013415\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 168450: 0.00043046716019\n",
      "Minibatch loss at step 168450: 0.065807\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 168500: 0.00043046716019\n",
      "Minibatch loss at step 168500: 0.158045\n",
      "Minibatch accuracy: 92.2%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 168550: 0.00043046716019\n",
      "Minibatch loss at step 168550: 0.010524\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 168600: 0.00043046716019\n",
      "Minibatch loss at step 168600: 0.069772\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 168650: 0.00043046716019\n",
      "Minibatch loss at step 168650: 0.053255\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 168700: 0.00043046716019\n",
      "Minibatch loss at step 168700: 0.004043\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 168750: 0.00043046716019\n",
      "Minibatch loss at step 168750: 0.023462\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 168800: 0.00043046716019\n",
      "Minibatch loss at step 168800: 0.009669\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 168850: 0.00043046716019\n",
      "Minibatch loss at step 168850: 0.021363\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 168900: 0.00043046716019\n",
      "Minibatch loss at step 168900: 0.076834\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 168950: 0.00043046716019\n",
      "Minibatch loss at step 168950: 0.058398\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 169000: 0.00043046716019\n",
      "Minibatch loss at step 169000: 0.113254\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 169050: 0.00043046716019\n",
      "Minibatch loss at step 169050: 0.334057\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 169100: 0.00043046716019\n",
      "Minibatch loss at step 169100: 0.152334\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 169150: 0.00043046716019\n",
      "Minibatch loss at step 169150: 0.393002\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 169200: 0.00043046716019\n",
      "Minibatch loss at step 169200: 0.329366\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 169250: 0.00043046716019\n",
      "Minibatch loss at step 169250: 0.151477\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 169300: 0.00043046716019\n",
      "Minibatch loss at step 169300: 0.128273\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 169350: 0.00043046716019\n",
      "Minibatch loss at step 169350: 0.212412\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 169400: 0.00043046716019\n",
      "Minibatch loss at step 169400: 0.115790\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 169450: 0.00043046716019\n",
      "Minibatch loss at step 169450: 0.221979\n",
      "Minibatch accuracy: 93.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 169500: 0.00043046716019\n",
      "Minibatch loss at step 169500: 0.111344\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 169550: 0.00043046716019\n",
      "Minibatch loss at step 169550: 0.041074\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 169600: 0.00043046716019\n",
      "Minibatch loss at step 169600: 0.008877\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 169650: 0.00043046716019\n",
      "Minibatch loss at step 169650: 0.007215\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 169700: 0.00043046716019\n",
      "Minibatch loss at step 169700: 0.004905\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 169750: 0.00043046716019\n",
      "Minibatch loss at step 169750: 0.021245\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 169800: 0.00043046716019\n",
      "Minibatch loss at step 169800: 0.032585\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 169850: 0.00043046716019\n",
      "Minibatch loss at step 169850: 0.002236\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 169900: 0.00043046716019\n",
      "Minibatch loss at step 169900: 0.057252\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 169950: 0.00043046716019\n",
      "Minibatch loss at step 169950: 0.014255\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 170000: 0.00043046716019\n",
      "Minibatch loss at step 170000: 0.004084\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 170050: 0.00043046716019\n",
      "Minibatch loss at step 170050: 0.023160\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 170100: 0.00043046716019\n",
      "Minibatch loss at step 170100: 0.009209\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 170150: 0.00043046716019\n",
      "Minibatch loss at step 170150: 0.007595\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 170200: 0.00043046716019\n",
      "Minibatch loss at step 170200: 0.095483\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 170250: 0.00043046716019\n",
      "Minibatch loss at step 170250: 0.150320\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 170300: 0.00043046716019\n",
      "Minibatch loss at step 170300: 0.004362\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 170350: 0.00043046716019\n",
      "Minibatch loss at step 170350: 0.016756\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 170400: 0.00043046716019\n",
      "Minibatch loss at step 170400: 0.006998\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 170450: 0.00043046716019\n",
      "Minibatch loss at step 170450: 0.243187\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 170500: 0.00043046716019\n",
      "Minibatch loss at step 170500: 0.168846\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.5%\n",
      "Learning rate at step 170550: 0.00043046716019\n",
      "Minibatch loss at step 170550: 0.018036\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 170600: 0.00043046716019\n",
      "Minibatch loss at step 170600: 0.212890\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 170650: 0.00043046716019\n",
      "Minibatch loss at step 170650: 0.115282\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 170700: 0.00043046716019\n",
      "Minibatch loss at step 170700: 0.054593\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 170750: 0.00043046716019\n",
      "Minibatch loss at step 170750: 0.019941\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 170800: 0.00043046716019\n",
      "Minibatch loss at step 170800: 0.104916\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 170850: 0.00043046716019\n",
      "Minibatch loss at step 170850: 0.007407\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 170900: 0.00043046716019\n",
      "Minibatch loss at step 170900: 0.164310\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 170950: 0.00043046716019\n",
      "Minibatch loss at step 170950: 0.194842\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 171000: 0.00043046716019\n",
      "Minibatch loss at step 171000: 0.031083\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 171050: 0.00043046716019\n",
      "Minibatch loss at step 171050: 0.064103\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 171100: 0.00043046716019\n",
      "Minibatch loss at step 171100: 0.013632\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 171150: 0.00043046716019\n",
      "Minibatch loss at step 171150: 0.014296\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 171200: 0.00043046716019\n",
      "Minibatch loss at step 171200: 0.013634\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 171250: 0.00043046716019\n",
      "Minibatch loss at step 171250: 0.060068\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 171300: 0.00043046716019\n",
      "Minibatch loss at step 171300: 0.021124\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 171350: 0.00043046716019\n",
      "Minibatch loss at step 171350: 0.162734\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 171400: 0.00043046716019\n",
      "Minibatch loss at step 171400: 0.108241\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 171450: 0.00043046716019\n",
      "Minibatch loss at step 171450: 0.156813\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 171500: 0.00043046716019\n",
      "Minibatch loss at step 171500: 0.131233\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.5%\n",
      "Learning rate at step 171550: 0.00043046716019\n",
      "Minibatch loss at step 171550: 0.000717\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 171600: 0.00043046716019\n",
      "Minibatch loss at step 171600: 0.086341\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 171650: 0.00043046716019\n",
      "Minibatch loss at step 171650: 0.041346\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 171700: 0.00043046716019\n",
      "Minibatch loss at step 171700: 0.093329\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 171750: 0.00043046716019\n",
      "Minibatch loss at step 171750: 0.029100\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 171800: 0.00043046716019\n",
      "Minibatch loss at step 171800: 0.003730\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 171850: 0.00043046716019\n",
      "Minibatch loss at step 171850: 0.083758\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 171900: 0.00043046716019\n",
      "Minibatch loss at step 171900: 0.056592\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 171950: 0.00043046716019\n",
      "Minibatch loss at step 171950: 0.109867\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 172000: 0.00043046716019\n",
      "Minibatch loss at step 172000: 0.055844\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 172050: 0.00043046716019\n",
      "Minibatch loss at step 172050: 0.115237\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 172100: 0.00043046716019\n",
      "Minibatch loss at step 172100: 0.041825\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 172150: 0.00043046716019\n",
      "Minibatch loss at step 172150: 0.048926\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 172200: 0.00043046716019\n",
      "Minibatch loss at step 172200: 0.088882\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 172250: 0.00043046716019\n",
      "Minibatch loss at step 172250: 0.189715\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 172300: 0.00043046716019\n",
      "Minibatch loss at step 172300: 0.465421\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 172350: 0.00043046716019\n",
      "Minibatch loss at step 172350: 0.138475\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 172400: 0.00043046716019\n",
      "Minibatch loss at step 172400: 0.018718\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 172450: 0.00043046716019\n",
      "Minibatch loss at step 172450: 0.010114\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 172500: 0.00043046716019\n",
      "Minibatch loss at step 172500: 0.003180\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.5%\n",
      "Learning rate at step 172550: 0.00043046716019\n",
      "Minibatch loss at step 172550: 0.004354\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 172600: 0.00043046716019\n",
      "Minibatch loss at step 172600: 0.029393\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 172650: 0.00043046716019\n",
      "Minibatch loss at step 172650: 0.013032\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 172700: 0.00043046716019\n",
      "Minibatch loss at step 172700: 0.077814\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 172750: 0.00043046716019\n",
      "Minibatch loss at step 172750: 0.195381\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 172800: 0.00043046716019\n",
      "Minibatch loss at step 172800: 0.109446\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 172850: 0.00043046716019\n",
      "Minibatch loss at step 172850: 0.222608\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 172900: 0.00043046716019\n",
      "Minibatch loss at step 172900: 0.115222\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 172950: 0.00043046716019\n",
      "Minibatch loss at step 172950: 0.069830\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 173000: 0.00043046716019\n",
      "Minibatch loss at step 173000: 0.072178\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 89.8%\n",
      "Learning rate at step 173050: 0.00043046716019\n",
      "Minibatch loss at step 173050: 0.043108\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 173100: 0.00043046716019\n",
      "Minibatch loss at step 173100: 0.011873\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 173150: 0.00043046716019\n",
      "Minibatch loss at step 173150: 0.000258\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 173200: 0.00043046716019\n",
      "Minibatch loss at step 173200: 0.006224\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 173250: 0.00043046716019\n",
      "Minibatch loss at step 173250: 0.029082\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 173300: 0.00043046716019\n",
      "Minibatch loss at step 173300: 0.074624\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 173350: 0.00043046716019\n",
      "Minibatch loss at step 173350: 0.056074\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 173400: 0.00043046716019\n",
      "Minibatch loss at step 173400: 0.026365\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 173450: 0.00043046716019\n",
      "Minibatch loss at step 173450: 0.121809\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 173500: 0.00043046716019\n",
      "Minibatch loss at step 173500: 0.007169\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 173550: 0.00043046716019\n",
      "Minibatch loss at step 173550: 0.003849\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 173600: 0.00043046716019\n",
      "Minibatch loss at step 173600: 0.115833\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 173650: 0.00043046716019\n",
      "Minibatch loss at step 173650: 0.011682\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 173700: 0.00043046716019\n",
      "Minibatch loss at step 173700: 0.027702\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 173750: 0.00043046716019\n",
      "Minibatch loss at step 173750: 0.025710\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 173800: 0.00043046716019\n",
      "Minibatch loss at step 173800: 0.070002\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 173850: 0.00043046716019\n",
      "Minibatch loss at step 173850: 0.019016\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 173900: 0.00043046716019\n",
      "Minibatch loss at step 173900: 0.039107\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 173950: 0.00043046716019\n",
      "Minibatch loss at step 173950: 0.170028\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 174000: 0.00043046716019\n",
      "Minibatch loss at step 174000: 0.053381\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 174050: 0.00043046716019\n",
      "Minibatch loss at step 174050: 0.003381\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 174100: 0.00043046716019\n",
      "Minibatch loss at step 174100: 0.070336\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 174150: 0.00043046716019\n",
      "Minibatch loss at step 174150: 0.077517\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 174200: 0.00043046716019\n",
      "Minibatch loss at step 174200: 0.105243\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 174250: 0.00043046716019\n",
      "Minibatch loss at step 174250: 0.023057\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 174300: 0.00043046716019\n",
      "Minibatch loss at step 174300: 0.153494\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 174350: 0.00043046716019\n",
      "Minibatch loss at step 174350: 0.020569\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 174400: 0.00043046716019\n",
      "Minibatch loss at step 174400: 0.063930\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 174450: 0.00043046716019\n",
      "Minibatch loss at step 174450: 0.071335\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 174500: 0.00043046716019\n",
      "Minibatch loss at step 174500: 0.019088\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 174550: 0.00043046716019\n",
      "Minibatch loss at step 174550: 0.062794\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 174600: 0.00043046716019\n",
      "Minibatch loss at step 174600: 0.160014\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 174650: 0.00043046716019\n",
      "Minibatch loss at step 174650: 0.127588\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 174700: 0.00043046716019\n",
      "Minibatch loss at step 174700: 0.025333\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 174750: 0.00043046716019\n",
      "Minibatch loss at step 174750: 0.202473\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 174800: 0.00043046716019\n",
      "Minibatch loss at step 174800: 0.099519\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 174850: 0.00043046716019\n",
      "Minibatch loss at step 174850: 0.245674\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 174900: 0.00043046716019\n",
      "Minibatch loss at step 174900: 0.281712\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 174950: 0.00043046716019\n",
      "Minibatch loss at step 174950: 0.255724\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 175000: 0.00043046716019\n",
      "Minibatch loss at step 175000: 0.029479\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 175050: 0.00043046716019\n",
      "Minibatch loss at step 175050: 0.037222\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 175100: 0.00043046716019\n",
      "Minibatch loss at step 175100: 0.230710\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 175150: 0.00043046716019\n",
      "Minibatch loss at step 175150: 0.185566\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 175200: 0.00043046716019\n",
      "Minibatch loss at step 175200: 0.145666\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 175250: 0.00043046716019\n",
      "Minibatch loss at step 175250: 0.163506\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 175300: 0.00043046716019\n",
      "Minibatch loss at step 175300: 0.199776\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 175350: 0.00043046716019\n",
      "Minibatch loss at step 175350: 0.012013\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 175400: 0.00043046716019\n",
      "Minibatch loss at step 175400: 0.008629\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 175450: 0.00043046716019\n",
      "Minibatch loss at step 175450: 0.063490\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 175500: 0.00043046716019\n",
      "Minibatch loss at step 175500: 0.099347\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 175550: 0.00043046716019\n",
      "Minibatch loss at step 175550: 0.060021\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 175600: 0.00043046716019\n",
      "Minibatch loss at step 175600: 0.011253\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 175650: 0.00043046716019\n",
      "Minibatch loss at step 175650: 0.170824\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 175700: 0.00043046716019\n",
      "Minibatch loss at step 175700: 0.005254\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 175750: 0.00043046716019\n",
      "Minibatch loss at step 175750: 0.282136\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 175800: 0.00043046716019\n",
      "Minibatch loss at step 175800: 0.010813\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 175850: 0.00043046716019\n",
      "Minibatch loss at step 175850: 0.178839\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 175900: 0.00043046716019\n",
      "Minibatch loss at step 175900: 0.067695\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 175950: 0.00043046716019\n",
      "Minibatch loss at step 175950: 0.128286\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 176000: 0.00043046716019\n",
      "Minibatch loss at step 176000: 0.066733\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.6%\n",
      "Learning rate at step 176050: 0.00043046716019\n",
      "Minibatch loss at step 176050: 0.015658\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 176100: 0.00043046716019\n",
      "Minibatch loss at step 176100: 0.208622\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 176150: 0.00043046716019\n",
      "Minibatch loss at step 176150: 0.017891\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 176200: 0.00043046716019\n",
      "Minibatch loss at step 176200: 0.044429\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 176250: 0.00043046716019\n",
      "Minibatch loss at step 176250: 0.008126\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 176300: 0.00043046716019\n",
      "Minibatch loss at step 176300: 0.024420\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 176350: 0.00043046716019\n",
      "Minibatch loss at step 176350: 0.029599\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 176400: 0.00043046716019\n",
      "Minibatch loss at step 176400: 0.023137\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 176450: 0.00043046716019\n",
      "Minibatch loss at step 176450: 0.041436\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 176500: 0.00043046716019\n",
      "Minibatch loss at step 176500: 0.142338\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.6%\n",
      "Learning rate at step 176550: 0.00043046716019\n",
      "Minibatch loss at step 176550: 0.052671\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 176600: 0.00043046716019\n",
      "Minibatch loss at step 176600: 0.006341\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 176650: 0.00043046716019\n",
      "Minibatch loss at step 176650: 0.002634\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 176700: 0.00043046716019\n",
      "Minibatch loss at step 176700: 0.022080\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 176750: 0.00043046716019\n",
      "Minibatch loss at step 176750: 0.043087\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 176800: 0.00043046716019\n",
      "Minibatch loss at step 176800: 0.019220\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 176850: 0.00043046716019\n",
      "Minibatch loss at step 176850: 0.003767\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 176900: 0.00043046716019\n",
      "Minibatch loss at step 176900: 0.061862\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 176950: 0.00043046716019\n",
      "Minibatch loss at step 176950: 0.125469\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 177000: 0.00043046716019\n",
      "Minibatch loss at step 177000: 0.043852\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 177050: 0.00043046716019\n",
      "Minibatch loss at step 177050: 0.142446\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 177100: 0.00043046716019\n",
      "Minibatch loss at step 177100: 0.080881\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 177150: 0.00043046716019\n",
      "Minibatch loss at step 177150: 0.061545\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 177200: 0.00043046716019\n",
      "Minibatch loss at step 177200: 0.073570\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 177250: 0.00043046716019\n",
      "Minibatch loss at step 177250: 0.055131\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 177300: 0.00043046716019\n",
      "Minibatch loss at step 177300: 0.024590\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 177350: 0.00043046716019\n",
      "Minibatch loss at step 177350: 0.086514\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 177400: 0.00043046716019\n",
      "Minibatch loss at step 177400: 0.049242\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 177450: 0.00043046716019\n",
      "Minibatch loss at step 177450: 0.181704\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 177500: 0.00043046716019\n",
      "Minibatch loss at step 177500: 0.253393\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 90.7%\n",
      "Learning rate at step 177550: 0.00043046716019\n",
      "Minibatch loss at step 177550: 0.305938\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 177600: 0.00043046716019\n",
      "Minibatch loss at step 177600: 0.032104\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 177650: 0.00043046716019\n",
      "Minibatch loss at step 177650: 0.327242\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 177700: 0.00043046716019\n",
      "Minibatch loss at step 177700: 0.308323\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 177750: 0.00043046716019\n",
      "Minibatch loss at step 177750: 0.045327\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 177800: 0.00043046716019\n",
      "Minibatch loss at step 177800: 0.091402\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 177850: 0.00043046716019\n",
      "Minibatch loss at step 177850: 0.125337\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 177900: 0.00043046716019\n",
      "Minibatch loss at step 177900: 0.321715\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 177950: 0.00043046716019\n",
      "Minibatch loss at step 177950: 0.106195\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 178000: 0.00043046716019\n",
      "Minibatch loss at step 178000: 0.188968\n",
      "Minibatch accuracy: 90.6%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 178050: 0.00043046716019\n",
      "Minibatch loss at step 178050: 0.137530\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 178100: 0.00043046716019\n",
      "Minibatch loss at step 178100: 0.157537\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 178150: 0.00043046716019\n",
      "Minibatch loss at step 178150: 0.016802\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 178200: 0.00043046716019\n",
      "Minibatch loss at step 178200: 0.043562\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 178250: 0.00043046716019\n",
      "Minibatch loss at step 178250: 0.093071\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 178300: 0.00043046716019\n",
      "Minibatch loss at step 178300: 0.017684\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 178350: 0.00043046716019\n",
      "Minibatch loss at step 178350: 0.024408\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 178400: 0.00043046716019\n",
      "Minibatch loss at step 178400: 0.014020\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 178450: 0.00043046716019\n",
      "Minibatch loss at step 178450: 0.205024\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 178500: 0.00043046716019\n",
      "Minibatch loss at step 178500: 0.066148\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 178550: 0.00043046716019\n",
      "Minibatch loss at step 178550: 0.137145\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 178600: 0.00043046716019\n",
      "Minibatch loss at step 178600: 0.207236\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 178650: 0.00043046716019\n",
      "Minibatch loss at step 178650: 0.104348\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 178700: 0.00043046716019\n",
      "Minibatch loss at step 178700: 0.002165\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 178750: 0.00043046716019\n",
      "Minibatch loss at step 178750: 0.150480\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 178800: 0.00043046716019\n",
      "Minibatch loss at step 178800: 0.048554\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 178850: 0.00043046716019\n",
      "Minibatch loss at step 178850: 0.055098\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 178900: 0.00043046716019\n",
      "Minibatch loss at step 178900: 0.006592\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 178950: 0.00043046716019\n",
      "Minibatch loss at step 178950: 0.041564\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 179000: 0.00043046716019\n",
      "Minibatch loss at step 179000: 0.008333\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.7%\n",
      "Learning rate at step 179050: 0.00043046716019\n",
      "Minibatch loss at step 179050: 0.015773\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 179100: 0.00043046716019\n",
      "Minibatch loss at step 179100: 0.014496\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 179150: 0.00043046716019\n",
      "Minibatch loss at step 179150: 0.046318\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 179200: 0.00043046716019\n",
      "Minibatch loss at step 179200: 0.028000\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 179250: 0.00043046716019\n",
      "Minibatch loss at step 179250: 0.192722\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 179300: 0.00043046716019\n",
      "Minibatch loss at step 179300: 0.012239\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 179350: 0.00043046716019\n",
      "Minibatch loss at step 179350: 0.112579\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 179400: 0.00043046716019\n",
      "Minibatch loss at step 179400: 0.150064\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 179450: 0.00043046716019\n",
      "Minibatch loss at step 179450: 0.106222\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 179500: 0.00043046716019\n",
      "Minibatch loss at step 179500: 0.068221\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 91.0%\n",
      "Learning rate at step 179550: 0.00043046716019\n",
      "Minibatch loss at step 179550: 0.183714\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 179600: 0.00043046716019\n",
      "Minibatch loss at step 179600: 0.062986\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 179650: 0.00043046716019\n",
      "Minibatch loss at step 179650: 0.286867\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 179700: 0.00043046716019\n",
      "Minibatch loss at step 179700: 0.056812\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 179750: 0.00043046716019\n",
      "Minibatch loss at step 179750: 0.006730\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 179800: 0.00043046716019\n",
      "Minibatch loss at step 179800: 0.043625\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 179850: 0.00043046716019\n",
      "Minibatch loss at step 179850: 0.071078\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 179900: 0.00043046716019\n",
      "Minibatch loss at step 179900: 0.003769\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 179950: 0.00043046716019\n",
      "Minibatch loss at step 179950: 0.009405\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 180000: 0.00038742041215\n",
      "Minibatch loss at step 180000: 0.048477\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.6%\n",
      "Learning rate at step 180050: 0.00038742041215\n",
      "Minibatch loss at step 180050: 0.107883\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 180100: 0.00038742041215\n",
      "Minibatch loss at step 180100: 0.010388\n",
      "Minibatch accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 180150: 0.00038742041215\n",
      "Minibatch loss at step 180150: 0.004718\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 180200: 0.00038742041215\n",
      "Minibatch loss at step 180200: 0.110731\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 180250: 0.00038742041215\n",
      "Minibatch loss at step 180250: 0.008286\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 180300: 0.00038742041215\n",
      "Minibatch loss at step 180300: 0.038160\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 180350: 0.00038742041215\n",
      "Minibatch loss at step 180350: 0.035757\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 180400: 0.00038742041215\n",
      "Minibatch loss at step 180400: 0.056998\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 180450: 0.00038742041215\n",
      "Minibatch loss at step 180450: 0.031925\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 180500: 0.00038742041215\n",
      "Minibatch loss at step 180500: 0.156964\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 180550: 0.00038742041215\n",
      "Minibatch loss at step 180550: 0.029653\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 180600: 0.00038742041215\n",
      "Minibatch loss at step 180600: 0.037641\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 180650: 0.00038742041215\n",
      "Minibatch loss at step 180650: 0.166823\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 180700: 0.00038742041215\n",
      "Minibatch loss at step 180700: 0.230738\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 180750: 0.00038742041215\n",
      "Minibatch loss at step 180750: 0.117370\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 180800: 0.00038742041215\n",
      "Minibatch loss at step 180800: 0.201966\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 180850: 0.00038742041215\n",
      "Minibatch loss at step 180850: 0.011486\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 180900: 0.00038742041215\n",
      "Minibatch loss at step 180900: 0.176868\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 180950: 0.00038742041215\n",
      "Minibatch loss at step 180950: 0.035415\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 181000: 0.00038742041215\n",
      "Minibatch loss at step 181000: 0.088519\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.0%\n",
      "Learning rate at step 181050: 0.00038742041215\n",
      "Minibatch loss at step 181050: 0.010271\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 181100: 0.00038742041215\n",
      "Minibatch loss at step 181100: 0.259651\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 181150: 0.00038742041215\n",
      "Minibatch loss at step 181150: 0.051933\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 181200: 0.00038742041215\n",
      "Minibatch loss at step 181200: 0.017768\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 181250: 0.00038742041215\n",
      "Minibatch loss at step 181250: 0.003286\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 181300: 0.00038742041215\n",
      "Minibatch loss at step 181300: 0.004829\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 181350: 0.00038742041215\n",
      "Minibatch loss at step 181350: 0.007217\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 181400: 0.00038742041215\n",
      "Minibatch loss at step 181400: 0.065945\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 181450: 0.00038742041215\n",
      "Minibatch loss at step 181450: 0.086116\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 181500: 0.00038742041215\n",
      "Minibatch loss at step 181500: 0.127741\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.5%\n",
      "Learning rate at step 181550: 0.00038742041215\n",
      "Minibatch loss at step 181550: 0.037316\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 181600: 0.00038742041215\n",
      "Minibatch loss at step 181600: 0.001565\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 181650: 0.00038742041215\n",
      "Minibatch loss at step 181650: 0.100788\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 181700: 0.00038742041215\n",
      "Minibatch loss at step 181700: 0.054872\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 181750: 0.00038742041215\n",
      "Minibatch loss at step 181750: 0.011923\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 181800: 0.00038742041215\n",
      "Minibatch loss at step 181800: 0.002104\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 181850: 0.00038742041215\n",
      "Minibatch loss at step 181850: 0.281395\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 181900: 0.00038742041215\n",
      "Minibatch loss at step 181900: 0.041644\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 181950: 0.00038742041215\n",
      "Minibatch loss at step 181950: 0.004631\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 182000: 0.00038742041215\n",
      "Minibatch loss at step 182000: 0.141693\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 182050: 0.00038742041215\n",
      "Minibatch loss at step 182050: 0.053455\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 182100: 0.00038742041215\n",
      "Minibatch loss at step 182100: 0.010113\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 182150: 0.00038742041215\n",
      "Minibatch loss at step 182150: 0.009653\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 182200: 0.00038742041215\n",
      "Minibatch loss at step 182200: 0.023360\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 182250: 0.00038742041215\n",
      "Minibatch loss at step 182250: 0.055582\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 182300: 0.00038742041215\n",
      "Minibatch loss at step 182300: 0.059437\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 182350: 0.00038742041215\n",
      "Minibatch loss at step 182350: 0.057946\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 182400: 0.00038742041215\n",
      "Minibatch loss at step 182400: 0.028707\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 182450: 0.00038742041215\n",
      "Minibatch loss at step 182450: 0.096971\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 182500: 0.00038742041215\n",
      "Minibatch loss at step 182500: 0.212136\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 182550: 0.00038742041215\n",
      "Minibatch loss at step 182550: 0.028991\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 182600: 0.00038742041215\n",
      "Minibatch loss at step 182600: 0.029226\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 182650: 0.00038742041215\n",
      "Minibatch loss at step 182650: 0.047444\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 182700: 0.00038742041215\n",
      "Minibatch loss at step 182700: 0.026254\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 182750: 0.00038742041215\n",
      "Minibatch loss at step 182750: 0.145003\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 182800: 0.00038742041215\n",
      "Minibatch loss at step 182800: 0.010607\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 182850: 0.00038742041215\n",
      "Minibatch loss at step 182850: 0.051261\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 182900: 0.00038742041215\n",
      "Minibatch loss at step 182900: 0.018354\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 182950: 0.00038742041215\n",
      "Minibatch loss at step 182950: 0.012177\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 183000: 0.00038742041215\n",
      "Minibatch loss at step 183000: 0.080357\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 183050: 0.00038742041215\n",
      "Minibatch loss at step 183050: 0.027509\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 183100: 0.00038742041215\n",
      "Minibatch loss at step 183100: 0.005909\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 183150: 0.00038742041215\n",
      "Minibatch loss at step 183150: 0.091516\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 183200: 0.00038742041215\n",
      "Minibatch loss at step 183200: 0.001307\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 183250: 0.00038742041215\n",
      "Minibatch loss at step 183250: 0.014969\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 183300: 0.00038742041215\n",
      "Minibatch loss at step 183300: 0.028087\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 183350: 0.00038742041215\n",
      "Minibatch loss at step 183350: 0.258132\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 183400: 0.00038742041215\n",
      "Minibatch loss at step 183400: 0.099970\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 183450: 0.00038742041215\n",
      "Minibatch loss at step 183450: 0.560662\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 183500: 0.00038742041215\n",
      "Minibatch loss at step 183500: 0.187304\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 183550: 0.00038742041215\n",
      "Minibatch loss at step 183550: 0.271556\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 183600: 0.00038742041215\n",
      "Minibatch loss at step 183600: 0.154561\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 183650: 0.00038742041215\n",
      "Minibatch loss at step 183650: 0.015037\n",
      "Minibatch accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 183700: 0.00038742041215\n",
      "Minibatch loss at step 183700: 0.248853\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 183750: 0.00038742041215\n",
      "Minibatch loss at step 183750: 0.027334\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 183800: 0.00038742041215\n",
      "Minibatch loss at step 183800: 0.103884\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 183850: 0.00038742041215\n",
      "Minibatch loss at step 183850: 0.015633\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 183900: 0.00038742041215\n",
      "Minibatch loss at step 183900: 0.285401\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 183950: 0.00038742041215\n",
      "Minibatch loss at step 183950: 0.280822\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 184000: 0.00038742041215\n",
      "Minibatch loss at step 184000: 0.258814\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 184050: 0.00038742041215\n",
      "Minibatch loss at step 184050: 0.056945\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 184100: 0.00038742041215\n",
      "Minibatch loss at step 184100: 0.068020\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 184150: 0.00038742041215\n",
      "Minibatch loss at step 184150: 0.098201\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 184200: 0.00038742041215\n",
      "Minibatch loss at step 184200: 0.050920\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 184250: 0.00038742041215\n",
      "Minibatch loss at step 184250: 0.017277\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 184300: 0.00038742041215\n",
      "Minibatch loss at step 184300: 0.032551\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 184350: 0.00038742041215\n",
      "Minibatch loss at step 184350: 0.032768\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 184400: 0.00038742041215\n",
      "Minibatch loss at step 184400: 0.020014\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 184450: 0.00038742041215\n",
      "Minibatch loss at step 184450: 0.048405\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 184500: 0.00038742041215\n",
      "Minibatch loss at step 184500: 0.003648\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 184550: 0.00038742041215\n",
      "Minibatch loss at step 184550: 0.003453\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 184600: 0.00038742041215\n",
      "Minibatch loss at step 184600: 0.027124\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 184650: 0.00038742041215\n",
      "Minibatch loss at step 184650: 0.016000\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 184700: 0.00038742041215\n",
      "Minibatch loss at step 184700: 0.047434\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 184750: 0.00038742041215\n",
      "Minibatch loss at step 184750: 0.056512\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 184800: 0.00038742041215\n",
      "Minibatch loss at step 184800: 0.129587\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 184850: 0.00038742041215\n",
      "Minibatch loss at step 184850: 0.010759\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 184900: 0.00038742041215\n",
      "Minibatch loss at step 184900: 0.014177\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 184950: 0.00038742041215\n",
      "Minibatch loss at step 184950: 0.217766\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 185000: 0.00038742041215\n",
      "Minibatch loss at step 185000: 0.075946\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 185050: 0.00038742041215\n",
      "Minibatch loss at step 185050: 0.018973\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 185100: 0.00038742041215\n",
      "Minibatch loss at step 185100: 0.060979\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 185150: 0.00038742041215\n",
      "Minibatch loss at step 185150: 0.024363\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 185200: 0.00038742041215\n",
      "Minibatch loss at step 185200: 0.003095\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 185250: 0.00038742041215\n",
      "Minibatch loss at step 185250: 0.067411\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 185300: 0.00038742041215\n",
      "Minibatch loss at step 185300: 0.046697\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 185350: 0.00038742041215\n",
      "Minibatch loss at step 185350: 0.177167\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 185400: 0.00038742041215\n",
      "Minibatch loss at step 185400: 0.101060\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 185450: 0.00038742041215\n",
      "Minibatch loss at step 185450: 0.069645\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 185500: 0.00038742041215\n",
      "Minibatch loss at step 185500: 0.071338\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 185550: 0.00038742041215\n",
      "Minibatch loss at step 185550: 0.006137\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 185600: 0.00038742041215\n",
      "Minibatch loss at step 185600: 0.005976\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 185650: 0.00038742041215\n",
      "Minibatch loss at step 185650: 0.129612\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 185700: 0.00038742041215\n",
      "Minibatch loss at step 185700: 0.020477\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 185750: 0.00038742041215\n",
      "Minibatch loss at step 185750: 0.017591\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 185800: 0.00038742041215\n",
      "Minibatch loss at step 185800: 0.158842\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 185850: 0.00038742041215\n",
      "Minibatch loss at step 185850: 0.009774\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 185900: 0.00038742041215\n",
      "Minibatch loss at step 185900: 0.176913\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 185950: 0.00038742041215\n",
      "Minibatch loss at step 185950: 0.071567\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 186000: 0.00038742041215\n",
      "Minibatch loss at step 186000: 0.013658\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.8%\n",
      "Learning rate at step 186050: 0.00038742041215\n",
      "Minibatch loss at step 186050: 0.090870\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 186100: 0.00038742041215\n",
      "Minibatch loss at step 186100: 0.212702\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 186150: 0.00038742041215\n",
      "Minibatch loss at step 186150: 0.006058\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 186200: 0.00038742041215\n",
      "Minibatch loss at step 186200: 0.033307\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 186250: 0.00038742041215\n",
      "Minibatch loss at step 186250: 0.100964\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 186300: 0.00038742041215\n",
      "Minibatch loss at step 186300: 0.220236\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 186350: 0.00038742041215\n",
      "Minibatch loss at step 186350: 0.188770\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 186400: 0.00038742041215\n",
      "Minibatch loss at step 186400: 0.071968\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 186450: 0.00038742041215\n",
      "Minibatch loss at step 186450: 0.148705\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 186500: 0.00038742041215\n",
      "Minibatch loss at step 186500: 0.191173\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 186550: 0.00038742041215\n",
      "Minibatch loss at step 186550: 0.307575\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 186600: 0.00038742041215\n",
      "Minibatch loss at step 186600: 0.088327\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 186650: 0.00038742041215\n",
      "Minibatch loss at step 186650: 0.113851\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 186700: 0.00038742041215\n",
      "Minibatch loss at step 186700: 0.285958\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 186750: 0.00038742041215\n",
      "Minibatch loss at step 186750: 0.025309\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 186800: 0.00038742041215\n",
      "Minibatch loss at step 186800: 0.034665\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 186850: 0.00038742041215\n",
      "Minibatch loss at step 186850: 0.006466\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 186900: 0.00038742041215\n",
      "Minibatch loss at step 186900: 0.105270\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 186950: 0.00038742041215\n",
      "Minibatch loss at step 186950: 0.022560\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 187000: 0.00038742041215\n",
      "Minibatch loss at step 187000: 0.102051\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 187050: 0.00038742041215\n",
      "Minibatch loss at step 187050: 0.167808\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 187100: 0.00038742041215\n",
      "Minibatch loss at step 187100: 0.056847\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 187150: 0.00038742041215\n",
      "Minibatch loss at step 187150: 0.012376\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 187200: 0.00038742041215\n",
      "Minibatch loss at step 187200: 0.027732\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 187250: 0.00038742041215\n",
      "Minibatch loss at step 187250: 0.001431\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 187300: 0.00038742041215\n",
      "Minibatch loss at step 187300: 0.016883\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 187350: 0.00038742041215\n",
      "Minibatch loss at step 187350: 0.002132\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 187400: 0.00038742041215\n",
      "Minibatch loss at step 187400: 0.023131\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 187450: 0.00038742041215\n",
      "Minibatch loss at step 187450: 0.094418\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 187500: 0.00038742041215\n",
      "Minibatch loss at step 187500: 0.027482\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 187550: 0.00038742041215\n",
      "Minibatch loss at step 187550: 0.039971\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 187600: 0.00038742041215\n",
      "Minibatch loss at step 187600: 0.060106\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 187650: 0.00038742041215\n",
      "Minibatch loss at step 187650: 0.076180\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 187700: 0.00038742041215\n",
      "Minibatch loss at step 187700: 0.150528\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 187750: 0.00038742041215\n",
      "Minibatch loss at step 187750: 0.103073\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 187800: 0.00038742041215\n",
      "Minibatch loss at step 187800: 0.088160\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 187850: 0.00038742041215\n",
      "Minibatch loss at step 187850: 0.053416\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 187900: 0.00038742041215\n",
      "Minibatch loss at step 187900: 0.181137\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 187950: 0.00038742041215\n",
      "Minibatch loss at step 187950: 0.042134\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 188000: 0.00038742041215\n",
      "Minibatch loss at step 188000: 0.108552\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 188050: 0.00038742041215\n",
      "Minibatch loss at step 188050: 0.116269\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 188100: 0.00038742041215\n",
      "Minibatch loss at step 188100: 0.325351\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 188150: 0.00038742041215\n",
      "Minibatch loss at step 188150: 0.025786\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 188200: 0.00038742041215\n",
      "Minibatch loss at step 188200: 0.045504\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 188250: 0.00038742041215\n",
      "Minibatch loss at step 188250: 0.054034\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 188300: 0.00038742041215\n",
      "Minibatch loss at step 188300: 0.005800\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 188350: 0.00038742041215\n",
      "Minibatch loss at step 188350: 0.004314\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 188400: 0.00038742041215\n",
      "Minibatch loss at step 188400: 0.022908\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 188450: 0.00038742041215\n",
      "Minibatch loss at step 188450: 0.009394\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 188500: 0.00038742041215\n",
      "Minibatch loss at step 188500: 0.021144\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.7%\n",
      "Learning rate at step 188550: 0.00038742041215\n",
      "Minibatch loss at step 188550: 0.024801\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 188600: 0.00038742041215\n",
      "Minibatch loss at step 188600: 0.019394\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 188650: 0.00038742041215\n",
      "Minibatch loss at step 188650: 0.135872\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 188700: 0.00038742041215\n",
      "Minibatch loss at step 188700: 0.062016\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 188750: 0.00038742041215\n",
      "Minibatch loss at step 188750: 0.078980\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 188800: 0.00038742041215\n",
      "Minibatch loss at step 188800: 0.097756\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 188850: 0.00038742041215\n",
      "Minibatch loss at step 188850: 0.043263\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 188900: 0.00038742041215\n",
      "Minibatch loss at step 188900: 0.048964\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 188950: 0.00038742041215\n",
      "Minibatch loss at step 188950: 0.005820\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 189000: 0.00038742041215\n",
      "Minibatch loss at step 189000: 0.002667\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.6%\n",
      "Learning rate at step 189050: 0.00038742041215\n",
      "Minibatch loss at step 189050: 0.056623\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 189100: 0.00038742041215\n",
      "Minibatch loss at step 189100: 0.133501\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 189150: 0.00038742041215\n",
      "Minibatch loss at step 189150: 0.061489\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 189200: 0.00038742041215\n",
      "Minibatch loss at step 189200: 0.128369\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 189250: 0.00038742041215\n",
      "Minibatch loss at step 189250: 0.202681\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 189300: 0.00038742041215\n",
      "Minibatch loss at step 189300: 0.056258\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 189350: 0.00038742041215\n",
      "Minibatch loss at step 189350: 0.013603\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 189400: 0.00038742041215\n",
      "Minibatch loss at step 189400: 0.204197\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 189450: 0.00038742041215\n",
      "Minibatch loss at step 189450: 0.146819\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 189500: 0.00038742041215\n",
      "Minibatch loss at step 189500: 0.114951\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 189550: 0.00038742041215\n",
      "Minibatch loss at step 189550: 0.209672\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 189600: 0.00038742041215\n",
      "Minibatch loss at step 189600: 0.031723\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 189650: 0.00038742041215\n",
      "Minibatch loss at step 189650: 0.059418\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 189700: 0.00038742041215\n",
      "Minibatch loss at step 189700: 0.006739\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 189750: 0.00038742041215\n",
      "Minibatch loss at step 189750: 0.030108\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 189800: 0.00038742041215\n",
      "Minibatch loss at step 189800: 0.016243\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 189850: 0.00038742041215\n",
      "Minibatch loss at step 189850: 0.061170\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 189900: 0.00038742041215\n",
      "Minibatch loss at step 189900: 0.128445\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 189950: 0.00038742041215\n",
      "Minibatch loss at step 189950: 0.004067\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 190000: 0.00038742041215\n",
      "Minibatch loss at step 190000: 0.482309\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 90.6%\n",
      "Learning rate at step 190050: 0.00038742041215\n",
      "Minibatch loss at step 190050: 0.014456\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 190100: 0.00038742041215\n",
      "Minibatch loss at step 190100: 0.007627\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 190150: 0.00038742041215\n",
      "Minibatch loss at step 190150: 0.005691\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 190200: 0.00038742041215\n",
      "Minibatch loss at step 190200: 0.003298\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 190250: 0.00038742041215\n",
      "Minibatch loss at step 190250: 0.043550\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 190300: 0.00038742041215\n",
      "Minibatch loss at step 190300: 0.039868\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 190350: 0.00038742041215\n",
      "Minibatch loss at step 190350: 0.034168\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 190400: 0.00038742041215\n",
      "Minibatch loss at step 190400: 0.032684\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 190450: 0.00038742041215\n",
      "Minibatch loss at step 190450: 0.080972\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 190500: 0.00038742041215\n",
      "Minibatch loss at step 190500: 0.088342\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.9%\n",
      "Learning rate at step 190550: 0.00038742041215\n",
      "Minibatch loss at step 190550: 0.010565\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 190600: 0.00038742041215\n",
      "Minibatch loss at step 190600: 0.003376\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 190650: 0.00038742041215\n",
      "Minibatch loss at step 190650: 0.180914\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 190700: 0.00038742041215\n",
      "Minibatch loss at step 190700: 0.144251\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 190750: 0.00038742041215\n",
      "Minibatch loss at step 190750: 0.291635\n",
      "Minibatch accuracy: 93.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 190800: 0.00038742041215\n",
      "Minibatch loss at step 190800: 0.108572\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 190850: 0.00038742041215\n",
      "Minibatch loss at step 190850: 0.131970\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 190900: 0.00038742041215\n",
      "Minibatch loss at step 190900: 0.003929\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 190950: 0.00038742041215\n",
      "Minibatch loss at step 190950: 0.073402\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 191000: 0.00038742041215\n",
      "Minibatch loss at step 191000: 0.026712\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.5%\n",
      "Learning rate at step 191050: 0.00038742041215\n",
      "Minibatch loss at step 191050: 0.015901\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 191100: 0.00038742041215\n",
      "Minibatch loss at step 191100: 0.111359\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 191150: 0.00038742041215\n",
      "Minibatch loss at step 191150: 0.198847\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 191200: 0.00038742041215\n",
      "Minibatch loss at step 191200: 0.029613\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 191250: 0.00038742041215\n",
      "Minibatch loss at step 191250: 0.071536\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 191300: 0.00038742041215\n",
      "Minibatch loss at step 191300: 0.031598\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 191350: 0.00038742041215\n",
      "Minibatch loss at step 191350: 0.064214\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 191400: 0.00038742041215\n",
      "Minibatch loss at step 191400: 0.025675\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 191450: 0.00038742041215\n",
      "Minibatch loss at step 191450: 0.030376\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 191500: 0.00038742041215\n",
      "Minibatch loss at step 191500: 0.131219\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.7%\n",
      "Learning rate at step 191550: 0.00038742041215\n",
      "Minibatch loss at step 191550: 0.018878\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 191600: 0.00038742041215\n",
      "Minibatch loss at step 191600: 0.016061\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 191650: 0.00038742041215\n",
      "Minibatch loss at step 191650: 0.070781\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 191700: 0.00038742041215\n",
      "Minibatch loss at step 191700: 0.079685\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 191750: 0.00038742041215\n",
      "Minibatch loss at step 191750: 0.048175\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 191800: 0.00038742041215\n",
      "Minibatch loss at step 191800: 0.011218\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 191850: 0.00038742041215\n",
      "Minibatch loss at step 191850: 0.126922\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 191900: 0.00038742041215\n",
      "Minibatch loss at step 191900: 0.034882\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 191950: 0.00038742041215\n",
      "Minibatch loss at step 191950: 0.116032\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 192000: 0.00038742041215\n",
      "Minibatch loss at step 192000: 0.187323\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 192050: 0.00038742041215\n",
      "Minibatch loss at step 192050: 0.149547\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 192100: 0.00038742041215\n",
      "Minibatch loss at step 192100: 0.091260\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 192150: 0.00038742041215\n",
      "Minibatch loss at step 192150: 0.082781\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 192200: 0.00038742041215\n",
      "Minibatch loss at step 192200: 0.129702\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 192250: 0.00038742041215\n",
      "Minibatch loss at step 192250: 0.021858\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 192300: 0.00038742041215\n",
      "Minibatch loss at step 192300: 0.208775\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 192350: 0.00038742041215\n",
      "Minibatch loss at step 192350: 0.137749\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 192400: 0.00038742041215\n",
      "Minibatch loss at step 192400: 0.212102\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 192450: 0.00038742041215\n",
      "Minibatch loss at step 192450: 0.127111\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 192500: 0.00038742041215\n",
      "Minibatch loss at step 192500: 0.020512\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 192550: 0.00038742041215\n",
      "Minibatch loss at step 192550: 0.086446\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 192600: 0.00038742041215\n",
      "Minibatch loss at step 192600: 0.044360\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 192650: 0.00038742041215\n",
      "Minibatch loss at step 192650: 0.011174\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 192700: 0.00038742041215\n",
      "Minibatch loss at step 192700: 0.007269\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 192750: 0.00038742041215\n",
      "Minibatch loss at step 192750: 0.002032\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 192800: 0.00038742041215\n",
      "Minibatch loss at step 192800: 0.321498\n",
      "Minibatch accuracy: 90.6%\n",
      "Learning rate at step 192850: 0.00038742041215\n",
      "Minibatch loss at step 192850: 0.016589\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 192900: 0.00038742041215\n",
      "Minibatch loss at step 192900: 0.020921\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 192950: 0.00038742041215\n",
      "Minibatch loss at step 192950: 0.033412\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 193000: 0.00038742041215\n",
      "Minibatch loss at step 193000: 0.180669\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.8%\n",
      "Learning rate at step 193050: 0.00038742041215\n",
      "Minibatch loss at step 193050: 0.056736\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 193100: 0.00038742041215\n",
      "Minibatch loss at step 193100: 0.007646\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 193150: 0.00038742041215\n",
      "Minibatch loss at step 193150: 0.133027\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 193200: 0.00038742041215\n",
      "Minibatch loss at step 193200: 0.000927\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 193250: 0.00038742041215\n",
      "Minibatch loss at step 193250: 0.128781\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 193300: 0.00038742041215\n",
      "Minibatch loss at step 193300: 0.023195\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 193350: 0.00038742041215\n",
      "Minibatch loss at step 193350: 0.039356\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 193400: 0.00038742041215\n",
      "Minibatch loss at step 193400: 0.082322\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 193450: 0.00038742041215\n",
      "Minibatch loss at step 193450: 0.039536\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 193500: 0.00038742041215\n",
      "Minibatch loss at step 193500: 0.102411\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 193550: 0.00038742041215\n",
      "Minibatch loss at step 193550: 0.040656\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 193600: 0.00038742041215\n",
      "Minibatch loss at step 193600: 0.111532\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 193650: 0.00038742041215\n",
      "Minibatch loss at step 193650: 0.226496\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 193700: 0.00038742041215\n",
      "Minibatch loss at step 193700: 0.021898\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 193750: 0.00038742041215\n",
      "Minibatch loss at step 193750: 0.169976\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 193800: 0.00038742041215\n",
      "Minibatch loss at step 193800: 0.190960\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 193850: 0.00038742041215\n",
      "Minibatch loss at step 193850: 0.036165\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 193900: 0.00038742041215\n",
      "Minibatch loss at step 193900: 0.025535\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 193950: 0.00038742041215\n",
      "Minibatch loss at step 193950: 0.092549\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 194000: 0.00038742041215\n",
      "Minibatch loss at step 194000: 0.006142\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 194050: 0.00038742041215\n",
      "Minibatch loss at step 194050: 0.072000\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 194100: 0.00038742041215\n",
      "Minibatch loss at step 194100: 0.017017\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 194150: 0.00038742041215\n",
      "Minibatch loss at step 194150: 0.038529\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 194200: 0.00038742041215\n",
      "Minibatch loss at step 194200: 0.202251\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 194250: 0.00038742041215\n",
      "Minibatch loss at step 194250: 0.101033\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 194300: 0.00038742041215\n",
      "Minibatch loss at step 194300: 0.053467\n",
      "Minibatch accuracy: 98.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 194350: 0.00038742041215\n",
      "Minibatch loss at step 194350: 0.091269\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 194400: 0.00038742041215\n",
      "Minibatch loss at step 194400: 0.018682\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 194450: 0.00038742041215\n",
      "Minibatch loss at step 194450: 0.170719\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 194500: 0.00038742041215\n",
      "Minibatch loss at step 194500: 0.007409\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 194550: 0.00038742041215\n",
      "Minibatch loss at step 194550: 0.003904\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 194600: 0.00038742041215\n",
      "Minibatch loss at step 194600: 0.025907\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 194650: 0.00038742041215\n",
      "Minibatch loss at step 194650: 0.005991\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 194700: 0.00038742041215\n",
      "Minibatch loss at step 194700: 0.057582\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 194750: 0.00038742041215\n",
      "Minibatch loss at step 194750: 0.004602\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 194800: 0.00038742041215\n",
      "Minibatch loss at step 194800: 0.127768\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 194850: 0.00038742041215\n",
      "Minibatch loss at step 194850: 0.106370\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 194900: 0.00038742041215\n",
      "Minibatch loss at step 194900: 0.119789\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 194950: 0.00038742041215\n",
      "Minibatch loss at step 194950: 0.152569\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 195000: 0.00038742041215\n",
      "Minibatch loss at step 195000: 0.123991\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 195050: 0.00038742041215\n",
      "Minibatch loss at step 195050: 0.063523\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 195100: 0.00038742041215\n",
      "Minibatch loss at step 195100: 0.076740\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 195150: 0.00038742041215\n",
      "Minibatch loss at step 195150: 0.113566\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 195200: 0.00038742041215\n",
      "Minibatch loss at step 195200: 0.125024\n",
      "Minibatch accuracy: 92.2%\n",
      "Learning rate at step 195250: 0.00038742041215\n",
      "Minibatch loss at step 195250: 0.174865\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 195300: 0.00038742041215\n",
      "Minibatch loss at step 195300: 0.056701\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 195350: 0.00038742041215\n",
      "Minibatch loss at step 195350: 0.041203\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 195400: 0.00038742041215\n",
      "Minibatch loss at step 195400: 0.096335\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 195450: 0.00038742041215\n",
      "Minibatch loss at step 195450: 0.013029\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 195500: 0.00038742041215\n",
      "Minibatch loss at step 195500: 0.067187\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 195550: 0.00038742041215\n",
      "Minibatch loss at step 195550: 0.019297\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 195600: 0.00038742041215\n",
      "Minibatch loss at step 195600: 0.145423\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 195650: 0.00038742041215\n",
      "Minibatch loss at step 195650: 0.066207\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 195700: 0.00038742041215\n",
      "Minibatch loss at step 195700: 0.012202\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 195750: 0.00038742041215\n",
      "Minibatch loss at step 195750: 0.136740\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 195800: 0.00038742041215\n",
      "Minibatch loss at step 195800: 0.018246\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 195850: 0.00038742041215\n",
      "Minibatch loss at step 195850: 0.067320\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 195900: 0.00038742041215\n",
      "Minibatch loss at step 195900: 0.056368\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 195950: 0.00038742041215\n",
      "Minibatch loss at step 195950: 0.025443\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 196000: 0.00038742041215\n",
      "Minibatch loss at step 196000: 0.118204\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 196050: 0.00038742041215\n",
      "Minibatch loss at step 196050: 0.016023\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 196100: 0.00038742041215\n",
      "Minibatch loss at step 196100: 0.005379\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 196150: 0.00038742041215\n",
      "Minibatch loss at step 196150: 0.033339\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 196200: 0.00038742041215\n",
      "Minibatch loss at step 196200: 0.004255\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 196250: 0.00038742041215\n",
      "Minibatch loss at step 196250: 0.071475\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 196300: 0.00038742041215\n",
      "Minibatch loss at step 196300: 0.185896\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 196350: 0.00038742041215\n",
      "Minibatch loss at step 196350: 0.165681\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 196400: 0.00038742041215\n",
      "Minibatch loss at step 196400: 0.119499\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 196450: 0.00038742041215\n",
      "Minibatch loss at step 196450: 0.043512\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 196500: 0.00038742041215\n",
      "Minibatch loss at step 196500: 0.110007\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 196550: 0.00038742041215\n",
      "Minibatch loss at step 196550: 0.053807\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 196600: 0.00038742041215\n",
      "Minibatch loss at step 196600: 0.004987\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 196650: 0.00038742041215\n",
      "Minibatch loss at step 196650: 0.040994\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 196700: 0.00038742041215\n",
      "Minibatch loss at step 196700: 0.015023\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 196750: 0.00038742041215\n",
      "Minibatch loss at step 196750: 0.001678\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 196800: 0.00038742041215\n",
      "Minibatch loss at step 196800: 0.135413\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 196850: 0.00038742041215\n",
      "Minibatch loss at step 196850: 0.002728\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 196900: 0.00038742041215\n",
      "Minibatch loss at step 196900: 0.186906\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 196950: 0.00038742041215\n",
      "Minibatch loss at step 196950: 0.017444\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 197000: 0.00038742041215\n",
      "Minibatch loss at step 197000: 0.066102\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.2%\n",
      "Learning rate at step 197050: 0.00038742041215\n",
      "Minibatch loss at step 197050: 0.197827\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 197100: 0.00038742041215\n",
      "Minibatch loss at step 197100: 0.283473\n",
      "Minibatch accuracy: 93.8%\n",
      "Learning rate at step 197150: 0.00038742041215\n",
      "Minibatch loss at step 197150: 0.080767\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 197200: 0.00038742041215\n",
      "Minibatch loss at step 197200: 0.165990\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 197250: 0.00038742041215\n",
      "Minibatch loss at step 197250: 0.022371\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 197300: 0.00038742041215\n",
      "Minibatch loss at step 197300: 0.147683\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 197350: 0.00038742041215\n",
      "Minibatch loss at step 197350: 0.015423\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 197400: 0.00038742041215\n",
      "Minibatch loss at step 197400: 0.057706\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 197450: 0.00038742041215\n",
      "Minibatch loss at step 197450: 0.075091\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 197500: 0.00038742041215\n",
      "Minibatch loss at step 197500: 0.036832\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.3%\n",
      "Learning rate at step 197550: 0.00038742041215\n",
      "Minibatch loss at step 197550: 0.120632\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 197600: 0.00038742041215\n",
      "Minibatch loss at step 197600: 0.085695\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 197650: 0.00038742041215\n",
      "Minibatch loss at step 197650: 0.095604\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 197700: 0.00038742041215\n",
      "Minibatch loss at step 197700: 0.020937\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 197750: 0.00038742041215\n",
      "Minibatch loss at step 197750: 0.019582\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 197800: 0.00038742041215\n",
      "Minibatch loss at step 197800: 0.066360\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 197850: 0.00038742041215\n",
      "Minibatch loss at step 197850: 0.136400\n",
      "Minibatch accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate at step 197900: 0.00038742041215\n",
      "Minibatch loss at step 197900: 0.241490\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 197950: 0.00038742041215\n",
      "Minibatch loss at step 197950: 0.255481\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 198000: 0.00038742041215\n",
      "Minibatch loss at step 198000: 0.249212\n",
      "Minibatch accuracy: 93.8%\n",
      "validation accuracy: 89.8%\n",
      "Learning rate at step 198050: 0.00038742041215\n",
      "Minibatch loss at step 198050: 0.100872\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 198100: 0.00038742041215\n",
      "Minibatch loss at step 198100: 0.108351\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 198150: 0.00038742041215\n",
      "Minibatch loss at step 198150: 0.051183\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 198200: 0.00038742041215\n",
      "Minibatch loss at step 198200: 0.008507\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 198250: 0.00038742041215\n",
      "Minibatch loss at step 198250: 0.034121\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 198300: 0.00038742041215\n",
      "Minibatch loss at step 198300: 0.008965\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 198350: 0.00038742041215\n",
      "Minibatch loss at step 198350: 0.013760\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 198400: 0.00038742041215\n",
      "Minibatch loss at step 198400: 0.024704\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 198450: 0.00038742041215\n",
      "Minibatch loss at step 198450: 0.139273\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 198500: 0.00038742041215\n",
      "Minibatch loss at step 198500: 0.145209\n",
      "Minibatch accuracy: 96.9%\n",
      "validation accuracy: 89.9%\n",
      "Learning rate at step 198550: 0.00038742041215\n",
      "Minibatch loss at step 198550: 0.264794\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 198600: 0.00038742041215\n",
      "Minibatch loss at step 198600: 0.066782\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 198650: 0.00038742041215\n",
      "Minibatch loss at step 198650: 0.011124\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 198700: 0.00038742041215\n",
      "Minibatch loss at step 198700: 0.057888\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 198750: 0.00038742041215\n",
      "Minibatch loss at step 198750: 0.003586\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 198800: 0.00038742041215\n",
      "Minibatch loss at step 198800: 0.046854\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 198850: 0.00038742041215\n",
      "Minibatch loss at step 198850: 0.004358\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 198900: 0.00038742041215\n",
      "Minibatch loss at step 198900: 0.042964\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 198950: 0.00038742041215\n",
      "Minibatch loss at step 198950: 0.102474\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 199000: 0.00038742041215\n",
      "Minibatch loss at step 199000: 0.103563\n",
      "Minibatch accuracy: 95.3%\n",
      "validation accuracy: 90.4%\n",
      "Learning rate at step 199050: 0.00038742041215\n",
      "Minibatch loss at step 199050: 0.027330\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 199100: 0.00038742041215\n",
      "Minibatch loss at step 199100: 0.032985\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 199150: 0.00038742041215\n",
      "Minibatch loss at step 199150: 0.014439\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 199200: 0.00038742041215\n",
      "Minibatch loss at step 199200: 0.116617\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 199250: 0.00038742041215\n",
      "Minibatch loss at step 199250: 0.006576\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 199300: 0.00038742041215\n",
      "Minibatch loss at step 199300: 0.177199\n",
      "Minibatch accuracy: 96.9%\n",
      "Learning rate at step 199350: 0.00038742041215\n",
      "Minibatch loss at step 199350: 0.006154\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 199400: 0.00038742041215\n",
      "Minibatch loss at step 199400: 0.082624\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 199450: 0.00038742041215\n",
      "Minibatch loss at step 199450: 0.013248\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 199500: 0.00038742041215\n",
      "Minibatch loss at step 199500: 0.030381\n",
      "Minibatch accuracy: 98.4%\n",
      "validation accuracy: 90.1%\n",
      "Learning rate at step 199550: 0.00038742041215\n",
      "Minibatch loss at step 199550: 0.036247\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 199600: 0.00038742041215\n",
      "Minibatch loss at step 199600: 0.122195\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 199650: 0.00038742041215\n",
      "Minibatch loss at step 199650: 0.012837\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 199700: 0.00038742041215\n",
      "Minibatch loss at step 199700: 0.005217\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 199750: 0.00038742041215\n",
      "Minibatch loss at step 199750: 0.075606\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 199800: 0.00038742041215\n",
      "Minibatch loss at step 199800: 0.005689\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 199850: 0.00038742041215\n",
      "Minibatch loss at step 199850: 0.121282\n",
      "Minibatch accuracy: 95.3%\n",
      "Learning rate at step 199900: 0.00038742041215\n",
      "Minibatch loss at step 199900: 0.028560\n",
      "Minibatch accuracy: 98.4%\n",
      "Learning rate at step 199950: 0.00038742041215\n",
      "Minibatch loss at step 199950: 0.021866\n",
      "Minibatch accuracy: 100.0%\n",
      "Learning rate at step 200000: 0.00034867835348\n",
      "Minibatch loss at step 200000: 0.003411\n",
      "Minibatch accuracy: 100.0%\n",
      "validation accuracy: 89.9%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph, config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver = tf.train.Saver().\n",
    "    writer = tf.summary.FileWriter('D:/magistratura/magistratura/MO/lab4/graph_info', session.graph)\n",
    "\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        #take batch data\n",
    "        batch_size = 64 \n",
    "        offset = (step * batch_size) % (train_size - batch_size)\n",
    "        batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = []\n",
    "        for i in range(num_classifiers):\n",
    "            batch_labels.append(trainOneHotLabels[i][offset:(offset + batch_size), :])\n",
    "            \n",
    "        #create feed dict with batch data and batch labels\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels_c1: batch_labels[0],\n",
    "                     tf_train_labels_c2: batch_labels[1], tf_train_labels_c3: batch_labels[2],\n",
    "                     tf_train_labels_c4: batch_labels[3], tf_train_labels_c5: batch_labels[4],\n",
    "                     tf_train_labels_c6: batch_labels[5]}\n",
    "        \n",
    "        #run adam optimizer with optimize function\n",
    "        _, l, c1, c2, c3, c4, c5, c6, lr = session.run(\n",
    "            [optimize, loss, train_prediction_c1, train_prediction_c2, train_prediction_c3, train_prediction_c4,\n",
    "             train_prediction_c5, train_prediction_c6, learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        predictions = [c1, c2, c3, c4, c5, c6]\n",
    "        \n",
    "        if (step % 50 == 0):\n",
    "            \n",
    "            batch_size_init = predictions[0].shape[0]\n",
    "            \n",
    "            #create predictions as string\n",
    "            batch_size = predictions[0].shape[0]\n",
    "            all_labels = []\n",
    "            for i in range(batch_size):\n",
    "                num_digits = np.argmax(predictions[0][i])\n",
    "                st = str(num_digits)\n",
    "                for j in range(1, num_classifiers):\n",
    "                    if (j > num_digits):\n",
    "                        break\n",
    "                    st = st + str(np.argmax(predictions[j][i]))\n",
    "                all_labels.append(st)\n",
    "            \n",
    "            \n",
    "            predictions = all_labels\n",
    "            \n",
    "            \n",
    "            #create labels as string\n",
    "            batch_size = batch_labels[0].shape[0]\n",
    "            all_labels = []\n",
    "            for i in range(batch_size):\n",
    "                num_digits = np.argmax(batch_labels[0][i])\n",
    "                st = str(num_digits)\n",
    "                for j in range(1, num_classifiers):\n",
    "                    if (j > num_digits):\n",
    "                        break\n",
    "                    st = st + str(np.argmax(batch_labels[j][i]))\n",
    "                all_labels.append(st)\n",
    "\n",
    "                \n",
    "            labels = all_labels\n",
    "            \n",
    "            \n",
    "            #count the number of matches and non-matches\n",
    "            equalities = np.zeros(batch_size_init)\n",
    "            for i in range(batch_size_init):\n",
    "                if predictions[i] == labels[i]:\n",
    "                    equalities[i] = 1\n",
    "            sum = np.sum(equalities)\n",
    "            \n",
    "            #calc accuracy\n",
    "            acc = (100.0 * sum) / batch_size_init\n",
    "            \n",
    "            \n",
    "            \n",
    "            print('Learning rate at step %d: %.14f' % (step, lr))\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            batch_train_accuracy = acc\n",
    "            print('Minibatch accuracy: %.1f%%' % batch_train_accuracy)\n",
    "            training_loss.append(l)\n",
    "            training_loss_epoch.append(step)\n",
    "            train_accuracy.append(batch_train_accuracy)\n",
    "            train_accuracy_epoch.append(step)\n",
    "            if(lr==0):   #if learning rate reaches 0 break\n",
    "                break\n",
    "        \n",
    "        #every 500 iteration calculate validation accuracy\n",
    "        if (step % 500 == 0):   \n",
    "            c1, c2, c3, c4, c5, c6 = session.run(\n",
    "                [valid_prediction_c1, valid_prediction_c2, valid_prediction_c3, valid_prediction_c4,\n",
    "                 valid_prediction_c5, valid_prediction_c6])\n",
    "            predictions = [c1, c2, c3, c4, c5, c6]\n",
    "            \n",
    "            batch_size_init = predictions[0].shape[0]\n",
    "            \n",
    "            \n",
    "            batch_size = predictions[0].shape[0]\n",
    "            all_labels = []\n",
    "            for i in range(batch_size):\n",
    "                num_digits = np.argmax(predictions[0][i])\n",
    "                st = str(num_digits)\n",
    "                for j in range(1, num_classifiers):\n",
    "                    if (j > num_digits):\n",
    "                        break\n",
    "                    st = st + str(np.argmax(predictions[j][i]))\n",
    "                all_labels.append(st)\n",
    "            \n",
    "            \n",
    "            predictions = all_labels\n",
    "            \n",
    "            \n",
    "        \n",
    "            batch_size = validOneHotLabels[0].shape[0]\n",
    "            all_labels = []\n",
    "            for i in range(batch_size):\n",
    "                num_digits = np.argmax(validOneHotLabels[0][i])\n",
    "                st = str(num_digits)\n",
    "                for j in range(1, num_classifiers):\n",
    "                    if (j > num_digits):\n",
    "                        break\n",
    "                    st = st + str(np.argmax(validOneHotLabels[j][i]))\n",
    "                all_labels.append(st)\n",
    "\n",
    "                \n",
    "            labels = all_labels\n",
    "            \n",
    "            \n",
    "            equalities = np.zeros(batch_size_init)\n",
    "            for i in range(batch_size_init):\n",
    "                if predictions[i] == labels[i]:\n",
    "                    equalities[i] = 1\n",
    "            sum = np.sum(equalities)\n",
    "            acc = (100.0 * sum) / batch_size_init\n",
    "            \n",
    "            \n",
    "            validation_accuracy = acc\n",
    "            print('validation accuracy: %.1f%%' % validation_accuracy)\n",
    "            valid_accuracy.append(validation_accuracy)\n",
    "            valid_accuracy_epoch.append(step)\n",
    "\n",
    "            \n",
    "            \n",
    "    #get test predictions in steps to avoid memory problems\n",
    "    test_pred_c1 = np.zeros((test_size, num_digits_labels))\n",
    "    test_pred_c2 = np.zeros((test_size, digits_labels))\n",
    "    test_pred_c3 = np.zeros((test_size, digits_labels))\n",
    "    test_pred_c4 = np.zeros((test_size, digits_labels))\n",
    "    test_pred_c5 = np.zeros((test_size, digits_labels))\n",
    "    test_pred_c6 = np.zeros((test_size, digits_labels))\n",
    "\n",
    "    for step in range(int(test_size / test_batch_size)):\n",
    "        offset = (step * test_batch_size) % (test_size - test_batch_size)\n",
    "        batch_data = test_data[offset:(offset + test_batch_size), :, :, :]\n",
    "        feed_dict = {tf_test_dataset: batch_data}\n",
    "        c1, c2, c3, c4, c5, c6 = session.run(\n",
    "            [test_prediction_c1, test_prediction_c2, test_prediction_c3, test_prediction_c4, test_prediction_c5,\n",
    "             test_prediction_c6], feed_dict=feed_dict)\n",
    "\n",
    "        test_pred_c1[offset:offset + test_batch_size] = c1\n",
    "        test_pred_c2[offset:offset + test_batch_size] = c2\n",
    "        test_pred_c3[offset:offset + test_batch_size] = c3\n",
    "        test_pred_c4[offset:offset + test_batch_size] = c4\n",
    "        test_pred_c5[offset:offset + test_batch_size] = c5\n",
    "        test_pred_c6[offset:offset + test_batch_size] = c6\n",
    "        \n",
    "    # calculate test accuracy\n",
    "    predictions = [test_pred_c1, test_pred_c2, test_pred_c3, test_pred_c4, test_pred_c5, test_pred_c6]\n",
    "    \n",
    "    \n",
    "    batch_size_init = predictions[0].shape[0]\n",
    "    \n",
    "    batch_size = predictions[0].shape[0]\n",
    "    all_labels = []\n",
    "    for i in range(batch_size):\n",
    "        num_digits = np.argmax(predictions[0][i])\n",
    "        st = str(num_digits)\n",
    "        for j in range(1, num_classifiers):\n",
    "            if (j > num_digits):\n",
    "                break\n",
    "            st = st + str(np.argmax(predictions[j][i]))\n",
    "        all_labels.append(st)\n",
    "\n",
    "\n",
    "    predictions = all_labels\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = testOneHotLabels[0].shape[0]\n",
    "    all_labels = []\n",
    "    for i in range(batch_size):\n",
    "        num_digits = np.argmax(testOneHotLabels[0][i])\n",
    "        st = str(num_digits)\n",
    "        for j in range(1, num_classifiers):\n",
    "            if (j > num_digits):\n",
    "                break\n",
    "            st = st + str(np.argmax(testOneHotLabels[j][i]))\n",
    "        all_labels.append(st)\n",
    "\n",
    "\n",
    "    labels = all_labels\n",
    "\n",
    "\n",
    "    equalities = np.zeros(batch_size_init)\n",
    "    for i in range(batch_size_init):\n",
    "        if predictions[i] == labels[i]:\n",
    "            equalities[i] = 1\n",
    "    sum = np.sum(equalities)\n",
    "    acc = (100.0 * sum) / batch_size_init\n",
    "    \n",
    "    test_accuracy = acc\n",
    "    test_predictions = predictions\n",
    "    writer.close()\n",
    "    \n",
    "    #save the model\n",
    "    saver.save(session, \"D:/magistratura/magistratura/MO/lab4/saved_model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17074e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt wuth training loss\n",
    "plt.figure()\n",
    "plt.plot(training_loss_epoch, training_loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training_loss.png')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,50])\n",
    "plt.savefig('D:/magistratura/magistratura/MO/lab4/output_images/' + 'training_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c16cf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt wuth validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(valid_accuracy_epoch, valid_accuracy)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('validation accuracy')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,100])\n",
    "plt.savefig('D:/magistratura/magistratura/MO/lab4/output_images/' + 'valid_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7645f10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt wuth training accuracy\n",
    "plt.figure()\n",
    "plt.plot(train_accuracy_epoch, train_accuracy)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training batch accuracy')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,100])\n",
    "plt.savefig('D:/magistratura/magistratura/MO/lab4/output_images/' + 'training_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d9cbb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\userr\\anaconda3\\envs\\py365\\lib\\site-packages\\ipykernel_launcher.py:4: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#random test predictions\n",
    "for image_num in range(100):\n",
    "    items = random.sample(range(test_data.shape[0]), 8)\n",
    "    for i, item in enumerate(items):\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.title(test_predictions[item][1:])\n",
    "        plt.imshow(test_data[item, :, :, 0])\n",
    "    plt.savefig('D:/magistratura/magistratura/MO/lab4/output_images/' + 'predictions' + str(image_num + 1) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ee7e918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 89.6%\n"
     ]
    }
   ],
   "source": [
    "#test accuracy\n",
    "print('Test accuracy: %.1f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849c714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
